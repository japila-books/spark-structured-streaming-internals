== DataStreamWriter

`DataStreamWriter` is an interface to write the result of executing a streaming query to a link:spark-sql-streaming-sink.adoc[streaming sink].

NOTE: A streaming query is a link:spark-sql-Dataset.adoc[Dataset] with a link:spark-sql-LogicalPlan.adoc#isStreaming[streaming logical plan].

`DataStreamWriter` is available using `writeStream` method of a streaming `Dataset`.

[source, scala]
----
import org.apache.spark.sql.streaming.DataStreamWriter
import org.apache.spark.sql.Row

val streamingQuery: Dataset[Long] = ...

scala> streamingQuery.isStreaming
res0: Boolean = true

val writer: DataStreamWriter[Row] = streamingQuery.writeStream
----

Like the batch `DataFrameWriter`, `DataStreamWriter` has a direct support for many <<writing-dataframes-to-files, file formats>> and <<format, an extension point to plug in new formats>>.

[source, scala]
----
// see above for writer definition

// Save dataset in JSON format
writer.format("json")
----

In the end, you start the actual continuous writing of the result of executing a `Dataset` to a sink using <<start, start>> operator.

[source, scala]
----
writer.save
----

Beside the above operators, there are the following ones working with a `Dataset` as a whole.

[[methods]]
.DataStreamWriter's Methods
[cols="1,2",options="header",width="100%"]
|===
| Method
| Description

| [[format]] `format`
a| Specifies the format of the output (which is an output data source and indirectly specifies the link:spark-sql-streaming-sink.adoc[streaming sink] to write the rows to)

Internally, `format` is referred to as a _source_ (as in the output data _source_).

Recognized "special" output data sources (in the code):

* `hive`
* `memory`
* <<foreach, foreach>>
* `console`

| <<foreach, foreach>>
| Sets link:spark-sql-streaming-ForeachWriter.adoc[ForeachWriter] in the full control of streaming writes.

| `option`
|

| `options`
|

| <<outputMode, outputMode>>
| Specifies the output mode

| `partitionBy`
|

| <<queryName, queryName>>
| Assigns the name of a query

| <<trigger, trigger>>
| Sets the link:spark-sql-streaming-trigger.adoc[Trigger] for how often a streaming query should be executed and the result saved.
|===

NOTE: `hive` <<start, is not supported>> for streaming writing (and leads to a `AnalysisException`).

NOTE: `DataFrameWriter` is responsible for writing in a batch fashion.

=== [[outputMode]] Specifying Output Mode -- `outputMode` method

[source, scala]
----
outputMode(outputMode: OutputMode): DataStreamWriter[T]
----

`outputMode` specifies *output mode* of a streaming link:spark-sql-dataset.adoc[Dataset] which is what gets written to a link:spark-sql-streaming-sink.adoc[streaming sink] when there is a new data available.

Currently, the following output modes are supported:

* `OutputMode.Append` -- only the new rows in the streaming dataset will be written to a sink.

* `OutputMode.Complete` -- entire streaming dataset (with all the rows) will be written to a sink every time there are updates. It is supported only for streaming queries with aggregations.

=== [[queryName]] Setting Query Name -- `queryName` method

[source, scala]
----
queryName(queryName: String): DataStreamWriter[T]
----

`queryName` sets the name of a link:spark-sql-streaming-StreamingQuery.adoc[streaming query].

Internally, it is just an additional <<option, option>> with the key `queryName`.

=== [[trigger]] Setting How Often to Execute Streaming Query -- `trigger` method

[source, scala]
----
trigger(trigger: Trigger): DataStreamWriter[T]
----

`trigger` method sets the time interval of the *trigger* (batch) for a streaming query.

NOTE: `Trigger` specifies how often results should be produced by a link:spark-sql-streaming-StreamingQuery.adoc[StreamingQuery]. See link:spark-sql-streaming-trigger.adoc[Trigger].

The default trigger is link:spark-sql-streaming-trigger.adoc#ProcessingTime[ProcessingTime(0L)] that runs a streaming query as often as possible.

TIP: Consult link:spark-sql-streaming-trigger.adoc[Trigger] to learn about `Trigger` and `ProcessingTime` types.

=== [[start]] Starting Continuous Writing to Sink -- `start` Method

[source, scala]
----
start(): StreamingQuery
start(path: String): StreamingQuery  // <1>
----
<1> Sets `path` option to `path` and passes the call on to `start()`

`start` starts a streaming query.

`start` gives a link:spark-sql-streaming-StreamingQuery.adoc[StreamingQuery] to control the execution of the continuous query.

NOTE: Whether or not you have to specify `path` option depends on the streaming sink in use.

Internally, `start` branches off per `source`.

* `memory`
* `foreach`
* other formats

...FIXME

[[start-options]]
.start's Options
[cols="1,2",options="header",width="100%"]
|===
| Option
| Description

| `queryName`
| Name of active streaming query

| `checkpointLocation`
| Directory for checkpointing.
|===

`start` reports a `AnalysisException` when `source` is `hive`.

[source, scala]
----
val q =  spark.
  readStream.
  text("server-logs/*").
  writeStream.
  format("hive") <-- hive format used as a streaming sink
scala> q.start
org.apache.spark.sql.AnalysisException: Hive data source can only be used with tables, you can not write files of Hive data source directly.;
  at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:234)
  ... 48 elided
----

NOTE: Define options using <<option, option>> or <<options, options>> methods.

=== [[foreach]] Making ForeachWriter in Charge of Streaming Writes -- `foreach` method

[source, scala]
----
foreach(writer: ForeachWriter[T]): DataStreamWriter[T]
----

`foreach` sets the input link:spark-sql-streaming-ForeachWriter.adoc[ForeachWriter] to be in control of streaming writes.

Internally, `foreach` sets the streaming output <<format, format>> as `foreach` and `foreachWriter` as the input `writer`.

NOTE: `foreach` uses `SparkSession` to access `SparkContext` to clean the `ForeachWriter`.

[NOTE]
====
`foreach` reports an `IllegalArgumentException` when `writer` is `null`.

```
foreach writer cannot be null
```
====
