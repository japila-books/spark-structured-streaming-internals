== [[HDFSBackedStateStoreProvider]] HDFSBackedStateStoreProvider -- Default StateStoreProvider

`HDFSBackedStateStoreProvider` is the default <<spark-sql-streaming-StateStoreProvider.adoc#, StateStoreProvider>> per <<spark-sql-streaming-properties.adoc#spark.sql.streaming.stateStore.providerClass, spark.sql.streaming.stateStore.providerClass>> internal configuration property.

When `StateStoreProvider` helper object is requested to <<spark-sql-streaming-StateStoreProvider.adoc#createAndInit, create and initialize the StateStoreProvider>>, `HDFSBackedStateStoreProvider` is created and requested to <<init, initialize>>.

[[baseDir]]
`HDFSBackedStateStoreProvider` uses the *state checkpoint base directory* (that is the <<spark-sql-streaming-StateStoreId.adoc#storeCheckpointLocation, storeCheckpointLocation>> of the <<stateStoreId, StateStoreId>>) for <<deltaFile, delta>> and <<snapshotFile, snapshot>> state files. The checkpoint directory is created when `HDFSBackedStateStoreProvider` is requested to <<init, initialize>>.

[[stateStoreId]]
The <<spark-sql-streaming-StateStoreProvider.adoc#stateStoreId, StateStoreId>> of a `HDFSBackedStateStoreProvider` is defined at <<init, initialization>>.

[[toString]]
When requested for the textual representation, `HDFSBackedStateStoreProvider` returns *HDFSStateStoreProvider[id = (op=[operatorId],part=[partitionId]),dir = [baseDir]]*.

[[internal-registries]]
.HDFSBackedStateStoreProvider's Internal Properties
[cols="1m,2",options="header",width="100%"]
|===
| Name
| Description

| loadedMaps
a| [[loadedMaps]]

[source, scala]
----
loadedMaps: TreeMap[Long, ConcurrentHashMap[UnsafeRow, UnsafeRow]]
----

https://docs.oracle.com/javase/8/docs/api/java/util/TreeMap.html[java.util.TreeMap] of FIXME sorted according to the reversed natural ordering of the keys

The current size estimation of `loadedMaps` is the <<memoryUsedBytes, memoryUsedBytes>> metric in the <<metrics, metrics>>.

A new entry (a version and the associated map) is added when `HDFSBackedStateStoreProvider` is requested to <<putStateIntoStateCacheMap, putStateIntoStateCacheMap>>

Used when...FIXME

| numberOfVersionsToRetainInMemory
a| [[numberOfVersionsToRetainInMemory]]

[source, scala]
----
numberOfVersionsToRetainInMemory: Int
----

`numberOfVersionsToRetainInMemory` is the <<spark-sql-streaming-properties.adoc#spark.sql.streaming.maxBatchesToRetainInMemory, spark.sql.streaming.maxBatchesToRetainInMemory>> configuration property that sets the upper limit on the number of entries in the <<loadedMaps, loadedMaps>> internal registry.

`numberOfVersionsToRetainInMemory` is used when `HDFSBackedStateStoreProvider` is requested to <<putStateIntoStateCacheMap, putStateIntoStateCacheMap>>.
|===

[[logging]]
[TIP]
====
Enable `DEBUG`, `INFO` or `WARN` logging level for `org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider=DEBUG
```

Refer to link:spark-sql-streaming-logging.adoc[Logging].
====

=== [[getStore]] Retrieving State Store for Version -- `getStore` Method

[source, scala]
----
getStore(version: Long): StateStore
----

NOTE: `getStore` is part of the <<spark-sql-streaming-StateStoreProvider.adoc#getStore, StateStoreProvider Contract>> to get the <<spark-sql-streaming-StateStore.adoc#, StateStore>> for a given version.

`getStore`...FIXME

=== [[deltaFile]] `deltaFile` Internal Method

[source, scala]
----
deltaFile(version: Long): Path
----

`deltaFile` creates a Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path] of the `[version].delta` file in the <<baseDir, baseDir>>.

NOTE: `deltaFile` is used when...FIXME

=== [[fetchFiles]] `fetchFiles` Internal Method

[source, scala]
----
fetchFiles(): Seq[StoreFile]
----

`fetchFiles`...FIXME

NOTE: `fetchFiles` is used when `HDFSBackedStateStoreProvider` is requested to <<latestIterator, latestIterator>>, <<doSnapshot, doSnapshot>> and <<cleanup, cleanup>>.

=== [[init]] Initializing StateStoreProvider -- `init` Method

[source, scala]
----
init(
  stateStoreId: StateStoreId,
  keySchema: StructType,
  valueSchema: StructType,
  indexOrdinal: Option[Int],
  storeConf: StateStoreConf,
  hadoopConf: Configuration): Unit
----

NOTE: `init` is part of the <<spark-sql-streaming-StateStoreProvider.adoc#init, StateStoreProvider Contract>> to initialize itself.

`init` assigns the values of the input arguments to <<stateStoreId, stateStoreId>>, <<keySchema, keySchema>>, <<valueSchema, valueSchema>>, <<storeConf, storeConf>>, and <<hadoopConf, hadoopConf>>.

`init` uses the `StateStoreConf` to requests for the <<spark.sql.streaming.maxBatchesToRetainInMemory, spark.sql.streaming.maxBatchesToRetainInMemory>> configuration property (that is then the <<numberOfVersionsToRetainInMemory, numberOfVersionsToRetainInMemory>>).

In the end, `init` requests the <<fm, CheckpointFileManager>> to create the <<baseDir, baseDir>> directory (with subdirectories).

=== [[latestIterator]] `latestIterator` Internal Method

[source, scala]
----
latestIterator(): Iterator[UnsafeRowPair]
----

`latestIterator`...FIXME

NOTE: `latestIterator` seems to be used exclusively in tests.

=== [[doSnapshot]] `doSnapshot` Internal Method

[source, scala]
----
doSnapshot(): Unit
----

`doSnapshot`...FIXME

NOTE: `doSnapshot` is used when...FIXME

=== [[cleanup]] `cleanup` Internal Method

[source, scala]
----
cleanup(): Unit
----

`cleanup`...FIXME

NOTE: `cleanup` is used when...FIXME

=== [[close]] Closing State Store Provider -- `close` Method

[source, scala]
----
close(): Unit
----

NOTE: `close` is part of the <<spark-sql-streaming-StateStoreProvider.adoc#close, StateStoreProvider Contract>> to close the state store provider.

`close`...FIXME

=== [[putStateIntoStateCacheMap]] `putStateIntoStateCacheMap` Internal Method

[source, scala]
----
putStateIntoStateCacheMap(
  newVersion: Long,
  map: ConcurrentHashMap[UnsafeRow, UnsafeRow]): Unit
----

`putStateIntoStateCacheMap`...FIXME

NOTE: `putStateIntoStateCacheMap` is used when `HDFSBackedStateStoreProvider` is requested to <<commitUpdates, commitUpdates>> and <<loadMap, loadMap>>.

=== [[commitUpdates]] `commitUpdates` Internal Method

[source, scala]
----
commitUpdates(
  newVersion: Long,
  map: ConcurrentHashMap[UnsafeRow, UnsafeRow],
  output: DataOutputStream): Unit
----

`commitUpdates`...FIXME

NOTE: `commitUpdates` is used exclusively when `HDFSBackedStateStore` is requested to <<spark-sql-streaming-HDFSBackedStateStore.adoc#commit, commit state changes>>.

=== [[loadMap]] `loadMap` Internal Method

[source, scala]
----
loadMap(version: Long): ConcurrentHashMap[UnsafeRow, UnsafeRow]
----

`loadMap`...FIXME

NOTE: `loadMap` is used when `HDFSBackedStateStoreProvider` is requested to <<getStore, getStore>> and <<latestIterator, latestIterator>>.
