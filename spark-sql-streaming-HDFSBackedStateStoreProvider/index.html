<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Demystifying inner-workings of Spark Structured Streaming"><link href=https://jaceklaskowski.github.io/spark-structured-streaming-book/spark-sql-streaming-HDFSBackedStateStoreProvider/ rel=canonical><meta name=author content="Jacek Laskowski"><link rel="shortcut icon" href=../assets/images/favicon.png><meta name=generator content="mkdocs-1.1.2, mkdocs-material-6.0.2"><title>HDFSBackedStateStoreProvider - The Internals of Spark Structured Streaming</title><link rel=stylesheet href=../assets/stylesheets/main.38780c08.min.css><link rel=stylesheet href=../assets/stylesheets/palette.3f72e892.min.css><link href=https://fonts.gstatic.com rel=preconnect crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-151208281-3","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}),document.addEventListener("DOMContentSwitch",function(){ga("send","pageview",document.location.pathname)})</script><script async src=https://www.google-analytics.com/analytics.js></script></head> <body dir=ltr data-md-color-scheme data-md-color-primary=none data-md-color-accent=none> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#refer-to class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid" aria-label=Header> <a href=https://jaceklaskowski.github.io/spark-structured-streaming-book/ title="The Internals of Spark Structured Streaming" class="md-header-nav__button md-logo" aria-label="The Internals of Spark Structured Streaming"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 2l-5 4.5v11l5-4.5V2M6.5 5C4.55 5 2.45 5.4 1 6.5v14.66c0 .25.25.5.5.5.1 0 .15-.07.25-.07 1.35-.65 3.3-1.09 4.75-1.09 1.95 0 4.05.4 5.5 1.5 1.35-.85 3.8-1.5 5.5-1.5 1.65 0 3.35.31 4.75 1.06.1.05.15.03.25.03.25 0 .5-.25.5-.5V6.5c-.6-.45-1.25-.75-2-1V19c-1.1-.35-2.3-.5-3.5-.5-1.7 0-4.15.65-5.5 1.5V6.5C10.55 5.4 8.45 5 6.5 5z"/></svg> </a> <label class="md-header-nav__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header-nav__title data-md-component=header-title> <div class=md-header-nav__ellipsis> <span class="md-header-nav__topic md-ellipsis"> The Internals of Spark Structured Streaming </span> <span class="md-header-nav__topic md-ellipsis"> HDFSBackedStateStoreProvider </span> </div> </div> <label class="md-header-nav__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query data-md-state=active> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <button type=reset class="md-search__icon md-icon" aria-label=Clear data-md-component=search-reset tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header-nav__source> <a href=https://github.com/jaceklaskowski/spark-structured-streaming-book/ title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </div> <div class=md-source__repository> spark-structured-streaming-book </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class="md-tabs md-tabs--active" aria-label=Tabs data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=.. class="md-tabs__link md-tabs__link--active"> Home </a> </li> <li class=md-tabs__item> <a href=../operators/ class=md-tabs__link> Streaming Operators </a> </li> <li class=md-tabs__item> <a href=../spark-sql-streaming-FileStreamSource/ class=md-tabs__link> Data Sources </a> </li> <li class=md-tabs__item> <a href=../monitoring/StreamingQueryListener/ class=md-tabs__link> Monitoring </a> </li> <li class=md-tabs__item> <a href=../webui/ class=md-tabs__link> Web UI </a> </li> <li class=md-tabs__item> <a href=../demo/spark-sql-streaming-demo-FlatMapGroupsWithStateExec/ class=md-tabs__link> Demos </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=https://jaceklaskowski.github.io/spark-structured-streaming-book/ title="The Internals of Spark Structured Streaming" class="md-nav__button md-logo" aria-label="The Internals of Spark Structured Streaming"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 2l-5 4.5v11l5-4.5V2M6.5 5C4.55 5 2.45 5.4 1 6.5v14.66c0 .25.25.5.5.5.1 0 .15-.07.25-.07 1.35-.65 3.3-1.09 4.75-1.09 1.95 0 4.05.4 5.5 1.5 1.35-.85 3.8-1.5 5.5-1.5 1.65 0 3.35.31 4.75 1.06.1.05.15.03.25.03.25 0 .5-.25.5-.5V6.5c-.6-.45-1.25-.75-2-1V19c-1.1-.35-2.3-.5-3.5-.5-1.7 0-4.15.65-5.5 1.5V6.5C10.55 5.4 8.45 5 6.5 5z"/></svg> </a> The Internals of Spark Structured Streaming </label> <div class=md-nav__source> <a href=https://github.com/jaceklaskowski/spark-structured-streaming-book/ title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </div> <div class=md-source__repository> spark-structured-streaming-book </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1 type=checkbox id=nav-1 checked> <label class=md-nav__link for=nav-1> Home <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Home data-md-level=1> <label class=md-nav__title for=nav-1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. class=md-nav__link> Welcome </a> </li> <li class=md-nav__item> <a href=../spark-structured-streaming/ class=md-nav__link> Spark Structured Streaming and Streaming Queries </a> </li> <li class=md-nav__item> <a href=../spark-structured-streaming-batch-processing-time/ class=md-nav__link> Batch Processing Time </a> </li> <li class=md-nav__item> <a href=../spark-structured-streaming-internals/ class=md-nav__link> Internals of Streaming Queries </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-5 type=checkbox id=nav-1-5> <label class=md-nav__link for=nav-1-5> Streaming Join <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Streaming Join" data-md-level=2> <label class=md-nav__title for=nav-1-5> <span class="md-nav__icon md-icon"></span> Streaming Join </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-join/ class=md-nav__link> Streaming Join </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreAwareZipPartitionsRDD/ class=md-nav__link> StateStoreAwareZipPartitionsRDD </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-5-3 type=checkbox id=nav-1-5-3> <label class=md-nav__link for=nav-1-5-3> SymmetricHashJoinStateManager <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=SymmetricHashJoinStateManager data-md-level=3> <label class=md-nav__title for=nav-1-5-3> <span class="md-nav__icon md-icon"></span> SymmetricHashJoinStateManager </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-SymmetricHashJoinStateManager/ class=md-nav__link> SymmetricHashJoinStateManager </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreHandler/ class=md-nav__link> StateStoreHandler </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-KeyToNumValuesStore/ class=md-nav__link> KeyToNumValuesStore </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-KeyWithIndexToValueStore/ class=md-nav__link> KeyWithIndexToValueStore </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-OneSideHashJoiner/ class=md-nav__link> OneSideHashJoiner </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-JoinStateWatermarkPredicates/ class=md-nav__link> JoinStateWatermarkPredicates </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-JoinStateWatermarkPredicate/ class=md-nav__link> JoinStateWatermarkPredicate </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-5-7 type=checkbox id=nav-1-5-7> <label class=md-nav__link for=nav-1-5-7> StateStoreAwareZipPartitionsHelper <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=StateStoreAwareZipPartitionsHelper data-md-level=3> <label class=md-nav__title for=nav-1-5-7> <span class="md-nav__icon md-icon"></span> StateStoreAwareZipPartitionsHelper </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreAwareZipPartitionsHelper/ class=md-nav__link> StateStoreAwareZipPartitionsHelper </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingSymmetricHashJoinHelper/ class=md-nav__link> StreamingSymmetricHashJoinHelper </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingJoinHelper/ class=md-nav__link> StreamingJoinHelper </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-6 type=checkbox id=nav-1-6> <label class=md-nav__link for=nav-1-6> Extending Structured Streaming with New Data Sources <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Extending Structured Streaming with New Data Sources" data-md-level=2> <label class=md-nav__title for=nav-1-6> <span class="md-nav__icon md-icon"></span> Extending Structured Streaming with New Data Sources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-extending-new-data-sources/ class=md-nav__link> Extending Structured Streaming with New Data Sources </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-BaseStreamingSource/ class=md-nav__link> BaseStreamingSource </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-BaseStreamingSink/ class=md-nav__link> BaseStreamingSink </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamWriteSupport/ class=md-nav__link> StreamWriteSupport </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamWriter/ class=md-nav__link> StreamWriter </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-DataSource/ class=md-nav__link> DataSource </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-7 type=checkbox id=nav-1-7> <label class=md-nav__link for=nav-1-7> Streaming Aggregation <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Streaming Aggregation" data-md-level=2> <label class=md-nav__title for=nav-1-7> <span class="md-nav__icon md-icon"></span> Streaming Aggregation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-aggregation/ class=md-nav__link> Streaming Aggregation </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreRDD/ class=md-nav__link> StateStoreRDD </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreOps/ class=md-nav__link> StateStoreOps </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingAggregationStateManager/ class=md-nav__link> StreamingAggregationStateManager </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingAggregationStateManagerBaseImpl/ class=md-nav__link> StreamingAggregationStateManagerBaseImpl </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingAggregationStateManagerImplV1/ class=md-nav__link> StreamingAggregationStateManagerImplV1 </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingAggregationStateManagerImplV2/ class=md-nav__link> StreamingAggregationStateManagerImplV2 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-8 type=checkbox id=nav-1-8 checked> <label class=md-nav__link for=nav-1-8> Stateful Stream Processing <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Stateful Stream Processing" data-md-level=2> <label class=md-nav__title for=nav-1-8> <span class="md-nav__icon md-icon"></span> Stateful Stream Processing </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-stateful-stream-processing/ class=md-nav__link> Stateful Stream Processing </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-watermark/ class=md-nav__link> Streaming Watermark </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-deduplication/ class=md-nav__link> Streaming Deduplication </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-limit/ class=md-nav__link> Streaming Limit </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-8-5 type=checkbox id=nav-1-8-5> <label class=md-nav__link for=nav-1-8-5> StateStore <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=StateStore data-md-level=3> <label class=md-nav__title for=nav-1-8-5> <span class="md-nav__icon md-icon"></span> StateStore </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStore/ class=md-nav__link> StateStore </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreId/ class=md-nav__link> StateStoreId </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-HDFSBackedStateStore/ class=md-nav__link> HDFSBackedStateStore </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-8-6 type=checkbox id=nav-1-8-6 checked> <label class=md-nav__link for=nav-1-8-6> StateStoreProvider <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=StateStoreProvider data-md-level=3> <label class=md-nav__title for=nav-1-8-6> <span class="md-nav__icon md-icon"></span> StateStoreProvider </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreProvider/ class=md-nav__link> StateStoreProvider </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreProviderId/ class=md-nav__link> StateStoreProviderId </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> HDFSBackedStateStoreProvider <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> HDFSBackedStateStoreProvider </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#sourcescala class=md-nav__link> [source,scala] </a> </li> <li class=md-nav__item> <a href=#basedir-path class=md-nav__link> baseDir: Path </a> </li> <li class=md-nav__item> <a href=#source-scala class=md-nav__link> [source, scala] </a> </li> <li class=md-nav__item> <a href=#tostring-string class=md-nav__link> toString: String </a> </li> <li class=md-nav__item> <a href=#source-scala_1 class=md-nav__link> [source, scala] </a> </li> <li class=md-nav__item> <a href=#source-scala_2 class=md-nav__link> [source, scala] </a> </li> <li class=md-nav__item> <a href=#deltafileversion-long-path class=md-nav__link> deltaFile(version: Long): Path </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-8-7 type=checkbox id=nav-1-8-7> <label class=md-nav__link for=nav-1-8-7> StateStoreCoordinator <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=StateStoreCoordinator data-md-level=3> <label class=md-nav__title for=nav-1-8-7> <span class="md-nav__icon md-icon"></span> StateStoreCoordinator </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreCoordinator/ class=md-nav__link> StateStoreCoordinator </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreCoordinatorRef/ class=md-nav__link> StateStoreCoordinatorRef </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-WatermarkSupport/ class=md-nav__link> WatermarkSupport </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StatefulOperatorStateInfo/ class=md-nav__link> StatefulOperatorStateInfo </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreMetrics/ class=md-nav__link> StateStoreMetrics </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreCustomMetric/ class=md-nav__link> StateStoreCustomMetric </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreUpdater/ class=md-nav__link> StateStoreUpdater </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-EventTimeStatsAccum/ class=md-nav__link> EventTimeStatsAccum </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreConf/ class=md-nav__link> StateStoreConf </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-9 type=checkbox id=nav-1-9> <label class=md-nav__link for=nav-1-9> Arbitrary Stateful Streaming Aggregation <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Arbitrary Stateful Streaming Aggregation" data-md-level=2> <label class=md-nav__title for=nav-1-9> <span class="md-nav__icon md-icon"></span> Arbitrary Stateful Streaming Aggregation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../arbitrary-stateful-streaming-aggregation/ class=md-nav__link> Arbitrary Stateful Streaming Aggregation </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-9-2 type=checkbox id=nav-1-9-2> <label class=md-nav__link for=nav-1-9-2> KeyValueGroupedDataset <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=KeyValueGroupedDataset data-md-level=3> <label class=md-nav__title for=nav-1-9-2> <span class="md-nav__icon md-icon"></span> KeyValueGroupedDataset </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../KeyValueGroupedDataset/ class=md-nav__link> KeyValueGroupedDataset </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-KeyValueGroupedDataset-mapGroupsWithState/ class=md-nav__link> mapGroupsWithState Operator </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-KeyValueGroupedDataset-flatMapGroupsWithState/ class=md-nav__link> flatMapGroupsWithState Operator </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-9-3 type=checkbox id=nav-1-9-3> <label class=md-nav__link for=nav-1-9-3> GroupState <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=GroupState data-md-level=3> <label class=md-nav__title for=nav-1-9-3> <span class="md-nav__icon md-icon"></span> GroupState </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../GroupState/ class=md-nav__link> GroupState </a> </li> <li class=md-nav__item> <a href=../GroupStateImpl/ class=md-nav__link> GroupStateImpl </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-GroupStateTimeout/ class=md-nav__link> GroupStateTimeout </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-9-5 type=checkbox id=nav-1-9-5> <label class=md-nav__link for=nav-1-9-5> StateManager <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=StateManager data-md-level=3> <label class=md-nav__title for=nav-1-9-5> <span class="md-nav__icon md-icon"></span> StateManager </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-StateManager/ class=md-nav__link> StateManager </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateManagerImplV2/ class=md-nav__link> StateManagerImplV2 </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateManagerImplBase/ class=md-nav__link> StateManagerImplBase </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateManagerImplV1/ class=md-nav__link> StateManagerImplV1 </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-FlatMapGroupsWithStateExecHelper/ class=md-nav__link> FlatMapGroupsWithStateExecHelper Helper Class </a> </li> <li class=md-nav__item> <a href=../InputProcessor/ class=md-nav__link> InputProcessor </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../StreamingQueryManager/ class=md-nav__link> StreamingQueryManager </a> </li> <li class=md-nav__item> <a href=../StreamingQueryListenerBus/ class=md-nav__link> StreamingQueryListenerBus </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-12 type=checkbox id=nav-1-12> <label class=md-nav__link for=nav-1-12> Developing Streaming Applications <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Developing Streaming Applications" data-md-level=2> <label class=md-nav__title for=nav-1-12> <span class="md-nav__icon md-icon"></span> Developing Streaming Applications </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-DataStreamReader/ class=md-nav__link> DataStreamReader </a> </li> <li class=md-nav__item> <a href=../SparkDataStream/ class=md-nav__link> SparkDataStream </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-12-3 type=checkbox id=nav-1-12-3> <label class=md-nav__link for=nav-1-12-3> DataStreamWriter <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=DataStreamWriter data-md-level=3> <label class=md-nav__title for=nav-1-12-3> <span class="md-nav__icon md-icon"></span> DataStreamWriter </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../DataStreamWriter/ class=md-nav__link> DataStreamWriter </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-OutputMode/ class=md-nav__link> OutputMode </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-Trigger/ class=md-nav__link> Trigger </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../StreamingQuery/ class=md-nav__link> StreamingQuery </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-window/ class=md-nav__link> window Function </a> </li> <li class=md-nav__item> <a href=../SQLConf/ class=md-nav__link> SQLConf </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-properties/ class=md-nav__link> Configuration Properties </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-13 type=checkbox id=nav-1-13> <label class=md-nav__link for=nav-1-13> Query Planning and Execution <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Query Planning and Execution" data-md-level=2> <label class=md-nav__title for=nav-1-13> <span class="md-nav__icon md-icon"></span> Query Planning and Execution </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../StreamExecution/ class=md-nav__link> StreamExecution </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-TriggerExecutor/ class=md-nav__link> TriggerExecutor </a> </li> <li class=md-nav__item> <a href=../IncrementalExecution/ class=md-nav__link> IncrementalExecution </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamMetadata/ class=md-nav__link> StreamMetadata </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-13-5 type=checkbox id=nav-1-13-5> <label class=md-nav__link for=nav-1-13-5> Logical Operators <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Logical Operators" data-md-level=3> <label class=md-nav__title for=nav-1-13-5> <span class="md-nav__icon md-icon"></span> Logical Operators </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../EventTimeWatermark/ class=md-nav__link> EventTimeWatermark </a> </li> <li class=md-nav__item> <a href=../logical-operators/FlatMapGroupsWithState/ class=md-nav__link> FlatMapGroupsWithState </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-Deduplicate/ class=md-nav__link> Deduplicate </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-MemoryPlan/ class=md-nav__link> MemoryPlan </a> </li> <li class=md-nav__item> <a href=../StreamingDataSourceV2Relation/ class=md-nav__link> StreamingDataSourceV2Relation </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingRelation/ class=md-nav__link> StreamingRelation </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingRelationV2/ class=md-nav__link> StreamingRelationV2 </a> </li> <li class=md-nav__item> <a href=../StreamingExecutionRelation/ class=md-nav__link> StreamingExecutionRelation </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-13-6 type=checkbox id=nav-1-13-6> <label class=md-nav__link for=nav-1-13-6> Physical Operators <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Physical Operators" data-md-level=3> <label class=md-nav__title for=nav-1-13-6> <span class="md-nav__icon md-icon"></span> Physical Operators </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../physical-operators/EventTimeWatermarkExec/ class=md-nav__link> EventTimeWatermarkExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/FlatMapGroupsWithStateExec/ class=md-nav__link> FlatMapGroupsWithStateExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/StatefulOperator/ class=md-nav__link> StatefulOperator </a> </li> <li class=md-nav__item> <a href=../physical-operators/StateStoreReader/ class=md-nav__link> StateStoreReader </a> </li> <li class=md-nav__item> <a href=../physical-operators/StateStoreRestoreExec/ class=md-nav__link> StateStoreRestoreExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/StateStoreSaveExec/ class=md-nav__link> StateStoreSaveExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/StateStoreWriter/ class=md-nav__link> StateStoreWriter </a> </li> <li class=md-nav__item> <a href=../physical-operators/StreamingDeduplicateExec/ class=md-nav__link> StreamingDeduplicateExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/StreamingGlobalLimitExec/ class=md-nav__link> StreamingGlobalLimitExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/StreamingRelationExec/ class=md-nav__link> StreamingRelationExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/StreamingSymmetricHashJoinExec/ class=md-nav__link> StreamingSymmetricHashJoinExec </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-13-7 type=checkbox id=nav-1-13-7> <label class=md-nav__link for=nav-1-13-7> Execution Planning Strategies <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Execution Planning Strategies" data-md-level=3> <label class=md-nav__title for=nav-1-13-7> <span class="md-nav__icon md-icon"></span> Execution Planning Strategies </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-FlatMapGroupsWithStateStrategy/ class=md-nav__link> FlatMapGroupsWithStateStrategy </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StatefulAggregationStrategy/ class=md-nav__link> StatefulAggregationStrategy </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingDeduplicationStrategy/ class=md-nav__link> StreamingDeduplicationStrategy </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingGlobalLimitStrategy/ class=md-nav__link> StreamingGlobalLimitStrategy </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingJoinStrategy/ class=md-nav__link> StreamingJoinStrategy </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingRelationStrategy/ class=md-nav__link> StreamingRelationStrategy </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingQueryWrapper/ class=md-nav__link> StreamingQueryWrapper </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-14 type=checkbox id=nav-1-14> <label class=md-nav__link for=nav-1-14> Offsets and Metadata Checkpointing <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Offsets and Metadata Checkpointing" data-md-level=2> <label class=md-nav__title for=nav-1-14> <span class="md-nav__icon md-icon"></span> Offsets and Metadata Checkpointing </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-offsets-and-metadata-checkpointing/ class=md-nav__link> Offsets and Metadata Checkpointing </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-MetadataLog/ class=md-nav__link> MetadataLog </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-HDFSMetadataLog/ class=md-nav__link> HDFSMetadataLog </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-14-4 type=checkbox id=nav-1-14-4> <label class=md-nav__link for=nav-1-14-4> CommitLog <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=CommitLog data-md-level=3> <label class=md-nav__title for=nav-1-14-4> <span class="md-nav__icon md-icon"></span> CommitLog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-CommitLog/ class=md-nav__link> CommitLog </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-CommitMetadata/ class=md-nav__link> CommitMetadata </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-14-5 type=checkbox id=nav-1-14-5> <label class=md-nav__link for=nav-1-14-5> OffsetSeqLog <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=OffsetSeqLog data-md-level=3> <label class=md-nav__title for=nav-1-14-5> <span class="md-nav__icon md-icon"></span> OffsetSeqLog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-OffsetSeqLog/ class=md-nav__link> OffsetSeqLog </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-OffsetSeq/ class=md-nav__link> OffsetSeq </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-14-6 type=checkbox id=nav-1-14-6> <label class=md-nav__link for=nav-1-14-6> CompactibleFileStreamLog <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=CompactibleFileStreamLog data-md-level=3> <label class=md-nav__title for=nav-1-14-6> <span class="md-nav__icon md-icon"></span> CompactibleFileStreamLog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-CompactibleFileStreamLog/ class=md-nav__link> CompactibleFileStreamLog </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-FileStreamSourceLog/ class=md-nav__link> FileStreamSourceLog </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-OffsetSeqMetadata/ class=md-nav__link> OffsetSeqMetadata </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-14-8 type=checkbox id=nav-1-14-8> <label class=md-nav__link for=nav-1-14-8> CheckpointFileManager <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=CheckpointFileManager data-md-level=3> <label class=md-nav__title for=nav-1-14-8> <span class="md-nav__icon md-icon"></span> CheckpointFileManager </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-CheckpointFileManager/ class=md-nav__link> CheckpointFileManager </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-FileContextBasedCheckpointFileManager/ class=md-nav__link> FileContextBasedCheckpointFileManager </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-FileSystemBasedCheckpointFileManager/ class=md-nav__link> FileSystemBasedCheckpointFileManager </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-Offset/ class=md-nav__link> Offset </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamProgress/ class=md-nav__link> StreamProgress </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-15 type=checkbox id=nav-1-15> <label class=md-nav__link for=nav-1-15> Micro-Batch Stream Processing <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Micro-Batch Stream Processing" data-md-level=2> <label class=md-nav__title for=nav-1-15> <span class="md-nav__icon md-icon"></span> Micro-Batch Stream Processing </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../micro-batch-stream-processing/ class=md-nav__link> Micro-Batch Stream Processing </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-15-2 type=checkbox id=nav-1-15-2> <label class=md-nav__link for=nav-1-15-2> MicroBatchExecution <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=MicroBatchExecution data-md-level=3> <label class=md-nav__title for=nav-1-15-2> <span class="md-nav__icon md-icon"></span> MicroBatchExecution </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../MicroBatchExecution/ class=md-nav__link> MicroBatchExecution </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-MicroBatchWriter/ class=md-nav__link> MicroBatchWriter </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-15-3 type=checkbox id=nav-1-15-3> <label class=md-nav__link for=nav-1-15-3> MicroBatchReadSupport <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=MicroBatchReadSupport data-md-level=3> <label class=md-nav__title for=nav-1-15-3> <span class="md-nav__icon md-icon"></span> MicroBatchReadSupport </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-MicroBatchReadSupport/ class=md-nav__link> MicroBatchReadSupport </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-MicroBatchReader/ class=md-nav__link> MicroBatchReader </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-WatermarkTracker/ class=md-nav__link> WatermarkTracker </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-15-5 type=checkbox id=nav-1-15-5> <label class=md-nav__link for=nav-1-15-5> Source <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Source data-md-level=3> <label class=md-nav__title for=nav-1-15-5> <span class="md-nav__icon md-icon"></span> Source </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../Source/ class=md-nav__link> Source </a> </li> <li class=md-nav__item> <a href=../StreamSourceProvider/ class=md-nav__link> StreamSourceProvider </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-15-6 type=checkbox id=nav-1-15-6> <label class=md-nav__link for=nav-1-15-6> Sink <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Sink data-md-level=3> <label class=md-nav__title for=nav-1-15-6> <span class="md-nav__icon md-icon"></span> Sink </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-Sink/ class=md-nav__link> Sink </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamSinkProvider/ class=md-nav__link> StreamSinkProvider </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-16 type=checkbox id=nav-1-16> <label class=md-nav__link for=nav-1-16> Continuous Stream Processing <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Continuous Stream Processing" data-md-level=2> <label class=md-nav__title for=nav-1-16> <span class="md-nav__icon md-icon"></span> Continuous Stream Processing </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-continuous-stream-processing/ class=md-nav__link> Continuous Stream Processing </a> </li> <li class=md-nav__item> <a href=../ContinuousExecution/ class=md-nav__link> ContinuousExecution </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-ContinuousReadSupport/ class=md-nav__link> ContinuousReadSupport Contract </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-ContinuousReader/ class=md-nav__link> ContinuousReader Contract </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-RateStreamContinuousReader/ class=md-nav__link> RateStreamContinuousReader </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-16-6 type=checkbox id=nav-1-16-6> <label class=md-nav__link for=nav-1-16-6> EpochCoordinator <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=EpochCoordinator data-md-level=3> <label class=md-nav__title for=nav-1-16-6> <span class="md-nav__icon md-icon"></span> EpochCoordinator </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-EpochCoordinator/ class=md-nav__link> EpochCoordinator RPC Endpoint </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-EpochCoordinatorRef/ class=md-nav__link> EpochCoordinatorRef </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-EpochTracker/ class=md-nav__link> EpochTracker </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-16-7 type=checkbox id=nav-1-16-7> <label class=md-nav__link for=nav-1-16-7> ContinuousQueuedDataReader <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=ContinuousQueuedDataReader data-md-level=3> <label class=md-nav__title for=nav-1-16-7> <span class="md-nav__icon md-icon"></span> ContinuousQueuedDataReader </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-ContinuousQueuedDataReader/ class=md-nav__link> ContinuousQueuedDataReader </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-ContinuousQueuedDataReader-DataReaderThread/ class=md-nav__link> DataReaderThread </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-ContinuousQueuedDataReader-EpochMarkerGenerator/ class=md-nav__link> EpochMarkerGenerator </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-PartitionOffset/ class=md-nav__link> PartitionOffset </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-ContinuousExecutionRelation/ class=md-nav__link> ContinuousExecutionRelation Leaf Logical Operator </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-WriteToContinuousDataSource/ class=md-nav__link> WriteToContinuousDataSource Unary Logical Operator </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-16-11 type=checkbox id=nav-1-16-11> <label class=md-nav__link for=nav-1-16-11> WriteToContinuousDataSourceExec <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=WriteToContinuousDataSourceExec data-md-level=3> <label class=md-nav__title for=nav-1-16-11> <span class="md-nav__icon md-icon"></span> WriteToContinuousDataSourceExec </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-WriteToContinuousDataSourceExec/ class=md-nav__link> WriteToContinuousDataSourceExec Unary Physical Operator </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-ContinuousWriteRDD/ class=md-nav__link> ContinuousWriteRDD </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-ContinuousDataSourceRDD/ class=md-nav__link> ContinuousDataSourceRDD </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-UnsupportedOperationChecker/ class=md-nav__link> UnsupportedOperationChecker </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2 type=checkbox id=nav-2> <label class=md-nav__link for=nav-2> Streaming Operators <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Streaming Operators" data-md-level=1> <label class=md-nav__title for=nav-2> <span class="md-nav__icon md-icon"></span> Streaming Operators </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../operators/ class=md-nav__link> Streaming Operators </a> </li> <li class=md-nav__item> <a href=../operators/crossJoin/ class=md-nav__link> crossJoin </a> </li> <li class=md-nav__item> <a href=../operators/dropDuplicates/ class=md-nav__link> dropDuplicates </a> </li> <li class=md-nav__item> <a href=../operators/explain/ class=md-nav__link> explain </a> </li> <li class=md-nav__item> <a href=../operators/flatMapGroupsWithState/ class=md-nav__link> flatMapGroupsWithState </a> </li> <li class=md-nav__item> <a href=../operators/groupBy/ class=md-nav__link> groupBy </a> </li> <li class=md-nav__item> <a href=../operators/groupByKey/ class=md-nav__link> groupByKey </a> </li> <li class=md-nav__item> <a href=../operators/join/ class=md-nav__link> join </a> </li> <li class=md-nav__item> <a href=../operators/joinWith/ class=md-nav__link> joinWith </a> </li> <li class=md-nav__item> <a href=../operators/withWatermark/ class=md-nav__link> withWatermark </a> </li> <li class=md-nav__item> <a href=../operators/writeStream/ class=md-nav__link> writeStream </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3 type=checkbox id=nav-3> <label class=md-nav__link for=nav-3> Data Sources <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Data Sources" data-md-level=1> <label class=md-nav__title for=nav-3> <span class="md-nav__icon md-icon"></span> Data Sources </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3-1 type=checkbox id=nav-3-1> <label class=md-nav__link for=nav-3-1> File-Based Data Source <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="File-Based Data Source" data-md-level=2> <label class=md-nav__title for=nav-3-1> <span class="md-nav__icon md-icon"></span> File-Based Data Source </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-FileStreamSource/ class=md-nav__link> FileStreamSource </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-FileStreamSink/ class=md-nav__link> FileStreamSink </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-FileStreamSinkLog/ class=md-nav__link> FileStreamSinkLog </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-SinkFileStatus/ class=md-nav__link> SinkFileStatus </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-ManifestFileCommitProtocol/ class=md-nav__link> ManifestFileCommitProtocol </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-MetadataLogFileIndex/ class=md-nav__link> MetadataLogFileIndex </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3-2 type=checkbox id=nav-3-2> <label class=md-nav__link for=nav-3-2> Kafka Data Source <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Kafka Data Source" data-md-level=2> <label class=md-nav__title for=nav-3-2> <span class="md-nav__icon md-icon"></span> Kafka Data Source </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-kafka-data-source/ class=md-nav__link> Kafka Data Source </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-KafkaSourceProvider/ class=md-nav__link> KafkaSourceProvider </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-KafkaSource/ class=md-nav__link> KafkaSource </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-KafkaRelation/ class=md-nav__link> KafkaRelation </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-KafkaSourceRDD/ class=md-nav__link> KafkaSourceRDD </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-CachedKafkaConsumer/ class=md-nav__link> CachedKafkaConsumer </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-KafkaSourceOffset/ class=md-nav__link> KafkaSourceOffset </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-KafkaOffsetReader/ class=md-nav__link> KafkaOffsetReader </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-ConsumerStrategy/ class=md-nav__link> ConsumerStrategy </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-KafkaSink/ class=md-nav__link> KafkaSink </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-KafkaOffsetRangeLimit/ class=md-nav__link> KafkaOffsetRangeLimit </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-KafkaDataConsumer/ class=md-nav__link> KafkaDataConsumer </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3-2-13 type=checkbox id=nav-3-2-13> <label class=md-nav__link for=nav-3-2-13> KafkaMicroBatchReader <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=KafkaMicroBatchReader data-md-level=3> <label class=md-nav__title for=nav-3-2-13> <span class="md-nav__icon md-icon"></span> KafkaMicroBatchReader </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-KafkaMicroBatchReader/ class=md-nav__link> KafkaMicroBatchReader </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-KafkaOffsetRangeCalculator/ class=md-nav__link> KafkaOffsetRangeCalculator </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-KafkaMicroBatchInputPartition/ class=md-nav__link> KafkaMicroBatchInputPartition </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-KafkaMicroBatchInputPartitionReader/ class=md-nav__link> KafkaMicroBatchInputPartitionReader </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-KafkaSourceInitialOffsetWriter/ class=md-nav__link> KafkaSourceInitialOffsetWriter </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3-2-14 type=checkbox id=nav-3-2-14> <label class=md-nav__link for=nav-3-2-14> KafkaContinuousReader <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=KafkaContinuousReader data-md-level=3> <label class=md-nav__title for=nav-3-2-14> <span class="md-nav__icon md-icon"></span> KafkaContinuousReader </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-KafkaContinuousReader/ class=md-nav__link> KafkaContinuousReader </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-KafkaContinuousInputPartition/ class=md-nav__link> KafkaContinuousInputPartition </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3-3 type=checkbox id=nav-3-3> <label class=md-nav__link for=nav-3-3> Text Socket Data Source <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Text Socket Data Source" data-md-level=2> <label class=md-nav__title for=nav-3-3> <span class="md-nav__icon md-icon"></span> Text Socket Data Source </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-TextSocketSourceProvider/ class=md-nav__link> TextSocketSourceProvider </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-TextSocketSource/ class=md-nav__link> TextSocketSource </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3-4 type=checkbox id=nav-3-4> <label class=md-nav__link for=nav-3-4> Rate Data Source <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Rate Data Source" data-md-level=2> <label class=md-nav__title for=nav-3-4> <span class="md-nav__icon md-icon"></span> Rate Data Source </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-RateSourceProvider/ class=md-nav__link> RateSourceProvider </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-RateStreamProvider/ class=md-nav__link> RateStreamProvider </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-RateStreamSource/ class=md-nav__link> RateStreamSource </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-RateStreamMicroBatchReader/ class=md-nav__link> RateStreamMicroBatchReader </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3-5 type=checkbox id=nav-3-5> <label class=md-nav__link for=nav-3-5> Console Data Sink <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Console Data Sink" data-md-level=2> <label class=md-nav__title for=nav-3-5> <span class="md-nav__icon md-icon"></span> Console Data Sink </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-ConsoleSinkProvider/ class=md-nav__link> ConsoleSinkProvider </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-ConsoleWriter/ class=md-nav__link> ConsoleWriter </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3-6 type=checkbox id=nav-3-6> <label class=md-nav__link for=nav-3-6> Foreach Data Sink <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Foreach Data Sink" data-md-level=2> <label class=md-nav__title for=nav-3-6> <span class="md-nav__icon md-icon"></span> Foreach Data Sink </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-ForeachWriterProvider/ class=md-nav__link> ForeachWriterProvider </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-ForeachWriter/ class=md-nav__link> ForeachWriter </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-ForeachSink/ class=md-nav__link> ForeachSink </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3-7 type=checkbox id=nav-3-7> <label class=md-nav__link for=nav-3-7> ForeachBatch Data Sink <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="ForeachBatch Data Sink" data-md-level=2> <label class=md-nav__title for=nav-3-7> <span class="md-nav__icon md-icon"></span> ForeachBatch Data Sink </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-ForeachBatchSink/ class=md-nav__link> ForeachBatchSink </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3-8 type=checkbox id=nav-3-8> <label class=md-nav__link for=nav-3-8> Memory Data Source <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Memory Data Source" data-md-level=2> <label class=md-nav__title for=nav-3-8> <span class="md-nav__icon md-icon"></span> Memory Data Source </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-memory-data-source/ class=md-nav__link> Memory Data Source </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-MemoryStream/ class=md-nav__link> MemoryStream </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-ContinuousMemoryStream/ class=md-nav__link> ContinuousMemoryStream </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-MemorySink/ class=md-nav__link> MemorySink </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3-8-5 type=checkbox id=nav-3-8-5> <label class=md-nav__link for=nav-3-8-5> MemorySinkV2 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=MemorySinkV2 data-md-level=3> <label class=md-nav__title for=nav-3-8-5> <span class="md-nav__icon md-icon"></span> MemorySinkV2 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-MemorySinkV2/ class=md-nav__link> MemorySinkV2 </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-MemoryStreamWriter/ class=md-nav__link> MemoryStreamWriter </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-MemoryStreamBase/ class=md-nav__link> MemoryStreamBase </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-MemorySinkBase/ class=md-nav__link> MemorySinkBase </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4 type=checkbox id=nav-4> <label class=md-nav__link for=nav-4> Monitoring <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Monitoring data-md-level=1> <label class=md-nav__title for=nav-4> <span class="md-nav__icon md-icon"></span> Monitoring </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../monitoring/StreamingQueryListener/ class=md-nav__link> StreamingQueryListener </a> </li> <li class=md-nav__item> <a href=../monitoring/ProgressReporter/ class=md-nav__link> ProgressReporter </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-3 type=checkbox id=nav-4-3> <label class=md-nav__link for=nav-4-3> StreamingQueryProgress <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=StreamingQueryProgress data-md-level=2> <label class=md-nav__title for=nav-4-3> <span class="md-nav__icon md-icon"></span> StreamingQueryProgress </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../monitoring/StreamingQueryProgress/ class=md-nav__link> StreamingQueryProgress </a> </li> <li class=md-nav__item> <a href=../monitoring/StateOperatorProgress/ class=md-nav__link> StateOperatorProgress </a> </li> <li class=md-nav__item> <a href=../monitoring/ExecutionStats/ class=md-nav__link> ExecutionStats </a> </li> <li class=md-nav__item> <a href=../monitoring/SourceProgress/ class=md-nav__link> SourceProgress </a> </li> <li class=md-nav__item> <a href=../monitoring/SinkProgress/ class=md-nav__link> SinkProgress </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../monitoring/StreamingQueryStatus/ class=md-nav__link> StreamingQueryStatus </a> </li> <li class=md-nav__item> <a href=../monitoring/MetricsReporter/ class=md-nav__link> MetricsReporter </a> </li> <li class=md-nav__item> <a href=../logging/ class=md-nav__link> Logging </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-5 type=checkbox id=nav-5> <label class=md-nav__link for=nav-5> Web UI <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Web UI" data-md-level=1> <label class=md-nav__title for=nav-5> <span class="md-nav__icon md-icon"></span> Web UI </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../webui/ class=md-nav__link> Web UI </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-6 type=checkbox id=nav-6> <label class=md-nav__link for=nav-6> Demos <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Demos data-md-level=1> <label class=md-nav__title for=nav-6> <span class="md-nav__icon md-icon"></span> Demos </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../demo/spark-sql-streaming-demo-FlatMapGroupsWithStateExec/ class=md-nav__link> Internals of FlatMapGroupsWithStateExec Physical Operator </a> </li> <li class=md-nav__item> <a href=../demo/arbitrary-stateful-streaming-aggregation-flatMapGroupsWithState/ class=md-nav__link> Arbitrary Stateful Streaming Aggregation with KeyValueGroupedDataset.flatMapGroupsWithState Operator </a> </li> <li class=md-nav__item> <a href=../demo/exploring-checkpointed-state/ class=md-nav__link> Exploring Checkpointed State </a> </li> <li class=md-nav__item> <a href=../demo/watermark-aggregation-append/ class=md-nav__link> Streaming Watermark with Aggregation in Append Output Mode </a> </li> <li class=md-nav__item> <a href=../demo/groupBy-running-count-complete/ class=md-nav__link> Streaming Query for Running Counts (Socket Source and Complete Output Mode) </a> </li> <li class=md-nav__item> <a href=../demo/kafka-data-source/ class=md-nav__link> Streaming Aggregation with Kafka Data Source </a> </li> <li class=md-nav__item> <a href=../demo/groupByKey-count-Update/ class=md-nav__link> groupByKey Streaming Aggregation in Update Mode </a> </li> <li class=md-nav__item> <a href=../demo/StateStoreSaveExec-Complete/ class=md-nav__link> StateStoreSaveExec with Complete Output Mode </a> </li> <li class=md-nav__item> <a href=../demo/StateStoreSaveExec-Update/ class=md-nav__link> StateStoreSaveExec with Update Output Mode </a> </li> <li class=md-nav__item> <a href=../demo/custom-sink-webui/ class=md-nav__link> Developing Custom Streaming Sink (and Monitoring SQL Queries in web UI) </a> </li> <li class=md-nav__item> <a href=../demo/current_timestamp/ class=md-nav__link> current_timestamp Function For Processing Time in Streaming Queries </a> </li> <li class=md-nav__item> <a href=../demo/StreamingQueryManager-awaitAnyTermination-resetTerminated/ class=md-nav__link> Using StreamingQueryManager for Query Termination Management </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#sourcescala class=md-nav__link> [source,scala] </a> </li> <li class=md-nav__item> <a href=#basedir-path class=md-nav__link> baseDir: Path </a> </li> <li class=md-nav__item> <a href=#source-scala class=md-nav__link> [source, scala] </a> </li> <li class=md-nav__item> <a href=#tostring-string class=md-nav__link> toString: String </a> </li> <li class=md-nav__item> <a href=#source-scala_1 class=md-nav__link> [source, scala] </a> </li> <li class=md-nav__item> <a href=#source-scala_2 class=md-nav__link> [source, scala] </a> </li> <li class=md-nav__item> <a href=#deltafileversion-long-path class=md-nav__link> deltaFile(version: Long): Path </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <a href=https://github.com/jaceklaskowski/spark-structured-streaming-book/edit/mkdocs-material/docs/spark-sql-streaming-HDFSBackedStateStoreProvider.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg> </a> <p>== [[HDFSBackedStateStoreProvider]] HDFSBackedStateStoreProvider -- Hadoop DFS-based StateStoreProvider</p> <p><code>HDFSBackedStateStoreProvider</code> is a &lt;<spark-sql-streaming-statestoreprovider.md#, statestoreprovider>&gt; that uses a Hadoop DFS-compatible file system for &lt;<basedir, versioned state checkpointing>&gt;.</p> <p><code>HDFSBackedStateStoreProvider</code> is the default <code>StateStoreProvider</code> per the &lt;<spark-sql-streaming-properties.md#spark.sql.streaming.statestore.providerclass, spark.sql.streaming.statestore.providerclass>&gt; internal configuration property.</p> <p><code>HDFSBackedStateStoreProvider</code> is &lt;<creating-instance, created>&gt; and immediately requested to &lt;<init, initialize>&gt; when <code>StateStoreProvider</code> utility is requested to &lt;<spark-sql-streaming-statestoreprovider.md#createandinit, create and initialize a statestoreprovider>&gt;. That is when <code>HDFSBackedStateStoreProvider</code> is given the &lt;<statestoreid, statestoreid>&gt; that uniquely identifies the &lt;<spark-sql-streaming-statestore.md#, state store>&gt; to use for a stateful operator and a partition.</p> <p><code>HDFSStateStoreProvider</code> uses &lt;<spark-sql-streaming-hdfsbackedstatestore.md#, hdfsbackedstatestores>&gt; to manage state (&lt;<getstore, one per version>&gt;).</p> <p><code>HDFSBackedStateStoreProvider</code> manages versioned state in delta and snapshot files (and uses a &lt;<loadedmaps, cache>&gt; internally for faster access to state versions).</p> <p>[[creating-instance]] <code>HDFSBackedStateStoreProvider</code> takes no arguments to be created.</p> <p>[[logging]] [TIP] ==== Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <div class=highlight><pre><span></span><code>log4j.logger.org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider=ALL
</code></pre></div> <h1 id=refer-to>Refer to &lt;<spark-sql-streaming-logging.md#, logging>&gt;.<a class=headerlink href=#refer-to title="Permanent link">&para;</a></h1> <p>=== [[metrics]] Performance Metrics</p> <p>[cols="30,70",options="header",width="100%"] |=== | Name (in web UI) | Description</p> <p>| memoryUsedBytes a| [[memoryUsedBytes]] Estimated size of the &lt;<loadedmaps, loadedmaps>&gt; internal registry</p> <p>| count of cache hit on states cache in provider a| [[metricLoadedMapCacheHit]][[loadedMapCacheHitCount]] The number of times &lt;<loadmap, loading the specified version of state>&gt; was successful and found (<em>hit</em>) the requested state version in the &lt;<loadedmaps, loadedmaps>&gt; internal cache</p> <p>| count of cache miss on states cache in provider a| [[metricLoadedMapCacheMiss]][[loadedMapCacheMissCount]] The number of times &lt;<loadmap, loading the specified version of state>&gt; could not find (<em>missed</em>) the requested state version in the &lt;<loadedmaps, loadedmaps>&gt; internal cache</p> <p>| estimated size of state only on current version a| [[metricStateOnCurrentVersionSizeBytes]][[stateOnCurrentVersionSizeBytes]] Estimated size of the &lt;<spark-sql-streaming-hdfsbackedstatestore.md#maptoupdate, current state>&gt; (of the &lt;<spark-sql-streaming-hdfsbackedstatestore.md#, hdfsbackedstatestore>&gt;)</p> <p>|===</p> <p>=== [[baseDir]] State Checkpoint Base Directory -- <code>baseDir</code> Lazy Internal Property</p> <h2 id=sourcescala>[source,scala]<a class=headerlink href=#sourcescala title="Permanent link">&para;</a></h2> <h2 id=basedir-path>baseDir: Path<a class=headerlink href=#basedir-path title="Permanent link">&para;</a></h2> <p><code>baseDir</code> is the base directory (as Hadoop DFS's <a href=https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path>https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path</a>]) for &lt;<spark-sql-streaming-offsets-and-metadata-checkpointing.md#, state checkpointing>&gt; (for &lt;<deltafile, delta>&gt; and &lt;<snapshotfile, snapshot>&gt; state files).</p> <p><code>baseDir</code> is initialized lazily since it is not yet known when <code>HDFSBackedStateStoreProvider</code> is &lt;<creating-instance, created>&gt;.</p> <p><code>baseDir</code> is initialized and created based on the &lt;<spark-sql-streaming-statestoreid.md#storecheckpointlocation, state checkpoint base directory>&gt; of the &lt;<statestoreid, statestoreid>&gt; when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;<init, initialize>&gt;.</p> <p>=== [[stateStoreId]][[stateStoreId_]] StateStoreId -- Unique Identifier of State Store</p> <p>As a &lt;<spark-sql-streaming-statestoreprovider.md#, statestoreprovider>&gt;, <code>HDFSBackedStateStoreProvider</code> is associated with a &lt;<spark-sql-streaming-statestoreprovider.md#statestoreid, statestoreid>&gt; (which is a unique identifier of the &lt;<spark-sql-streaming-statestore.md#, state store>&gt; for a stateful operator and a partition).</p> <p><code>HDFSBackedStateStoreProvider</code> is given the &lt;<statestoreid, statestoreid>&gt; at &lt;<init, initialization>&gt; (as requested by the &lt;<spark-sql-streaming-statestoreprovider.md#, statestoreprovider>&gt; contract).</p> <p>The &lt;<statestoreid, statestoreid>&gt; is then used for the following:</p> <ul> <li> <p><code>HDFSBackedStateStore</code> is requested for the &lt;<spark-sql-streaming-hdfsbackedstatestore.md#id, id>&gt;</p> </li> <li> <p><code>HDFSBackedStateStoreProvider</code> is requested for the &lt;<tostring, textual representation>&gt; and the &lt;<basedir, state checkpoint base directory>&gt;</p> </li> </ul> <p>=== [[toString]] Textual Representation -- <code>toString</code> Method</p> <h2 id=source-scala>[source, scala]<a class=headerlink href=#source-scala title="Permanent link">&para;</a></h2> <h2 id=tostring-string>toString: String<a class=headerlink href=#tostring-string title="Permanent link">&para;</a></h2> <p>NOTE: <code>toString</code> is part of the ++<a href=https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object>https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object</a>] contract for the string representation of the object.</p> <p><code>HDFSBackedStateStoreProvider</code> uses the &lt;<statestoreid, statestoreid>&gt; and the &lt;<basedir, state checkpoint base directory>&gt; for the textual representation:</p> <div class=highlight><pre><span></span><code>HDFSStateStoreProvider[id = (op=[operatorId],part=[partitionId]),dir = [baseDir]]
</code></pre></div> <p>=== [[getStore]] Loading Specified Version of State (Store) For Update -- <code>getStore</code> Method</p> <h2 id=source-scala_1>[source, scala]<a class=headerlink href=#source-scala_1 title="Permanent link">&para;</a></h2> <p>getStore( version: Long): StateStore</p> <hr> <p>NOTE: <code>getStore</code> is part of the &lt;<spark-sql-streaming-statestoreprovider.md#getstore, statestoreprovider contract>&gt; for the &lt;<spark-sql-streaming-statestore.md#, statestore>&gt; for a specified version.</p> <p><code>getStore</code> creates a new empty state (<code>ConcurrentHashMap[UnsafeRow, UnsafeRow]</code>) and &lt;<loadmap, loads the specified version of state (from internal cache or snapshot and delta files)>&gt; for versions greater than <code>0</code>.</p> <p>In the end, <code>getStore</code> creates a new &lt;<spark-sql-streaming-hdfsbackedstatestore.md#, hdfsbackedstatestore>&gt; for the specified version with the new state and prints out the following INFO message to the logs:</p> <div class=highlight><pre><span></span><code>Retrieved version [version] of [this] for update
</code></pre></div> <p><code>getStore</code> throws an <code>IllegalArgumentException</code> when the specified version is less than <code>0</code> (negative):</p> <div class=highlight><pre><span></span><code>Version cannot be less than 0
</code></pre></div> <p>=== [[deltaFile]] <code>deltaFile</code> Internal Method</p> <h2 id=source-scala_2>[source, scala]<a class=headerlink href=#source-scala_2 title="Permanent link">&para;</a></h2> <h2 id=deltafileversion-long-path>deltaFile(version: Long): Path<a class=headerlink href=#deltafileversion-long-path title="Permanent link">&para;</a></h2> <p><code>deltaFile</code> simply returns the Hadoop <a href=https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path>https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path</a>] of the <code>[version].delta</code> file in the &lt;<basedir, state checkpoint base directory>&gt;.</p> <h1 id=note>[NOTE]<a class=headerlink href=#note title="Permanent link">&para;</a></h1> <p><code>deltaFile</code> is used when:</p> <ul> <li>&lt;<spark-sql-streaming-hdfsbackedstatestore.md#, hdfsbackedstatestore>&gt; is created (and creates the &lt;<finaldeltafile, final delta file>&gt;)</li> </ul> <h1 id=hdfsbackedstatestoreprovider-is-requested-to>* <code>HDFSBackedStateStoreProvider</code> is requested to &lt;<updatefromdeltafile, updatefromdeltafile>&gt;<a class=headerlink href=#hdfsbackedstatestoreprovider-is-requested-to title="Permanent link">&para;</a></h1> <p>=== [[snapshotFile]] <code>snapshotFile</code> Internal Method</p> <h2 id=source-scala_3>[source, scala]<a class=headerlink href=#source-scala_3 title="Permanent link">&para;</a></h2> <h2 id=snapshotfileversion-long-path>snapshotFile(version: Long): Path<a class=headerlink href=#snapshotfileversion-long-path title="Permanent link">&para;</a></h2> <p><code>snapshotFile</code> simply returns the Hadoop <a href=https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path>https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path</a>] of the <code>[version].snapshot</code> file in the &lt;<basedir, state checkpoint base directory>&gt;.</p> <p>NOTE: <code>snapshotFile</code> is used when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;<writesnapshotfile, writesnapshotfile>&gt; or &lt;<readsnapshotfile, readsnapshotfile>&gt;.</p> <p>=== [[fetchFiles]] Listing All Delta And Snapshot Files In State Checkpoint Directory -- <code>fetchFiles</code> Internal Method</p> <h2 id=source-scala_4>[source, scala]<a class=headerlink href=#source-scala_4 title="Permanent link">&para;</a></h2> <h2 id=fetchfiles-seqstorefile>fetchFiles(): Seq[StoreFile]<a class=headerlink href=#fetchfiles-seqstorefile title="Permanent link">&para;</a></h2> <p><code>fetchFiles</code> requests the &lt;<fm, checkpointfilemanager>&gt; for &lt;<spark-sql-streaming-checkpointfilemanager.md#list, all the files>&gt; in the &lt;<basedir, state checkpoint directory>&gt;.</p> <p>For every file, <code>fetchFiles</code> splits the name into two parts with <code>.</code> (dot) as a separator (files with more or less than two parts are simply ignored) and registers a new <code>StoreFile</code> for <code>snapshot</code> and <code>delta</code> files:</p> <ul> <li> <p>For <code>snapshot</code> files, <code>fetchFiles</code> creates a new <code>StoreFile</code> with <code>isSnapshot</code> flag on (<code>true</code>)</p> </li> <li> <p>For <code>delta</code> files, <code>fetchFiles</code> creates a new <code>StoreFile</code> with <code>isSnapshot</code> flag off (<code>false</code>)</p> </li> </ul> <p>NOTE: <code>delta</code> files are only registered if there was no <code>snapshot</code> file for the version.</p> <p><code>fetchFiles</code> prints out the following WARN message to the logs for any other files:</p> <div class=highlight><pre><span></span><code>Could not identify file [path] for [this]
</code></pre></div> <p>In the end, <code>fetchFiles</code> sorts the <code>StoreFiles</code> based on their version, prints out the following DEBUG message to the logs, and returns the files.</p> <div class=highlight><pre><span></span><code>Current set of files for [this]: [storeFiles]
</code></pre></div> <p>NOTE: <code>fetchFiles</code> is used when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;<dosnapshot, dosnapshot>&gt; and &lt;<cleanup, cleanup>&gt;.</p> <p>=== [[init]] Initializing StateStoreProvider -- <code>init</code> Method</p> <h2 id=source-scala_5>[source, scala]<a class=headerlink href=#source-scala_5 title="Permanent link">&para;</a></h2> <p>init( stateStoreId: StateStoreId, keySchema: StructType, valueSchema: StructType, indexOrdinal: Option[Int], storeConf: StateStoreConf, hadoopConf: Configuration): Unit</p> <hr> <p>NOTE: <code>init</code> is part of the &lt;<spark-sql-streaming-statestoreprovider.md#init, statestoreprovider contract>&gt; to initialize itself.</p> <p><code>init</code> records the values of the input arguments as the &lt;<statestoreid, statestoreid>&gt;, &lt;<keyschema, keyschema>&gt;, &lt;<valueschema, valueschema>&gt;, &lt;<storeconf, storeconf>&gt;, and &lt;<hadoopconf, hadoopconf>&gt; internal properties.</p> <p><code>init</code> requests the given <code>StateStoreConf</code> for the &lt;<spark-sql-streaming-statestoreconf.md#maxversionstoretaininmemory, spark.sql.streaming.maxbatchestoretaininmemory>&gt; configuration property (that is then recorded in the &lt;<numberofversionstoretaininmemory, numberofversionstoretaininmemory>&gt; internal property).</p> <p>In the end, <code>init</code> requests the &lt;<fm, checkpointfilemanager>&gt; to &lt;<spark-sql-streaming-checkpointfilemanager.md#mkdirs, create>&gt; the &lt;<basedir, basedir>&gt; directory (with parent directories).</p> <p>=== [[filesForVersion]] Finding Snapshot File and Delta Files For Version -- <code>filesForVersion</code> Internal Method</p> <h2 id=source-scala_6>[source, scala]<a class=headerlink href=#source-scala_6 title="Permanent link">&para;</a></h2> <p>filesForVersion( allFiles: Seq[StoreFile], version: Long): Seq[StoreFile]</p> <hr> <p><code>filesForVersion</code> finds the latest snapshot version among the given <code>allFiles</code> files up to and including the given version (it may or may not be available).</p> <p>If a snapshot file was found (among the given file up to and including the given version), <code>filesForVersion</code> takes all delta files between the version of the snapshot file (exclusive) and the given version (inclusive) from the given <code>allFiles</code> files.</p> <p>NOTE: The number of delta files should be the given version minus the snapshot version.</p> <p>If a snapshot file was not found, <code>filesForVersion</code> takes all delta files up to the given version (inclusive) from the given <code>allFiles</code> files.</p> <p>In the end, <code>filesForVersion</code> returns a snapshot version (if available) and all delta files up to the given version (inclusive).</p> <p>NOTE: <code>filesForVersion</code> is used when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;<dosnapshot, dosnapshot>&gt; and &lt;<cleanup, cleanup>&gt;.</p> <p>=== [[doMaintenance]] State Maintenance (Snapshotting and Cleaning Up) -- <code>doMaintenance</code> Method</p> <h2 id=source-scala_7>[source, scala]<a class=headerlink href=#source-scala_7 title="Permanent link">&para;</a></h2> <h2 id=domaintenance-unit>doMaintenance(): Unit<a class=headerlink href=#domaintenance-unit title="Permanent link">&para;</a></h2> <p>NOTE: <code>doMaintenance</code> is part of the &lt;<spark-sql-streaming-statestoreprovider.md#domaintenance, statestoreprovider contract>&gt; for optional state maintenance.</p> <p><code>doMaintenance</code> simply does &lt;<dosnapshot, state snapshoting>&gt; followed by &lt;<cleanup, cleaning up (removing old state files)>&gt;.</p> <p>In case of any non-fatal errors, <code>doMaintenance</code> simply prints out the following WARN message to the logs:</p> <div class=highlight><pre><span></span><code>Error performing snapshot and cleaning up [this]
</code></pre></div> <p>==== [[doSnapshot]] State Snapshoting (Rolling Up Delta Files into Snapshot File) -- <code>doSnapshot</code> Internal Method</p> <h2 id=source-scala_8>[source, scala]<a class=headerlink href=#source-scala_8 title="Permanent link">&para;</a></h2> <h2 id=dosnapshot-unit>doSnapshot(): Unit<a class=headerlink href=#dosnapshot-unit title="Permanent link">&para;</a></h2> <p><code>doSnapshot</code> &lt;<fetchfiles, lists all delta and snapshot files in the state checkpoint directory>&gt; (<code>files</code>) and prints out the following DEBUG message to the logs:</p> <div class=highlight><pre><span></span><code>fetchFiles() took [time] ms.
</code></pre></div> <p><code>doSnapshot</code> returns immediately (and does nothing) when there are no delta and snapshot files.</p> <p><code>doSnapshot</code> takes the version of the latest file (<code>lastVersion</code>).</p> <p><code>doSnapshot</code> &lt;<filesforversion, finds the snapshot file and delta files for the version>&gt; (among the files and for the last version).</p> <p><code>doSnapshot</code> looks up the last version in the &lt;<loadedmaps, internal state cache>&gt;.</p> <p>When the last version was found in the cache and the number of delta files is above &lt;<spark-sql-streaming-properties.md#spark.sql.streaming.statestore.mindeltasforsnapshot, spark.sql.streaming.statestore.mindeltasforsnapshot>&gt; internal threshold, <code>doSnapshot</code> &lt;<writesnapshotfile, writes a compressed snapshot file for the last version>&gt;.</p> <p>In the end, <code>doSnapshot</code> prints out the following DEBUG message to the logs:</p> <div class=highlight><pre><span></span><code>writeSnapshotFile() took [time] ms.
</code></pre></div> <p>In case of non-fatal errors, <code>doSnapshot</code> simply prints out the following WARN message to the logs:</p> <div class=highlight><pre><span></span><code>Error doing snapshots for [this]
</code></pre></div> <p>NOTE: <code>doSnapshot</code> is used exclusively when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;<domaintenance, do state maintenance (state snapshotting and cleaning up)>&gt;.</p> <p>==== [[cleanup]] Cleaning Up (Removing Old State Files) -- <code>cleanup</code> Internal Method</p> <h2 id=source-scala_9>[source, scala]<a class=headerlink href=#source-scala_9 title="Permanent link">&para;</a></h2> <h2 id=cleanup-unit>cleanup(): Unit<a class=headerlink href=#cleanup-unit title="Permanent link">&para;</a></h2> <p><code>cleanup</code> &lt;<fetchfiles, lists all delta and snapshot files in the state checkpoint directory>&gt; (<code>files</code>) and prints out the following DEBUG message to the logs:</p> <div class=highlight><pre><span></span><code>fetchFiles() took [time] ms.
</code></pre></div> <p><code>cleanup</code> returns immediately (and does nothing) when there are no delta and snapshot files.</p> <p><code>cleanup</code> takes the version of the latest state file (<code>lastVersion</code>) and decrements it by &lt;<spark-sql-streaming-properties.md#spark.sql.streaming.minbatchestoretain, spark.sql.streaming.minbatchestoretain>&gt; configuration property (default: <code>100</code>) that gives the earliest version to retain (and all older state files to be removed).</p> <p><code>cleanup</code> requests the &lt;<fm, checkpointfilemanager>&gt; to &lt;<spark-sql-streaming-checkpointfilemanager.md#delete, delete the path>&gt; of every old state file.</p> <p><code>cleanup</code> prints out the following DEBUG message to the logs:</p> <div class=highlight><pre><span></span><code>deleting files took [time] ms.
</code></pre></div> <p>In the end, <code>cleanup</code> prints out the following INFO message to the logs:</p> <div class=highlight><pre><span></span><code>Deleted files older than [version] for [this]: [filesToDelete]
</code></pre></div> <p>In case of a non-fatal exception, <code>cleanup</code> prints out the following WARN message to the logs:</p> <div class=highlight><pre><span></span><code>Error cleaning up files for [this]
</code></pre></div> <p>NOTE: <code>cleanup</code> is used exclusively when <code>HDFSBackedStateStoreProvider</code> is requested for &lt;<domaintenance, state maintenance (state snapshotting and cleaning up)>&gt;.</p> <p>=== [[close]] Closing State Store Provider -- <code>close</code> Method</p> <h2 id=source-scala_10>[source, scala]<a class=headerlink href=#source-scala_10 title="Permanent link">&para;</a></h2> <h2 id=close-unit>close(): Unit<a class=headerlink href=#close-unit title="Permanent link">&para;</a></h2> <p>NOTE: <code>close</code> is part of the &lt;<spark-sql-streaming-statestoreprovider.md#close, statestoreprovider contract>&gt; to close the state store provider.</p> <p><code>close</code>...FIXME</p> <p>=== [[getMetricsForProvider]] <code>getMetricsForProvider</code> Method</p> <h2 id=source-scala_11>[source, scala]<a class=headerlink href=#source-scala_11 title="Permanent link">&para;</a></h2> <h2 id=getmetricsforprovider-mapstring-long>getMetricsForProvider(): Map[String, Long]<a class=headerlink href=#getmetricsforprovider-mapstring-long title="Permanent link">&para;</a></h2> <p><code>getMetricsForProvider</code> returns the following &lt;<metrics, performance metrics>&gt;:</p> <ul> <li> <p>&lt;<memoryusedbytes, memoryusedbytes>&gt;</p> </li> <li> <p>&lt;<metricloadedmapcachehit, metricloadedmapcachehit>&gt;</p> </li> <li> <p>&lt;<metricloadedmapcachemiss, metricloadedmapcachemiss>&gt;</p> </li> </ul> <p>NOTE: <code>getMetricsForProvider</code> is used exclusively when <code>HDFSBackedStateStore</code> is requested for &lt;<spark-sql-streaming-hdfsbackedstatestore.md#metrics, performance metrics>&gt;.</p> <p>=== [[supportedCustomMetrics]] Supported StateStoreCustomMetrics -- <code>supportedCustomMetrics</code> Method</p> <h2 id=source-scala_12>[source, scala]<a class=headerlink href=#source-scala_12 title="Permanent link">&para;</a></h2> <h2 id=supportedcustommetrics-seqstatestorecustommetric>supportedCustomMetrics: Seq[StateStoreCustomMetric]<a class=headerlink href=#supportedcustommetrics-seqstatestorecustommetric title="Permanent link">&para;</a></h2> <p>NOTE: <code>supportedCustomMetrics</code> is part of the &lt;<spark-sql-streaming-statestoreprovider.md#supportedcustommetrics, statestoreprovider contract>&gt; for the &lt;<spark-sql-streaming-statestorecustommetric.md#, statestorecustommetrics>&gt; of a state store provider.</p> <p><code>supportedCustomMetrics</code> includes the following &lt;<spark-sql-streaming-statestorecustommetric.md#, statestorecustommetrics>&gt;:</p> <ul> <li> <p>&lt;<metricstateoncurrentversionsizebytes, metricstateoncurrentversionsizebytes>&gt;</p> </li> <li> <p>&lt;<metricloadedmapcachehit, metricloadedmapcachehit>&gt;</p> </li> <li> <p>&lt;<metricloadedmapcachemiss, metricloadedmapcachemiss>&gt;</p> </li> </ul> <p>=== [[commitUpdates]] Committing State Changes (As New Version of State) -- <code>commitUpdates</code> Internal Method</p> <h2 id=source-scala_13>[source, scala]<a class=headerlink href=#source-scala_13 title="Permanent link">&para;</a></h2> <p>commitUpdates( newVersion: Long, map: ConcurrentHashMap[UnsafeRow, UnsafeRow], output: DataOutputStream): Unit</p> <hr> <p><code>commitUpdates</code> &lt;<finalizedeltafile, finalizedeltafile>&gt; (with the given <code>DataOutputStream</code>) followed by &lt;<putstateintostatecachemap, caching the new version of state>&gt; (with the given <code>newVersion</code> and the <code>map</code> state).</p> <p>NOTE: <code>commitUpdates</code> is used exclusively when <code>HDFSBackedStateStore</code> is requested to &lt;<spark-sql-streaming-hdfsbackedstatestore.md#commit, commit state changes>&gt;.</p> <p>=== [[loadMap]] Loading Specified Version of State (from Internal Cache or Snapshot and Delta Files) -- <code>loadMap</code> Internal Method</p> <h2 id=source-scala_14>[source, scala]<a class=headerlink href=#source-scala_14 title="Permanent link">&para;</a></h2> <p>loadMap( version: Long): ConcurrentHashMap[UnsafeRow, UnsafeRow]</p> <hr> <p><code>loadMap</code> firstly tries to find the state version in the &lt;<loadedmaps, loadedmaps>&gt; internal cache and, if found, returns it immediately and increments the &lt;<loadedmapcachehitcount, loadedmapcachehitcount>&gt; metric.</p> <p>If the requested state version could not be found in the &lt;<loadedmaps, loadedmaps>&gt; internal cache, <code>loadMap</code> prints out the following WARN message to the logs:</p> <h2 id=optionswrap>[options="wrap"]<a class=headerlink href=#optionswrap title="Permanent link">&para;</a></h2> <h2 id=the-state-for-version-version-doesnt-exist-in-loadedmaps-reading-snapshot-file-and-delta-files-if-needednote-that-this-is-normal-for-the-first-batch-of-starting-query>The state for version [version] doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.<a class=headerlink href=#the-state-for-version-version-doesnt-exist-in-loadedmaps-reading-snapshot-file-and-delta-files-if-needednote-that-this-is-normal-for-the-first-batch-of-starting-query title="Permanent link">&para;</a></h2> <p><code>loadMap</code> increments the &lt;<loadedmapcachemisscount, loadedmapcachemisscount>&gt; metric.</p> <p><code>loadMap</code> &lt;<readsnapshotfile, tries to load the state snapshot file for the version>&gt; and, if found, &lt;<putstateintostatecachemap, puts the version of state in the internal cache>&gt; and returns it.</p> <p>If not found, <code>loadMap</code> tries to find the most recent state version by decrementing the requested version until one is found in the &lt;<loadedmaps, loadedmaps>&gt; internal cache or &lt;<readsnapshotfile, loaded from a state snapshot (file)>&gt;.</p> <p><code>loadMap</code> &lt;<updatefromdeltafile, updatefromdeltafile>&gt; for all the remaining versions (from the snapshot version up to the requested one). <code>loadMap</code> &lt;<putstateintostatecachemap, puts the final version of state in the internal cache>&gt; (the closest snapshot and the remaining delta versions) and returns it.</p> <p>In the end, <code>loadMap</code> prints out the following DEBUG message to the logs:</p> <div class=highlight><pre><span></span><code>Loading state for [version] takes [elapsedMs] ms.
</code></pre></div> <p>NOTE: <code>loadMap</code> is used exclusively when <code>HDFSBackedStateStoreProvider</code> is requested for the &lt;<getstore, specified version of a state store for update>&gt;.</p> <p>=== [[readSnapshotFile]] Loading State Snapshot File For Specified Version -- <code>readSnapshotFile</code> Internal Method</p> <h2 id=source-scala_15>[source, scala]<a class=headerlink href=#source-scala_15 title="Permanent link">&para;</a></h2> <p>readSnapshotFile( version: Long): Option[ConcurrentHashMap[UnsafeRow, UnsafeRow]]</p> <hr> <p><code>readSnapshotFile</code> &lt;<snapshotfile, creates the path of the snapshot file>&gt; for the given <code>version</code>.</p> <p><code>readSnapshotFile</code> requests the &lt;<fm, checkpointfilemanager>&gt; to &lt;<spark-sql-streaming-checkpointfilemanager.md#open, open the snapshot file for reading>&gt; and &lt;<decompressstream, creates a decompressed datainputstream>&gt; (<code>input</code>).</p> <p><code>readSnapshotFile</code> reads the decompressed input stream until an EOF (that is marked as the integer <code>-1</code> in the stream) and inserts key and value rows in a state map (<code>ConcurrentHashMap[UnsafeRow, UnsafeRow]</code>):</p> <ul> <li> <p>First integer is the size of a key (buffer) followed by the key itself (of the size). <code>readSnapshotFile</code> creates an <code>UnsafeRow</code> for the key (with the number of fields as indicated by the number of fields of the &lt;<keyschema, key schema>&gt;).</p> </li> <li> <p>Next integer is the size of a value (buffer) followed by the value itself (of the size). <code>readSnapshotFile</code> creates an <code>UnsafeRow</code> for the value (with the number of fields as indicated by the number of fields of the &lt;<valueschema, value schema>&gt;).</p> </li> </ul> <p>In the end, <code>readSnapshotFile</code> prints out the following INFO message to the logs and returns the key-value map.</p> <div class=highlight><pre><span></span><code>Read snapshot file for version [version] of [this] from [fileToRead]
</code></pre></div> <p>In case of <code>FileNotFoundException</code> <code>readSnapshotFile</code> simply returns <code>None</code> (to indicate no snapshot state file was available and so no state for the version).</p> <p><code>readSnapshotFile</code> throws an <code>IOException</code> for the size of a key or a value below <code>0</code>:</p> <div class=highlight><pre><span></span><code>Error reading snapshot file [fileToRead] of [this]: [key|value] size cannot be [keySize|valueSize]
</code></pre></div> <p>NOTE: <code>readSnapshotFile</code> is used exclusively when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;<loadmap, load the specified version of state (from the internal cache or snapshot and delta files)>&gt;.</p> <p>=== [[updateFromDeltaFile]] Updating State with State Changes For Specified Version (per Delta File) -- <code>updateFromDeltaFile</code> Internal Method</p> <h2 id=source-scala_16>[source, scala]<a class=headerlink href=#source-scala_16 title="Permanent link">&para;</a></h2> <p>updateFromDeltaFile( version: Long, map: ConcurrentHashMap[UnsafeRow, UnsafeRow]): Unit</p> <hr> <h1 id=note_1>[NOTE]<a class=headerlink href=#note_1 title="Permanent link">&para;</a></h1> <p><code>updateFromDeltaFile</code> is very similar code-wise to &lt;<readsnapshotfile, readsnapshotfile>&gt; with the two main differences:</p> <ul> <li> <p><code>updateFromDeltaFile</code> is given the state map to update (while &lt;<readsnapshotfile, readsnapshotfile>&gt; loads the state from a snapshot file)</p> </li> <li> <p><code>updateFromDeltaFile</code> removes a key from the state map when the value (size) is <code>-1</code> (while &lt;<readsnapshotfile, readsnapshotfile>&gt; throws an <code>IOException</code>)</p> </li> </ul> <h1 id=the-following-description-is-almost-an-exact-copy-of-just-for-completeness>The following description is almost an exact copy of &lt;<readsnapshotfile, readsnapshotfile>&gt; just for completeness.<a class=headerlink href=#the-following-description-is-almost-an-exact-copy-of-just-for-completeness title="Permanent link">&para;</a></h1> <p><code>updateFromDeltaFile</code> &lt;<deltafile, creates the path of the delta file>&gt; for the requested <code>version</code>.</p> <p><code>updateFromDeltaFile</code> requests the &lt;<fm, checkpointfilemanager>&gt; to &lt;<spark-sql-streaming-checkpointfilemanager.md#open, open the delta file for reading>&gt; and &lt;<decompressstream, creates a decompressed datainputstream>&gt; (<code>input</code>).</p> <p><code>updateFromDeltaFile</code> reads the decompressed input stream until an EOF (that is marked as the integer <code>-1</code> in the stream) and inserts key and value rows in the given state map:</p> <ul> <li> <p>First integer is the size of a key (buffer) followed by the key itself (of the size). <code>updateFromDeltaFile</code> creates an <code>UnsafeRow</code> for the key (with the number of fields as indicated by the number of fields of the &lt;<keyschema, key schema>&gt;).</p> </li> <li> <p>Next integer is the size of a value (buffer) followed by the value itself (of the size). <code>updateFromDeltaFile</code> creates an <code>UnsafeRow</code> for the value (with the number of fields as indicated by the number of fields of the &lt;<valueschema, value schema>&gt;) or removes the corresponding key from the state map (if the value size is <code>-1</code>)</p> </li> </ul> <p>NOTE: <code>updateFromDeltaFile</code> removes the key-value entry from the state map if the value (size) is <code>-1</code>.</p> <p>In the end, <code>updateFromDeltaFile</code> prints out the following INFO message to the logs and returns the key-value map.</p> <div class=highlight><pre><span></span><code>Read delta file for version [version] of [this] from [fileToRead]
</code></pre></div> <p><code>updateFromDeltaFile</code> throws an <code>IllegalStateException</code> in case of <code>FileNotFoundException</code> while opening the delta file for the specified version:</p> <div class=highlight><pre><span></span><code>Error reading delta file [fileToRead] of [this]: [fileToRead] does not exist
</code></pre></div> <p>NOTE: <code>updateFromDeltaFile</code> is used exclusively when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;<loadmap, load the specified version of state (from the internal cache or snapshot and delta files)>&gt;.</p> <p>=== [[putStateIntoStateCacheMap]] Caching New Version of State -- <code>putStateIntoStateCacheMap</code> Internal Method</p> <h2 id=source-scala_17>[source, scala]<a class=headerlink href=#source-scala_17 title="Permanent link">&para;</a></h2> <p>putStateIntoStateCacheMap( newVersion: Long, map: ConcurrentHashMap[UnsafeRow, UnsafeRow]): Unit</p> <hr> <p><code>putStateIntoStateCacheMap</code> registers state for a given version, i.e. adds the <code>map</code> state under the <code>newVersion</code> key in the &lt;<loadedmaps, loadedmaps>&gt; internal registry.</p> <p>With the &lt;<numberofversionstoretaininmemory, numberofversionstoretaininmemory>&gt; threshold as <code>0</code> or below, <code>putStateIntoStateCacheMap</code> simply removes all entries from the &lt;<loadedmaps, loadedmaps>&gt; internal registry and returns.</p> <p><code>putStateIntoStateCacheMap</code> removes the oldest state version(s) in the &lt;<loadedmaps, loadedmaps>&gt; internal registry until its size is at the &lt;<numberofversionstoretaininmemory, numberofversionstoretaininmemory>&gt; threshold.</p> <p>With the size of the &lt;<loadedmaps, loadedmaps>&gt; internal registry is at the &lt;<numberofversionstoretaininmemory, numberofversionstoretaininmemory>&gt; threshold, <code>putStateIntoStateCacheMap</code> does two more optimizations per <code>newVersion</code></p> <ul> <li> <p>It does not add the given state when the version of the oldest state is earlier (larger) than the given <code>newVersion</code></p> </li> <li> <p>It removes the oldest state when older (smaller) than the given <code>newVersion</code></p> </li> </ul> <p>NOTE: <code>putStateIntoStateCacheMap</code> is used when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;<commitupdates, commit state (as a new version)>&gt; and &lt;<loadmap, load the specified version of state (from the internal cache or snapshot and delta files)>&gt;.</p> <p>=== [[writeSnapshotFile]] Writing Compressed Snapshot File for Specified Version -- <code>writeSnapshotFile</code> Internal Method</p> <h2 id=source-scala_18>[source, scala]<a class=headerlink href=#source-scala_18 title="Permanent link">&para;</a></h2> <p>writeSnapshotFile( version: Long, map: ConcurrentHashMap[UnsafeRow, UnsafeRow]): Unit</p> <hr> <p><code>writeSnapshotFile</code> &lt;<snapshotfile, snapshotfile>&gt; for the given version.</p> <p><code>writeSnapshotFile</code> requests the &lt;<fm, checkpointfilemanager>&gt; to &lt;<spark-sql-streaming-checkpointfilemanager.md#createatomic, create the snapshot file>&gt; (with overwriting enabled) and &lt;<compressstream, compress the stream>&gt;.</p> <p>For every key-value <code>UnsafeRow</code> pair in the given map, <code>writeSnapshotFile</code> writes the size of the key followed by the key itself (as bytes). <code>writeSnapshotFile</code> then writes the size of the value followed by the value itself (as bytes).</p> <p>In the end, <code>writeSnapshotFile</code> prints out the following INFO message to the logs:</p> <div class=highlight><pre><span></span><code>Written snapshot file for version [version] of [this] at [targetFile]
</code></pre></div> <p>In case of any <code>Throwable</code> exception, <code>writeSnapshotFile</code> &lt;<canceldeltafile, canceldeltafile>&gt; and re-throws the exception.</p> <p>NOTE: <code>writeSnapshotFile</code> is used exclusively when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;<dosnapshot, dosnapshot>&gt;.</p> <p>=== [[compressStream]] <code>compressStream</code> Internal Method</p> <h2 id=source-scala_19>[source, scala]<a class=headerlink href=#source-scala_19 title="Permanent link">&para;</a></h2> <p>compressStream( outputStream: DataOutputStream): DataOutputStream</p> <hr> <p><code>compressStream</code> creates a new <code>LZ4CompressionCodec</code> (based on the &lt;<sparkconf, sparkconf>&gt;) and requests it to create a <code>LZ4BlockOutputStream</code> with the given <code>DataOutputStream</code>.</p> <p>In the end, <code>compressStream</code> creates a new <code>DataOutputStream</code> with the <code>LZ4BlockOutputStream</code>.</p> <p>NOTE: <code>compressStream</code> is used when...FIXME</p> <p>=== [[cancelDeltaFile]] <code>cancelDeltaFile</code> Internal Method</p> <h2 id=source-scala_20>[source, scala]<a class=headerlink href=#source-scala_20 title="Permanent link">&para;</a></h2> <p>cancelDeltaFile( compressedStream: DataOutputStream, rawStream: CancellableFSDataOutputStream): Unit</p> <hr> <p><code>cancelDeltaFile</code>...FIXME</p> <p>NOTE: <code>cancelDeltaFile</code> is used when...FIXME</p> <p>=== [[finalizeDeltaFile]] <code>finalizeDeltaFile</code> Internal Method</p> <h2 id=source-scala_21>[source, scala]<a class=headerlink href=#source-scala_21 title="Permanent link">&para;</a></h2> <p>finalizeDeltaFile( output: DataOutputStream): Unit</p> <hr> <p><code>finalizeDeltaFile</code> simply writes <code>-1</code> to the given <code>DataOutputStream</code> (to indicate end of file) and closes it.</p> <p>NOTE: <code>finalizeDeltaFile</code> is used exclusively when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;<commitupdates, commit state changes (a new version of state)>&gt;.</p> <p>=== [[loadedMaps]] Lookup Table (Cache) of States By Version -- <code>loadedMaps</code> Internal Method</p> <h2 id=source-scala_22>[source, scala]<a class=headerlink href=#source-scala_22 title="Permanent link">&para;</a></h2> <p>loadedMaps: TreeMap[ Long, // version ConcurrentHashMap[UnsafeRow, UnsafeRow]] // state (as keys and values)</p> <hr> <p><code>loadedMaps</code> is a <a href=https://docs.oracle.com/javase/8/docs/api/java/util/TreeMap.html[java.util.TreeMap>https://docs.oracle.com/javase/8/docs/api/java/util/TreeMap.html[java.util.TreeMap</a>] of state versions sorted according to the reversed ordering of the versions (i.e. long numbers).</p> <p>A new entry (a version and the state updates) can only be added when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;<putstateintostatecachemap, putstateintostatecachemap>&gt; (and only when the &lt;<spark-sql-streaming-properties.md#spark.sql.streaming.maxbatchestoretaininmemory, spark.sql.streaming.maxbatchestoretaininmemory>&gt; internal configuration is above <code>0</code>).</p> <p><code>loadedMaps</code> is mainly used when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;<loadmap, load the specified version of state (from the internal cache or snapshot and delta files)>&gt;. Positive hits (when a version could be found in the cache) is available as the &lt;<loadedmapcachehitcount, count of cache hit on states cache in provider>&gt; performance metric while misses are counted in the &lt;<loadedmapcachemisscount, count of cache miss on states cache in provider>&gt; performance metric.</p> <p>NOTE: With no or missing versions in cache &lt;<loadedmapcachemisscount, count of cache miss on states cache in provider>&gt; metric should be above <code>0</code> while &lt;<loadedmapcachehitcount, count of cache hit on states cache in provider>&gt; always <code>0</code> (or smaller than the other metric).</p> <p>The estimated size of <code>loadedMaps</code> is available as the &lt;<memoryusedbytes, memoryusedbytes>&gt; performance metric.</p> <p>The &lt;<spark-sql-streaming-properties.md#spark.sql.streaming.maxbatchestoretaininmemory, spark.sql.streaming.maxbatchestoretaininmemory>&gt; internal configuration is used as the threshold of the number of elements in <code>loadedMaps</code>. When <code>0</code> or negative, every &lt;<putstateintostatecachemap, putstateintostatecachemap>&gt; removes all elements in (<em>clears</em>) <code>loadedMaps</code>.</p> <p>NOTE: It is possible to change the configuration at restart of a structured query.</p> <p>The state deltas (the values) in <code>loadedMaps</code> are cleared (all entries removed) when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;<close, close>&gt;.</p> <p>Used when <code>HDFSBackedStateStoreProvider</code> is requested for the following:</p> <ul> <li> <p>&lt;<putstateintostatecachemap, cache a version of state>&gt;</p> </li> <li> <p>&lt;<loadmap, loading the specified version of state (from the internal cache or snapshot and delta files)>&gt;</p> </li> </ul> <p>=== [[internal-properties]] Internal Properties</p> <p>[cols="30m,70",options="header",width="100%"] |=== | Name | Description</p> <p>| fm a| [[fm]] &lt;<spark-sql-streaming-checkpointfilemanager.md#, checkpointfilemanager>&gt; for the &lt;<basedir, state checkpoint base directory>&gt; (and the &lt;<hadoopconf, hadoop configuration>&gt;)</p> <p>Used when:</p> <ul> <li> <p>Creating a new &lt;<spark-sql-streaming-hdfsbackedstatestore.md#, hdfsbackedstatestore>&gt; (to create the &lt;<spark-sql-streaming-hdfsbackedstatestore.md#deltafilestream, cancellablefsdataoutputstream>&gt; for the &lt;<spark-sql-streaming-hdfsbackedstatestore.md#finaldeltafile, finaldeltafile>&gt;)</p> </li> <li> <p><code>HDFSBackedStateStoreProvider</code> is requested to &lt;<init, initialize>&gt; (to create the &lt;<basedir, state checkpoint base directory>&gt;), &lt;<updatefromdeltafile, updatefromdeltafile>&gt;, &lt;<writesnapshotfile, write the compressed snapshot file for a specified state version>&gt;, &lt;<readsnapshotfile, readsnapshotfile>&gt;, &lt;<cleanup, clean up>&gt;, and &lt;<fetchfiles, list all delta and snapshot files in the state checkpoint directory>&gt;</p> </li> </ul> <p>| hadoopConf a| [[hadoopConf]] Hadoop <a href=https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration>https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration</a>] of the &lt;<fm, checkpointfilemanager>&gt;</p> <p>Given when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;<init, initialize>&gt;</p> <p>| keySchema a| [[keySchema]]</p> <h2 id=source-scala_23>[source, scala]<a class=headerlink href=#source-scala_23 title="Permanent link">&para;</a></h2> <h2 id=keyschema-structtype>keySchema: StructType<a class=headerlink href=#keyschema-structtype title="Permanent link">&para;</a></h2> <p>Schema of the state keys</p> <p>| valueSchema a| [[valueSchema]]</p> <h2 id=source-scala_24>[source, scala]<a class=headerlink href=#source-scala_24 title="Permanent link">&para;</a></h2> <h2 id=valueschema-structtype>valueSchema: StructType<a class=headerlink href=#valueschema-structtype title="Permanent link">&para;</a></h2> <p>Schema of the state values</p> <p>| numberOfVersionsToRetainInMemory a| [[numberOfVersionsToRetainInMemory]]</p> <h2 id=source-scala_25>[source, scala]<a class=headerlink href=#source-scala_25 title="Permanent link">&para;</a></h2> <h2 id=numberofversionstoretaininmemory-int>numberOfVersionsToRetainInMemory: Int<a class=headerlink href=#numberofversionstoretaininmemory-int title="Permanent link">&para;</a></h2> <p><code>numberOfVersionsToRetainInMemory</code> is the maximum number of entries in the &lt;<loadedmaps, loadedmaps>&gt; internal registry and is configured by the &lt;<spark-sql-streaming-properties.md#spark.sql.streaming.maxbatchestoretaininmemory, spark.sql.streaming.maxbatchestoretaininmemory>&gt; internal configuration.</p> <p><code>numberOfVersionsToRetainInMemory</code> is a threshold when <code>HDFSBackedStateStoreProvider</code> removes the last key from the &lt;<loadedmaps, loadedmaps>&gt; internal registry (per reverse ordering of state versions) when requested to &lt;<putstateintostatecachemap, putstateintostatecachemap>&gt;.</p> <p>| sparkConf a| [[sparkConf]] <code>SparkConf</code></p> <p>|===</p> </article> </div> </div> </main> <footer class=md-footer> <div class=md-footer-nav> <nav class="md-footer-nav__inner md-grid" aria-label=Footer> <a href=../spark-sql-streaming-StateStoreProviderId/ class="md-footer-nav__link md-footer-nav__link--prev" rel=prev> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </div> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Previous </span> StateStoreProviderId </div> </div> </a> <a href=../spark-sql-streaming-StateStoreCoordinator/ class="md-footer-nav__link md-footer-nav__link--next" rel=next> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Next </span> StateStoreCoordinator </div> </div> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg> </div> </a> </nav> </div> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2020 <a href=https://twitter.com/jaceklaskowski target=_blank rel=noopener>Jacek Laskowski</a> </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-footer-social> <a href=https://github.com/jaceklaskowski target=_blank rel=noopener title=github.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> <a href=https://twitter.com/jaceklaskowski target=_blank rel=noopener title=twitter.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg> </a> <a href=https://linkedin.com/in/jaceklaskowski target=_blank rel=noopener title=linkedin.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg> </a> </div> </div> </div> </footer> </div> <script src=../assets/javascripts/vendor.77e55a48.min.js></script> <script src=../assets/javascripts/bundle.9554a270.min.js></script><script id=__lang type=application/json>{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}</script> <script>
        app = initialize({
          base: "..",
          features: ['navigation.tabs', 'navigation.instant'],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.4ac00218.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script> </body> </html>