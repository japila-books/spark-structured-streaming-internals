== Demo: Streaming Join and StreamingSymmetricHashJoinExec Physical Operator

The following code shows a <<spark-sql-streaming-join.adoc#, streaming join>> of two streaming queries (and <<spark-sql-streaming-StreamingSymmetricHashJoinExec.adoc#, StreamingSymmetricHashJoinExec>> physical operator).

[source, scala]
----
// START: Only for easier debugging
// Reduce the number of partitions
// The state is then only for one partition
// which should make monitoring easier
val numShufflePartitions = 1
import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS
spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, numShufflePartitions)

assert(spark.sessionState.conf.numShufflePartitions == numShufflePartitions)
// END: Only for easier debugging

import java.sql.Timestamp
case class Event(time: Timestamp, value: Long)
import scala.concurrent.duration._
object Event {
  def apply(n: Long, value: Long): Event = {
    Event(new Timestamp(n.seconds.toMillis), value)
  }
}

import org.apache.spark.sql.execution.streaming.MemoryStream
implicit val sqlCtx = spark.sqlContext

val leftEvents = MemoryStream[Event]
leftEvents.addData(
  Event(1,1),
  Event(1,2),
  Event(3,3),
  Event(4,3),
  Event(5,2),
  Event(6,2),
  Event(10,9))
val left = leftEvents.toDS

val rightEvents = MemoryStream[Event]
rightEvents.addData(
  Event(1,1),
  Event(1,2),
  Event(3,3),
  Event(4,3),
  Event(5,2),
  Event(6,2),
  Event(10,9))
val right = rightEvents.toDS

// Streaming inner join
val equiStreamingJoin = left.join(right, Seq("value", "time"), "inner")
== Physical Plan ==
*(3) Project [value#3L, time#2]
+- StreamingSymmetricHashJoin [value#3L, time#2], [value#8L, time#7], Inner, condition = [ leftOnly = null, rightOnly = null, both = null, full = null ], state info [ checkpoint = <unknown>, runId = c079027b-b68d-4289-a96f-b3c860e76e28, opId = 0, ver = 0, numPartitions = 1], 0, state cleanup [ left = null, right = null ]
   :- Exchange hashpartitioning(value#3L, time#2, 1)
   :  +- *(1) Filter isnotnull(time#2)
   :     +- StreamingRelation MemoryStream[time#2,value#3L], [time#2, value#3L]
   +- Exchange hashpartitioning(value#8L, time#7, 1)
      +- *(2) Filter isnotnull(time#7)
         +- StreamingRelation MemoryStream[time#7,value#8L], [time#7, value#8L]

val queryName = "stream_stream_inner_join"
val checkpointLocation = s"/tmp/checkpoint-$queryName"

// Delete the checkpoint location from previous executions
import java.nio.file.{Files, FileSystems}
import java.util.Comparator
import scala.collection.JavaConverters._
val path = FileSystems.getDefault.getPath(checkpointLocation)
if (Files.exists(path)) {
  Files.walk(path)
    .sorted(Comparator.reverseOrder())
    .iterator
    .asScala
    .foreach(p => p.toFile.delete)
}

import org.apache.spark.sql.streaming.OutputMode.Append
import org.apache.spark.sql.streaming.Trigger
import scala.concurrent.duration._
val batchInterval = 30.seconds
val streamingQuery = equiStreamingJoin
  .writeStream
  .format("memory")
  .queryName(queryName)
  .option("checkpointLocation", checkpointLocation)
  .outputMode(Append)
  .trigger(Trigger.ProcessingTime(batchInterval))
  .start

streamingQuery.processAllAvailable()

val output = spark.table(queryName).orderBy("value", "time")
output.show(truncate = false)
/**
+-----+-------------------+
|value|time               |
+-----+-------------------+
|1    |1970-01-01 01:00:01|
|2    |1970-01-01 01:00:01|
|2    |1970-01-01 01:00:05|
|2    |1970-01-01 01:00:06|
|3    |1970-01-01 01:00:03|
|3    |1970-01-01 01:00:04|
|9    |1970-01-01 01:00:10|
+-----+-------------------+
*/
assert(output.collect.size == 7)

scala> streamingQuery.explain
== Physical Plan ==
*(3) Project [value#68L, time#67]
+- StreamingSymmetricHashJoin [value#68L, time#67], [value#66L, time#65], Inner, condition = [ leftOnly = null, rightOnly = null, both = null, full = null ], state info [ checkpoint = file:/tmp/checkpoint-stream_stream_inner_join/state, runId = 7adaf1c4-c0a5-471d-aa8e-11c08c039de4, opId = 0, ver = 0, numPartitions = 1], 0, state cleanup [ left = null, right = null ]
   :- *(1) Filter isnotnull(time#67)
   :  +- *(1) Project [time#67, value#68L]
   :     +- *(1) ScanV2 MemoryStreamDataSource$[time#67, value#68L]
   +- *(2) Filter isnotnull(time#65)
      +- *(2) Project [time#65, value#66L]
         +- *(2) ScanV2 MemoryStreamDataSource$[time#65, value#66L]

// Eventually...
streamingQuery.stop()
----
