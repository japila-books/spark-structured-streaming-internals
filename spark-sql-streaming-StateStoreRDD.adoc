== [[StateStoreRDD]] StateStoreRDD -- RDD for Updating State (in StateStores Across Spark Cluster)

`StateStoreRDD` is an `RDD` for <<compute, executing storeUpdateFunction>> with link:spark-sql-streaming-StateStore.adoc[StateStore] (and data from partitions of a <<dataRDD, new batch RDD>>).

`StateStoreRDD` is <<creating-instance, created>> when the following stateful physical operators are executed (using <<spark-sql-streaming-StateStoreOps.adoc#mapPartitionsWithStateStore, StateStoreOps.mapPartitionsWithStateStore>>):

* <<spark-sql-streaming-FlatMapGroupsWithStateExec.adoc#, FlatMapGroupsWithStateExec>>
* <<spark-sql-streaming-StateStoreRestoreExec.adoc#, StateStoreRestoreExec>>
* <<spark-sql-streaming-StateStoreSaveExec.adoc#, StateStoreSaveExec>>
* <<spark-sql-streaming-StreamingDeduplicateExec.adoc#, StreamingDeduplicateExec>>
* <<spark-sql-streaming-StreamingGlobalLimitExec.adoc#, StreamingGlobalLimitExec>>

.StateStoreRDD, Physical and Logical Plans, and operators
image::images/StateStoreRDD-SparkPlans-LogicalPlans-operators.png[align="center"]

`StateStoreRDD` uses `StateStoreCoordinator` for <<getPreferredLocations, preferred locations>> for job scheduling.

.StateStoreRDD and StateStoreCoordinator
image::images/StateStoreRDD-StateStoreCoordinator.png[align="center"]

[[getPartitions]]
`getPartitions` is exactly the partitions of the <<dataRDD, data RDD>>.

[[internal-registries]]
.StateStoreRDD's Internal Registries and Counters
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[hadoopConfBroadcast]] `hadoopConfBroadcast`
|

| [[storeConf]] `storeConf`
| Configuration parameters (as `StateStoreConf`) using the current `SQLConf` (from `SessionState`)
|===

=== [[compute]] Computing Partition (in TaskContext) -- `compute` Method

[source, scala]
----
compute(partition: Partition, ctxt: TaskContext): Iterator[U]
----

NOTE: `compute` is a part of the RDD Contract to compute a given partition in a `TaskContext`.

`compute` computes <<dataRDD, dataRDD>> passing the result on to <<storeUpdateFunction, storeUpdateFunction>> (with a configured link:spark-sql-streaming-StateStore.adoc[StateStore]).

Internally, (and similarly to <<getPreferredLocations, getPreferredLocations>>) `compute` creates a `StateStoreProviderId` with `StateStoreId` (using <<checkpointLocation, checkpointLocation>>, <<operatorId, operatorId>> and the index of the input `partition`) and <<queryRunId, queryRunId>>.

`compute` then requests `StateStore` for link:spark-sql-streaming-StateStore.adoc#get[the store for the StateStoreProviderId].

In the end, `compute` computes <<dataRDD, dataRDD>> (using the input `partition` and `ctxt`) followed by executing <<storeUpdateFunction, storeUpdateFunction>> (with the store and the result).

=== [[getPreferredLocations]] Getting Placement Preferences of Partition -- `getPreferredLocations` Method

[source, scala]
----
getPreferredLocations(partition: Partition): Seq[String]
----

NOTE: `getPreferredLocations` is a part of the RDD Contract to specify placement preferences (aka _preferred task locations_), i.e. where tasks should be executed to be as close to the data as possible.

`getPreferredLocations` creates a `StateStoreProviderId` with `StateStoreId` (using <<checkpointLocation, checkpointLocation>>, <<operatorId, operatorId>> and the index of the input `partition`) and <<queryRunId, queryRunId>>.

NOTE: <<checkpointLocation, checkpointLocation>> and <<operatorId, operatorId>> are shared across different partitions and so the only difference in `StateStoreProviderIds` is the partition index.

In the end, `getPreferredLocations` requests <<storeCoordinator, StateStoreCoordinatorRef>> for the link:spark-sql-streaming-StateStoreCoordinatorRef.adoc#getLocation[location of the state store] for `StateStoreProviderId`.

NOTE: link:spark-sql-streaming-StateStoreCoordinator.adoc[StateStoreCoordinator] coordinates instances of `StateStores` across Spark executors in the cluster, and tracks their locations for job scheduling.

=== [[creating-instance]] Creating StateStoreRDD Instance

`StateStoreRDD` takes the following when created:

* [[dataRDD]] `RDD` with the new streaming batch data (to update the aggregates in a state store)
* [[storeUpdateFunction]] Store update function (i.e. `(StateStore, Iterator[T]) => Iterator[U]` with `T` being the type of the new batch data)
* [[checkpointLocation]] The path to the checkpoint location
* [[queryRunId]] `queryRunId`
* [[operatorId]] Operator id
* [[storeVersion]] Store version
* [[keySchema]] Schema of the keys
* [[valueSchema]] Schema of the values
* [[indexOrdinal]] Optional index
* [[sessionState]] `SessionState`
* [[storeCoordinator]] Optional link:spark-sql-streaming-StateStoreCoordinatorRef.adoc[StateStoreCoordinatorRef]

`StateStoreRDD` initializes the <<internal-registries, internal registries and counters>>.
