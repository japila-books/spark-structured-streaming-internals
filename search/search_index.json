{"config":{"lang":["en"],"separator":"[\\s\\-]+"},"docs":[{"title":"The Internals of Spark Structured Streaming (Apache Spark 3.3.0)","text":"<p>Welcome to The Internals of Spark Structured Streaming online book! \ud83e\udd19</p> <p>I'm Jacek Laskowski, an IT freelancer specializing in Apache Spark, Delta Lake and Apache Kafka (incl. Kafka Streams and ksqlDB) (with brief forays into a wider data engineering space, e.g. Trino, Dask and dbt, mostly during Warsaw Data Engineering meetups).</p> <p>I'm very excited to have you here and hope you will enjoy exploring the internals of Spark Structured Streaming as much as I have.</p>  <p>Flannery O'Connor</p> <p>I write to discover what I know.</p>   \"The Internals Of\" series <p>I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page.</p>  <p>Expect text and code snippets from a variety of public sources. Attribution follows.</p> <p>Now, let's take a deep dive into Spark Structured Streaming \ud83d\udd25</p>  <p>Last update: 2022-10-10</p>","location":""},{"title":"AcceptsLatestSeenOffsetHandler","text":"<p><code>AcceptsLatestSeenOffsetHandler</code> is...FIXME</p>","location":"AcceptsLatestSeenOffsetHandler/"},{"title":"CheckpointFileManager","text":"<p><code>CheckpointFileManager</code> is an abstraction of checkpoint managers that manage checkpoint files (metadata of streaming batches) on Hadoop DFS-compatible file systems.</p> <p><code>CheckpointFileManager</code> is created based on spark.sql.streaming.checkpointFileManagerClass configuration property if defined before reverting to the available checkpoint managers.</p>","location":"CheckpointFileManager/"},{"title":"Contract (Subset)","text":"","location":"CheckpointFileManager/#contract-subset"},{"title":"createAtomic <pre><code>createAtomic(\n  path: Path,\n  overwriteIfPossible: Boolean): CancellableFSDataOutputStream\n</code></pre> <p>Used when:</p> <ul> <li><code>HDFSMetadataLog</code> is requested to addNewBatchByStream</li> <li><code>StreamMetadata</code> is requested to write a metadata to a file</li> <li><code>HDFSBackedStateStoreProvider</code> is requested for the deltaFileStream and writeSnapshotFile</li> <li><code>RocksDBFileManager</code> is requested for the zipToDfsFile</li> <li><code>StateSchemaCompatibilityChecker</code> is requested for the createSchemaFile</li> </ul>","text":"","location":"CheckpointFileManager/#createatomic"},{"title":"createCheckpointDirectory <pre><code>createCheckpointDirectory(): Path\n</code></pre> <p>Creates the checkpoint path</p> <p>Used when:</p> <ul> <li><code>ResolveWriteToStream</code> is requested to <code>resolveCheckpointLocation</code></li> </ul>","text":"","location":"CheckpointFileManager/#createcheckpointdirectory"},{"title":"Implementations","text":"<ul> <li>FileContextBasedCheckpointFileManager</li> <li>FileSystemBasedCheckpointFileManager</li> </ul>","location":"CheckpointFileManager/#implementations"},{"title":"Creating CheckpointFileManager <pre><code>create(\n  path: Path,\n  hadoopConf: Configuration): CheckpointFileManager\n</code></pre> <p><code>create</code> uses spark.sql.streaming.checkpointFileManagerClass as the name of the implementation to instantiate.</p> <p>If undefined, <code>create</code> creates a FileContextBasedCheckpointFileManager first, and, in case of an <code>UnsupportedFileSystemException</code>, falls back to a FileSystemBasedCheckpointFileManager.</p> <p><code>create</code> prints out the following WARN message to the logs if <code>UnsupportedFileSystemException</code> happens:</p> <pre><code>Could not use FileContext API for managing Structured Streaming checkpoint files at [path].\nUsing FileSystem API instead for managing log files.\nIf the implementation of FileSystem.rename() is not atomic, then the correctness and fault-tolerance of your Structured Streaming is not guaranteed.\n</code></pre>  <p><code>create</code> is used when:</p> <ul> <li><code>HDFSMetadataLog</code> is requested for the fileManager</li> <li><code>StreamExecution</code> is requested for the fileManager</li> <li><code>ResolveWriteToStream</code> is requested to <code>resolveCheckpointLocation</code></li> <li><code>StreamMetadata</code> is requested to read the metadata from a file and write a metadata to a file</li> <li><code>HDFSBackedStateStoreProvider</code> is requested for the fm</li> <li><code>RocksDBFileManager</code> is requested for the fm</li> <li><code>StateSchemaCompatibilityChecker</code> is requested for the fm</li> </ul>","text":"","location":"CheckpointFileManager/#creating-checkpointfilemanager"},{"title":"CommitLog \u2014 HDFSMetadataLog for Offset Commit Log","text":"<p><code>CommitLog</code> is an HDFSMetadataLog with CommitMetadata metadata.</p> <p><code>CommitLog</code> is the offset commit log of streaming query execution engines.</p> <p>[[CommitMetadata]][[nextBatchWatermarkMs]] <code>CommitLog</code> uses <code>CommitMetadata</code> for the metadata with nextBatchWatermarkMs attribute (of type <code>Long</code> and the default <code>0</code>).</p> <p><code>CommitLog</code> &lt;&gt; commit metadata to files with names that are offsets. <pre><code>$ ls -tr [checkpoint-directory]/commits\n0 1 2 3 4 5 6 7 8 9\n\n$ cat [checkpoint-directory]/commits/8\nv1\n{\"nextBatchWatermarkMs\": 0}\n</code></pre> <p>[[VERSION]] <code>CommitLog</code> uses 1 for the version.</p> <p>[[creating-instance]] <code>CommitLog</code> (like the parent HDFSMetadataLog) takes the following to be created:</p> <ul> <li>[[sparkSession]] <code>SparkSession</code></li> <li>[[path]] Path of the metadata log directory</li> </ul> <p>=== [[serialize]] Serializing Metadata (Writing Metadata to Persistent Storage) -- <code>serialize</code> Method</p>","location":"CommitLog/"},{"title":"[source, scala]","text":"<p>serialize(   metadata: CommitMetadata,   out: OutputStream): Unit</p>  <p><code>serialize</code> writes out the &lt;&gt; prefixed with <code>v</code> on a single line (e.g. <code>v1</code>) followed by the given <code>CommitMetadata</code> in JSON format. <p><code>serialize</code> is part of HDFSMetadataLog abstraction.</p> <p>=== [[deserialize]] Deserializing Metadata -- <code>deserialize</code> Method</p>","location":"CommitLog/#source-scala"},{"title":"[source, scala]","text":"","location":"CommitLog/#source-scala_1"},{"title":"deserialize(in: InputStream): CommitMetadata","text":"<p><code>deserialize</code> simply reads (deserializes) two lines from the given <code>InputStream</code> for version and the &lt;&gt; attribute. <p><code>deserialize</code> is part of HDFSMetadataLog abstraction.</p> <p>=== [[add-batchId]] <code>add</code> Method</p>","location":"CommitLog/#deserializein-inputstream-commitmetadata"},{"title":"[source, scala]","text":"","location":"CommitLog/#source-scala_2"},{"title":"add(batchId: Long): Unit","text":"<p><code>add</code>...FIXME</p> <p>NOTE: <code>add</code> is used when...FIXME</p> <p>=== [[add-batchId-metadata]] <code>add</code> Method</p>","location":"CommitLog/#addbatchid-long-unit"},{"title":"[source, scala]","text":"","location":"CommitLog/#source-scala_3"},{"title":"add(batchId: Long, metadata: String): Boolean","text":"<p><code>add</code>...FIXME</p> <p><code>add</code> is part of MetadataLog abstraction.</p>","location":"CommitLog/#addbatchid-long-metadata-string-boolean"},{"title":"CommitMetadata","text":"<p>== [[CommitMetadata]] CommitMetadata</p> <p><code>CommitMetadata</code> is...FIXME</p>","location":"CommitMetadata/"},{"title":"ContinuousDataSourceRDD","text":"<p><code>ContinuousDataSourceRDD</code> is a specialized <code>RDD</code> (<code>RDD[InternalRow]</code>) that is used exclusively for the only input RDD (with the input rows) of <code>DataSourceV2ScanExec</code> leaf physical operator with a ContinuousReader.</p> <p><code>ContinuousDataSourceRDD</code> is &lt;&gt; exclusively when <code>DataSourceV2ScanExec</code> leaf physical operator is requested for the input RDDs (which there is only one actually). <p>[[spark.sql.streaming.continuous.executorQueueSize]] <code>ContinuousDataSourceRDD</code> uses spark.sql.streaming.continuous.executorQueueSize configuration property for the &lt;&gt;. <p>[[spark.sql.streaming.continuous.executorPollIntervalMs]] <code>ContinuousDataSourceRDD</code> uses spark.sql.streaming.continuous.executorPollIntervalMs configuration property for the &lt;&gt;. <p>[[creating-instance]] <code>ContinuousDataSourceRDD</code> takes the following to be created:</p> <ul> <li>[[sc]] <code>SparkContext</code></li> <li>[[dataQueueSize]] Size of the data queue</li> <li>[[epochPollIntervalMs]] <code>epochPollIntervalMs</code></li> <li>[[readerInputPartitions]] <code>InputPartition[InternalRow]</code>s</li> </ul> <p>[[getPreferredLocations]] <code>ContinuousDataSourceRDD</code> uses <code>InputPartition</code> (of a <code>ContinuousDataSourceRDDPartition</code>) for preferred host locations (where the input partition reader can run faster).</p> <p>=== [[compute]] Computing Partition -- <code>compute</code> Method</p>","location":"ContinuousDataSourceRDD/"},{"title":"[source, scala]","text":"<p>compute(   split: Partition,   context: TaskContext): Iterator[InternalRow]</p>  <p>NOTE: <code>compute</code> is part of the RDD Contract to compute a given partition.</p> <p><code>compute</code>...FIXME</p> <p>=== [[getPartitions]] <code>getPartitions</code> Method</p>","location":"ContinuousDataSourceRDD/#source-scala"},{"title":"[source, scala]","text":"","location":"ContinuousDataSourceRDD/#source-scala_1"},{"title":"getPartitions: Array[Partition]","text":"<p>NOTE: <code>getPartitions</code> is part of the <code>RDD</code> Contract to specify the partitions to &lt;&gt;. <p><code>getPartitions</code>...FIXME</p>","location":"ContinuousDataSourceRDD/#getpartitions-arraypartition"},{"title":"ContinuousQueuedDataReader","text":"<p>== [[ContinuousQueuedDataReader]] ContinuousQueuedDataReader</p> <p><code>ContinuousQueuedDataReader</code> is &lt;&gt; exclusively when <code>ContinuousDataSourceRDD</code> is requested to &lt;&gt;. <p>[[ContinuousRecord]] <code>ContinuousQueuedDataReader</code> uses two types of continuous records:</p> <ul> <li>[[EpochMarker]] <code>EpochMarker</code></li> <li>[[ContinuousRow]] <code>ContinuousRow</code> (with the <code>InternalRow</code> at <code>PartitionOffset</code>)</li> </ul> <p>=== [[next]] Fetching Next Row -- <code>next</code> Method</p>","location":"ContinuousQueuedDataReader/"},{"title":"[source, scala]","text":"","location":"ContinuousQueuedDataReader/#source-scala"},{"title":"next(): InternalRow","text":"<p><code>next</code>...FIXME</p> <p>NOTE: <code>next</code> is used when...FIXME</p> <p>=== [[close]] Closing ContinuousQueuedDataReader -- <code>close</code> Method</p>","location":"ContinuousQueuedDataReader/#next-internalrow"},{"title":"[source, scala]","text":"","location":"ContinuousQueuedDataReader/#source-scala_1"},{"title":"close(): Unit","text":"<p>NOTE: <code>close</code> is part of the https://docs.oracle.com/javase/8/docs/api/java/io/Closeable.html[java.io.Closeable] to close this stream and release any system resources associated with it.</p> <p><code>close</code>...FIXME</p> <p>=== [[creating-instance]] Creating ContinuousQueuedDataReader Instance</p> <p><code>ContinuousQueuedDataReader</code> takes the following to be created:</p> <ul> <li>[[partition]] <code>ContinuousDataSourceRDDPartition</code></li> <li>[[context]] <code>TaskContext</code></li> <li>[[dataQueueSize]] Size of the &lt;&gt; <li>[[epochPollIntervalMs]] <code>epochPollIntervalMs</code></li>  <p><code>ContinuousQueuedDataReader</code> initializes the &lt;&gt;. <p>=== [[internal-properties]] Internal Properties</p> <p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| coordinatorId a| [[coordinatorId]] Epoch Coordinator Identifier</p> <p>Used when...FIXME</p> <p>| currentOffset a| [[currentOffset]] <code>PartitionOffset</code></p> <p>Used when...FIXME</p> <p>| dataReaderThread a| [[dataReaderThread]] &lt;&gt; daemon thread that is created and started immediately when <code>ContinuousQueuedDataReader</code> is &lt;&gt; <p>Used when...FIXME</p> <p>| epochCoordEndpoint a| [[epochCoordEndpoint]] <code>RpcEndpointRef</code> of the &lt;&gt; per &lt;&gt; <p>Used when...FIXME</p> <p>| epochMarkerExecutor a| [[epochMarkerExecutor]] https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ScheduledExecutorService.html[java.util.concurrent.ScheduledExecutorService]</p> <p>Used when...FIXME</p> <p>| epochMarkerGenerator a| [[epochMarkerGenerator]] &lt;&gt; <p>Used when...FIXME</p> <p>| reader a| [[reader]] <code>InputPartitionReader</code></p> <p>Used when...FIXME</p> <p>| queue a| [[queue]] https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ArrayBlockingQueue.html[java.util.concurrent.ArrayBlockingQueue] of &lt;&gt; (of the given &lt;&gt;) <p>Used when...FIXME</p> <p>|===</p>","location":"ContinuousQueuedDataReader/#close-unit"},{"title":"ContinuousStream","text":"<p><code>ContinuousStream</code> is...FIXME</p>","location":"ContinuousStream/"},{"title":"ContinuousWriteRDD -- RDD of WriteToContinuousDataSourceExec Unary Physical Operator","text":"<p><code>ContinuousWriteRDD</code> is a specialized <code>RDD</code> (<code>RDD[Unit]</code>) that is used exclusively as the underlying RDD of <code>WriteToContinuousDataSourceExec</code> unary physical operator to &lt;&gt;. <p><code>ContinuousWriteRDD</code> is &lt;&gt; exclusively when <code>WriteToContinuousDataSourceExec</code> unary physical operator is requested to &lt;&gt;. <p>[[partitioner]] [[getPartitions]] <code>ContinuousWriteRDD</code> uses the &lt;&gt; for the partitions and the partitioner. <p>[[creating-instance]] <code>ContinuousWriteRDD</code> takes the following to be created:</p> <ul> <li>[[prev]] Parent RDD (<code>RDD[InternalRow]</code>)</li> <li>[[writeTask]] Write task (<code>DataWriterFactory[InternalRow]</code>)</li> </ul> <p>=== [[compute]] Computing Partition -- <code>compute</code> Method</p>","location":"ContinuousWriteRDD/"},{"title":"[source, scala]","text":"<p>compute(   split: Partition,   context: TaskContext): Iterator[Unit]</p>  <p>NOTE: <code>compute</code> is part of the <code>RDD</code> Contract to compute a partition.</p> <p><code>compute</code> requests the <code>EpochCoordinatorRef</code> helper for a &lt;&gt; (using the &lt;&gt;). <p>NOTE: The &lt;&gt; runs on the driver as the single point to coordinate epochs across partition tasks. <p><code>compute</code> uses the <code>EpochTracker</code> helper to &lt;&gt; (using the &lt;&gt; local property). <p>[[compute-loop]] <code>compute</code> then executes the following steps (in a loop) until the task (as the given <code>TaskContext</code>) is killed or completed.</p> <p><code>compute</code> requests the &lt;&gt; to compute the given partition (that gives an <code>Iterator[InternalRow]</code>). <p><code>compute</code> requests the &lt;&gt; to create a <code>DataWriter</code> (for the partition and the task attempt IDs from the given <code>TaskContext</code> and the &lt;&gt; from the <code>EpochTracker</code> helper) and requests it to write all records (from the <code>Iterator[InternalRow]</code>). <p><code>compute</code> prints out the following INFO message to the logs:</p> <pre><code>Writer for partition [partitionId] in epoch [epoch] is committing.\n</code></pre> <p><code>compute</code> requests the <code>DataWriter</code> to commit (that gives a <code>WriterCommitMessage</code>).</p> <p><code>compute</code> requests the EpochCoordinator RPC endpoint reference to send out a &lt;&gt; message (with the <code>WriterCommitMessage</code>). <p><code>compute</code> prints out the following INFO message to the logs:</p> <pre><code>Writer for partition [partitionId] in epoch [epoch] is committed.\n</code></pre> <p>In the end (of the loop), <code>compute</code> uses the <code>EpochTracker</code> helper to &lt;&gt;. <p>In case of an error, <code>compute</code> prints out the following ERROR message to the logs and requests the <code>DataWriter</code> to abort.</p> <pre><code>Writer for partition [partitionId] is aborting.\n</code></pre> <p>In the end, <code>compute</code> prints out the following ERROR message to the logs:</p> <pre><code>Writer for partition [partitionId] aborted.\n</code></pre>","location":"ContinuousWriteRDD/#source-scala"},{"title":"DataReaderThread","text":"<p>== [[DataReaderThread]] DataReaderThread</p> <p><code>DataReaderThread</code> is...FIXME</p>","location":"DataReaderThread/"},{"title":"DataSource","text":"<p>The Internals of Spark SQL</p> <p>Learn more about DataSource in The Internals of Spark SQL online book.</p>","location":"DataSource/"},{"title":"Creating Streaming Source (Data Source V1) <pre><code>createSource(\n  metadataPath: String): Source\n</code></pre> <p><code>createSource</code> creates a new instance of the data source class and branches off per the type:</p> <ul> <li>StreamSourceProvider</li> <li>FileFormat</li> <li>other types</li> </ul> <p><code>createSource</code> is used when:</p> <ul> <li><code>MicroBatchExecution</code> is requested for an analyzed logical plan</li> </ul>","text":"","location":"DataSource/#creating-streaming-source-data-source-v1"},{"title":"StreamSourceProvider <p>For a StreamSourceProvider, <code>createSource</code> requests the <code>StreamSourceProvider</code> to create a source.</p>","text":"","location":"DataSource/#streamsourceprovider"},{"title":"FileFormat <p>For a <code>FileFormat</code>, <code>createSource</code> creates a new FileStreamSource.</p> <p><code>createSource</code> throws an <code>IllegalArgumentException</code> when <code>path</code> option was not specified for a <code>FileFormat</code> data source:</p> <pre><code>'path' is not specified\n</code></pre>","text":"","location":"DataSource/#fileformat"},{"title":"Other Types <p>For any other data source type, <code>createSource</code> simply throws an <code>UnsupportedOperationException</code>:</p> <pre><code>Data source [className] does not support streamed reading\n</code></pre>","text":"","location":"DataSource/#other-types"},{"title":"SourceInfo <pre><code>sourceInfo: SourceInfo\n</code></pre> <p>Metadata of a Source with the following:</p> <ul> <li>Name (alias)</li> <li>Schema</li> <li>Partitioning columns</li> </ul> <p><code>sourceInfo</code> is initialized (lazily) using sourceSchema.</p>  Lazy Value <p><code>sourceInfo</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and cached afterwards.</p>  <p>Used when:</p> <ul> <li> <p><code>DataSource</code> is requested to create a streaming source for a File-Based Data Source (when <code>MicroBatchExecution</code> is requested to initialize the analyzed logical plan)</p> </li> <li> <p><code>StreamingRelation</code> utility is used to create a StreamingRelation (when <code>DataStreamReader</code> is requested for a streaming query)</p> </li> </ul>","text":"","location":"DataSource/#sourceinfo"},{"title":"Generating Metadata of Streaming Source <pre><code>sourceSchema(): SourceInfo\n</code></pre> <p><code>sourceSchema</code> creates a new instance of the data source class and branches off per the type:</p> <ul> <li>StreamSourceProvider</li> <li>FileFormat</li> <li>other types</li> </ul>  <p><code>sourceSchema</code> is used when:</p> <ul> <li><code>DataSource</code> is requested for the SourceInfo</li> </ul>","text":"","location":"DataSource/#generating-metadata-of-streaming-source"},{"title":"StreamSourceProvider <p>For a StreamSourceProvider, <code>sourceSchema</code> requests the <code>StreamSourceProvider</code> for the name and schema (of the streaming source).</p> <p>In the end, <code>sourceSchema</code> returns the name and the schema as part of <code>SourceInfo</code> (with partition columns unspecified).</p>","text":"","location":"DataSource/#streamsourceprovider_1"},{"title":"FileFormat <p>For a <code>FileFormat</code>, <code>sourceSchema</code>...FIXME</p>","text":"","location":"DataSource/#fileformat_1"},{"title":"Other Types <p>For any other data source type, <code>sourceSchema</code> simply throws an <code>UnsupportedOperationException</code>:</p> <pre><code>Data source [className] does not support streamed reading\n</code></pre>","text":"","location":"DataSource/#other-types_1"},{"title":"Creating Streaming Sink <pre><code>createSink(\n  outputMode: OutputMode): Sink\n</code></pre> <p><code>createSink</code> creates a streaming sink for StreamSinkProvider or <code>FileFormat</code> data sources.</p>  <p>The Internals of Spark SQL</p> <p>Learn more about FileFormat in The Internals of Spark SQL online book.</p>  <p><code>createSink</code> creates a new instance of the data source class and branches off per the type:</p> <ul> <li> <p>For a StreamSinkProvider, <code>createSink</code> simply delegates the call and requests it to create a streaming sink</p> </li> <li> <p>For a <code>FileFormat</code>, <code>createSink</code> creates a FileStreamSink when <code>path</code> option is specified and the output mode is Append.</p> </li> </ul> <p><code>createSink</code> throws a <code>IllegalArgumentException</code> when <code>path</code> option is not specified for a <code>FileFormat</code> data source:</p> <pre><code>'path' is not specified\n</code></pre> <p><code>createSink</code> throws an <code>AnalysisException</code> when the given OutputMode is different from Append for a <code>FileFormat</code> data source:</p> <pre><code>Data source [className] does not support [outputMode] output mode\n</code></pre> <p><code>createSink</code> throws an <code>UnsupportedOperationException</code> for unsupported data source formats:</p> <pre><code>Data source [className] does not support streamed writing\n</code></pre> <p><code>createSink</code> is used when <code>DataStreamWriter</code> is requested to start a streaming query.</p>","text":"","location":"DataSource/#creating-streaming-sink"},{"title":"DataStreamReader","text":"<p><code>DataStreamReader</code> is an interface that Spark developers use to describe how to load data from a streaming data source.</p> <p></p>","location":"DataStreamReader/"},{"title":"Accessing DataStreamReader <p><code>DataStreamReader</code> is available using <code>SparkSession.readStream</code> method.</p> <pre><code>import org.apache.spark.sql.SparkSession\nval spark: SparkSession = ...\n\nval streamReader = spark.readStream\n</code></pre>","text":"","location":"DataStreamReader/#accessing-datastreamreader"},{"title":"Built-in Formats <p><code>DataStreamReader</code> supports loading streaming data from the following built-in data sources:</p> <ul> <li><code>csv</code></li> <li><code>json</code></li> <li><code>orc</code></li> <li><code>parquet</code> (default)</li> <li><code>text</code></li> <li><code>textFile</code></li> </ul>  <p>Tip</p> <p>Use <code>spark.sql.sources.default</code> configuration property to change the default data source.</p>   <p>Note</p> <p>Hive data source can only be used with tables, and it is an <code>AnalysisException</code> when specified explicitly.</p>","text":"","location":"DataStreamReader/#built-in-formats"},{"title":"Loading Data <pre><code>load(): DataFrame\nload(\n  path: String): DataFrame\n</code></pre> <p><code>DataStreamReader</code> gives specialized methods for built-in data systems and formats.</p> <p>In order to plug in a custom data source, <code>DataStreamReader</code> gives <code>format</code> and <code>load</code> methods.</p> <p><code>load</code> creates a streaming <code>DataFrame</code> that represents a \"loading\" streaming data node (and is internally a logical plan with a StreamingRelationV2 or StreamingRelation leaf logical operators).</p> <p><code>load</code> uses <code>DataSource.lookupDataSource</code> utility to look up the data source by <code>source</code> alias.</p>  <p>Tip</p> <p>Learn more about DataSource.lookupDataSource utility in The Internals of Spark SQL online book.</p>","text":"","location":"DataStreamReader/#loading-data"},{"title":"SupportsRead Tables with MICRO_BATCH_READ or CONTINUOUS_READ","text":"<p>For a <code>TableProvider</code> (that is not a <code>FileDataSourceV2</code>), <code>load</code> requests it for a <code>Table</code>.</p>  <p>Tip</p> <p>Learn more about TableProvider in The Internals of Spark SQL online book.</p>  <p>For a <code>Table</code> with <code>SupportsRead</code> with <code>MICRO_BATCH_READ</code> or <code>CONTINUOUS_READ</code> capabilities, <code>load</code> creates a <code>DataFrame</code> with StreamingRelationV2 leaf logical operator.</p>  <p>Tip</p> <p>Learn more about Table, SupportsRead and capabilities in The Internals of Spark SQL online book.</p>  <p>If the <code>DataSource</code> is a StreamSourceProvider, <code>load</code> creates the <code>StreamingRelationV2</code> with a StreamingRelation leaf logical operator.</p> <p>For other <code>Table</code>s, <code>load</code> creates a <code>DataFrame</code> with a StreamingRelation leaf logical operator.</p>","location":"DataStreamReader/#supportsread-tables-with-micro_batch_read-or-continuous_read"},{"title":"Other Data Sources","text":"<p><code>load</code> creates a <code>DataFrame</code> with a StreamingRelation leaf logical operator.</p>","location":"DataStreamReader/#other-data-sources"},{"title":"DataStreamWriter","text":"<p><code>DataStreamWriter</code> is an interface that Spark developers use to describe when the result of executing a streaming query is sent out to a streaming data source.</p>","location":"DataStreamWriter/"},{"title":"Accessing DataStreamWriter <p><code>DataStreamWriter</code> is available using Dataset.writeStream method.</p> <pre><code>import org.apache.spark.sql.streaming.DataStreamWriter\nimport org.apache.spark.sql.Row\n\nval streamingQuery: Dataset[Long] = ...\n\nassert(streamingQuery.isStreaming)\n\nval writer: DataStreamWriter[Row] = streamingQuery.writeStream\n</code></pre>","text":"","location":"DataStreamWriter/#accessing-datastreamwriter"},{"title":"Writing to ForeachWriter <pre><code>foreach(\n  writer: ForeachWriter[T]): DataStreamWriter[T]\n</code></pre> <p>Sets ForeachWriter as responsible for streaming writes</p>","text":"","location":"DataStreamWriter/#writing-to-foreachwriter"},{"title":"Writing Micro-Batches to ForeachBatchSink <pre><code>foreachBatch(\n  function: (Dataset[T], Long) =&gt; Unit): DataStreamWriter[T]\n</code></pre> <p>Sets the source as foreachBatch and creates a ForeachBatchSink to be responsible for streaming writes.</p>  <p>SPARK-24565</p> <p>As per SPARK-24565 Add API for in Structured Streaming for exposing output rows of each microbatch as a DataFrame, the purpose of the method is to expose the micro-batch output as a dataframe for the following:</p> <p>Pass the output rows of each batch to a library that is designed for the batch jobs only Reuse batch data sources for output whose streaming version does not exist Multi-writes where the output rows are written to multiple outputs by writing twice for every batch</p>","text":"","location":"DataStreamWriter/#writing-micro-batches-to-foreachbatchsink"},{"title":"Streaming Sink by Name <pre><code>format(\n  source: String): DataStreamWriter[T]\n</code></pre> <p>Specifies the streaming sink by name (alias)</p>","text":"","location":"DataStreamWriter/#streaming-sink-by-name"},{"title":"Output Mode <pre><code>outputMode(\n  outputMode: OutputMode): DataStreamWriter[T]\noutputMode(\n  outputMode: String): DataStreamWriter[T]\n</code></pre> <p>Specifies the OutputMode of the streaming query (what data is sent out to a streaming sink when there is new data available in streaming data sources)</p> <p>Default: Append</p> <pre><code>import org.apache.spark.sql.streaming.OutputMode.Update\nval inputStream = spark\n  .readStream\n  .format(\"rate\")\n  .load\n  .writeStream\n  .format(\"console\")\n  .outputMode(Update) // &lt;-- update output mode\n  .start\n</code></pre>","text":"","location":"DataStreamWriter/#output-mode"},{"title":"Partitioning Streaming Writes <pre><code>partitionBy(\n  colNames: String*): DataStreamWriter[T]\n</code></pre>","text":"","location":"DataStreamWriter/#partitioning-streaming-writes"},{"title":"Query Name <pre><code>queryName(\n  queryName: String): DataStreamWriter[T]\n</code></pre> <p>Assigns the name of a query that is just an additional option with the key <code>queryName</code>.</p>","text":"","location":"DataStreamWriter/#query-name"},{"title":"Starting Streaming Query (Streaming Writes) <pre><code>start(): StreamingQuery\n// Explicit `path` (that could also be specified as an option)\nstart(\n  path: String): StreamingQuery\n</code></pre> <p>Creates and immediately starts a StreamingQuery that is returned as a handle to control the execution of the query</p> <p>Internally, <code>start</code> branches off per <code>source</code>.</p> <ul> <li><code>memory</code></li> <li><code>foreach</code></li> <li>other formats</li> </ul> <p>...FIXME</p> <p><code>start</code> throws an <code>AnalysisException</code> for <code>source</code> to be <code>hive</code>.</p> <pre><code>val q =  spark.\n  readStream.\n  text(\"server-logs/*\").\n  writeStream.\n  format(\"hive\") &lt;-- hive format used as a streaming sink\nscala&gt; q.start\norg.apache.spark.sql.AnalysisException: Hive data source can only be used with tables, you can not write files of Hive data source directly.;\n  at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:234)\n  ... 48 elided\n</code></pre>","text":"","location":"DataStreamWriter/#starting-streaming-query-streaming-writes"},{"title":"Trigger <pre><code>trigger(\n  trigger: Trigger): DataStreamWriter[T]\n</code></pre> <p>Sets the Trigger for how often the streaming query should be executed</p> <p>Default: ProcessingTime(0L) that runs a streaming query as often as possible.</p>","text":"","location":"DataStreamWriter/#trigger"},{"title":"EpochCoordinator RPC Endpoint","text":"<p><code>EpochCoordinator</code> is a <code>ThreadSafeRpcEndpoint</code> that tracks offsets and epochs (coordinates epochs) by handling &lt;&gt; (in &lt;&gt; and &lt;&gt; modes) from...FIXME <p><code>EpochCoordinator</code> is &lt;&gt; (using &lt;&gt; factory method) when <code>ContinuousExecution</code> is requested to &lt;&gt;. <p>[[messages]] [[EpochCoordinatorMessage]] .EpochCoordinator RPC Endpoint's Messages [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Message | Description</p> <p>a| <code>CommitPartitionEpoch</code></p> <ul> <li>[[CommitPartitionEpoch-partitionId]] Partition ID</li> <li>[[CommitPartitionEpoch-epoch]] Epoch</li> <li>[[CommitPartitionEpoch-message]] DataSource API V2's <code>WriterCommitMessage</code></li> </ul> <p>| [[CommitPartitionEpoch]] Sent out (in one-way asynchronous mode) exclusively when <code>ContinuousWriteRDD</code> is requested to &lt;&gt; (after all rows were written down to a streaming sink) <p>| GetCurrentEpoch | [[GetCurrentEpoch]] Sent out (in request-response synchronous mode) exclusively when <code>EpochMarkerGenerator</code> thread is requested to &lt;&gt; <p>| IncrementAndGetEpoch | [[IncrementAndGetEpoch]] Sent out (in request-response synchronous mode) exclusively when <code>ContinuousExecution</code> is requested to &lt;&gt; (and start a separate epoch update thread) <p>a| <code>ReportPartitionOffset</code></p> <ul> <li>[[ReportPartitionOffset-partitionId]] Partition ID</li> <li>[[ReportPartitionOffset-epoch]] Epoch</li> <li>[[ReportPartitionOffset-offset]] PartitionOffset</li> </ul> <p>| [[ReportPartitionOffset]] Sent out (in one-way asynchronous mode) exclusively when <code>ContinuousQueuedDataReader</code> is requested for the &lt;&gt; to be read in the current epoch, and the epoch is done <p>a| <code>SetReaderPartitions</code></p> <ul> <li>[[SetReaderPartitions-numPartitions]] Number of partitions</li> </ul> <p>| [[SetReaderPartitions]] Sent out (in request-response synchronous mode) exclusively when <code>DataSourceV2ScanExec</code> leaf physical operator is requested for the input RDDs (for a ContinuousReader and is about to create a &lt;&gt;) <p>The &lt;&gt; is exactly the number of <code>InputPartitions</code> from the <code>ContinuousReader</code>. <p>a| <code>SetWriterPartitions</code></p> <ul> <li>[[SetWriterPartitions-numPartitions]] Number of partitions</li> </ul> <p>| [[SetWriterPartitions]] Sent out (in request-response synchronous mode) exclusively when <code>WriteToContinuousDataSourceExec</code> leaf physical operator is requested to &lt;&gt; (and requests a &lt;&gt; to collect that simply never finishes...and that's the trick of continuous mode) <p>a| <code>StopContinuousExecutionWrites</code> | [[StopContinuousExecutionWrites]] Sent out (in request-response synchronous mode) exclusively when <code>ContinuousExecution</code> is requested to &lt;&gt; (and it finishes successfully or not) <p>|===</p> <p>[[logging]] [TIP] ==== Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.streaming.continuous.EpochCoordinatorRef*</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.streaming.continuous.EpochCoordinatorRef*=ALL\n</code></pre>","location":"EpochCoordinator/"},{"title":"Refer to &lt;&gt;. <p>=== [[receive]] Receiving Messages (Fire-And-Forget One-Way Mode) -- <code>receive</code> Method</p>","text":"","location":"EpochCoordinator/#refer-to"},{"title":"[source, scala]","text":"","location":"EpochCoordinator/#source-scala"},{"title":"receive: PartialFunction[Any, Unit]","text":"<p>NOTE: <code>receive</code> is part of the <code>RpcEndpoint</code> Contract in Apache Spark to receive messages in fire-and-forget one-way mode.</p> <p><code>receive</code> handles the following messages:</p> <ul> <li>&lt;&gt; <li>&lt;&gt;  <p>With the &lt;&gt; turned on, <code>receive</code> simply swallows messages and does nothing. <p>=== [[receiveAndReply]] Receiving Messages (Request-Response Two-Way Mode) -- <code>receiveAndReply</code> Method</p>","location":"EpochCoordinator/#receive-partialfunctionany-unit"},{"title":"[source, scala]","text":"","location":"EpochCoordinator/#source-scala_1"},{"title":"receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit]","text":"<p>NOTE: <code>receiveAndReply</code> is part of the <code>RpcEndpoint</code> Contract in Apache Spark to receive and reply to messages in request-response two-way mode.</p> <p><code>receiveAndReply</code> handles the following messages:</p> <ul> <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt;  <p>==== [[resolveCommitsAtEpoch]] <code>resolveCommitsAtEpoch</code> Internal Method</p>","location":"EpochCoordinator/#receiveandreplycontext-rpccallcontext-partialfunctionany-unit"},{"title":"[source, scala]","text":"","location":"EpochCoordinator/#source-scala_2"},{"title":"resolveCommitsAtEpoch(epoch: Long): Unit","text":"<p><code>resolveCommitsAtEpoch</code>...FIXME</p> <p>NOTE: <code>resolveCommitsAtEpoch</code> is used exclusively when <code>EpochCoordinator</code> is requested to handle &lt;&gt; and &lt;&gt; messages. <p>==== [[commitEpoch]] <code>commitEpoch</code> Internal Method</p>","location":"EpochCoordinator/#resolvecommitsatepochepoch-long-unit"},{"title":"[source, scala]","text":"<p>commitEpoch(   epoch: Long,   messages: Iterable[WriterCommitMessage]): Unit</p>  <p><code>commitEpoch</code>...FIXME</p> <p>NOTE: <code>commitEpoch</code> is used exclusively when <code>EpochCoordinator</code> is requested to &lt;&gt;.","location":"EpochCoordinator/#source-scala_3"},{"title":"Creating Instance","text":"<p><code>EpochCoordinator</code> takes the following to be created:</p> <ul> <li>[[reader]] ContinuousReader</li> <li>[[query]] ContinuousExecution</li> <li>[[startEpoch]] Start epoch</li> <li>[[session]] <code>SparkSession</code></li> <li>[[rpcEnv]] <code>RpcEnv</code></li> </ul> <p>=== [[create]] Registering EpochCoordinator RPC Endpoint -- <code>create</code> Factory Method</p>","location":"EpochCoordinator/#creating-instance"},{"title":"[source, scala]","text":"<p>create(   writer: StreamWriter,   reader: ContinuousReader,   query: ContinuousExecution,   epochCoordinatorId: String,   startEpoch: Long,   session: SparkSession,   env: SparkEnv): RpcEndpointRef</p>  <p><code>create</code> simply &lt;&gt; and requests the <code>RpcEnv</code> to register a RPC endpoint as EpochCoordinator-[id] (where <code>id</code> is the given <code>epochCoordinatorId</code>). <p><code>create</code> prints out the following INFO message to the logs:</p> <pre><code>Registered EpochCoordinator endpoint\n</code></pre> <p>NOTE: <code>create</code> is used exclusively when <code>ContinuousExecution</code> is requested to &lt;&gt;. <p>=== [[internal-properties]] Internal Properties</p> <p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| queryWritesStopped | [[queryWritesStopped]] Flag that indicates whether to drop messages (<code>true</code>) or not (<code>false</code>) when requested to &lt;&gt; <p>Default: <code>false</code></p> <p>Turned on (<code>true</code>) when requested to &lt;&gt; |===","location":"EpochCoordinator/#source-scala_4"},{"title":"EpochCoordinatorRef","text":"<p>== [[EpochCoordinatorRef]] EpochCoordinatorRef</p> <p><code>EpochCoordinatorRef</code> is...FIXME</p> <p>=== [[create]] Creating Remote Reference to EpochCoordinator RPC Endpoint -- <code>create</code> Factory Method</p>","location":"EpochCoordinatorRef/"},{"title":"[source, scala]","text":"<p>create(   writer: StreamWriter,   reader: ContinuousReader,   query: ContinuousExecution,   epochCoordinatorId: String,   startEpoch: Long,   session: SparkSession,   env: SparkEnv): RpcEndpointRef</p>  <p><code>create</code>...FIXME</p> <p>NOTE: <code>create</code> is used exclusively when <code>ContinuousExecution</code> is requested to &lt;&gt;. <p>=== [[get]] Getting Remote Reference to EpochCoordinator RPC Endpoint -- <code>get</code> Factory Method</p>","location":"EpochCoordinatorRef/#source-scala"},{"title":"[source, scala]","text":"","location":"EpochCoordinatorRef/#source-scala_1"},{"title":"get(id: String, env: SparkEnv): RpcEndpointRef","text":"<p><code>get</code>...FIXME</p>","location":"EpochCoordinatorRef/#getid-string-env-sparkenv-rpcendpointref"},{"title":"[NOTE]","text":"<p><code>get</code> is used when:</p> <ul> <li> <p><code>DataSourceV2ScanExec</code> leaf physical operator is requested for the input RDDs (and creates a &lt;&gt; for a ContinuousReader)  <li> <p><code>ContinuousQueuedDataReader</code> is created (and initializes the &lt;&gt;)  <li> <p><code>EpochMarkerGenerator</code> is created (and initializes the &lt;&gt;)  <li> <p><code>ContinuousWriteRDD</code> is requested to &lt;&gt;","location":"EpochCoordinatorRef/#note"},{"title":"* <code>WriteToContinuousDataSourceExec</code> is requested to &lt;&gt;","text":"","location":"EpochCoordinatorRef/#writetocontinuousdatasourceexec-is-requested-to"},{"title":"EpochMarkerGenerator","text":"<p>== [[EpochMarkerGenerator]] EpochMarkerGenerator Thread</p> <p><code>EpochMarkerGenerator</code> is...FIXME</p> <p>=== [[run]] <code>run</code> Method</p>","location":"EpochMarkerGenerator/"},{"title":"[source, scala]","text":"","location":"EpochMarkerGenerator/#source-scala"},{"title":"run(): Unit","text":"<p>NOTE: <code>run</code> is part of the https://docs.oracle.com/javase/8/docs/api/java/lang/Runnable.html[java.lang.Runnable] Contract to be executed upon starting a thread.</p> <p><code>run</code>...FIXME</p>","location":"EpochMarkerGenerator/#run-unit"},{"title":"EpochTracker","text":"<p>== [[EpochTracker]] EpochTracker</p> <p><code>EpochTracker</code> is...FIXME</p> <p>=== [[getCurrentEpoch]] Current Epoch -- <code>getCurrentEpoch</code> Method</p>","location":"EpochTracker/"},{"title":"[source, scala]","text":"","location":"EpochTracker/#source-scala"},{"title":"getCurrentEpoch: Option[Long]","text":"<p><code>getCurrentEpoch</code>...FIXME</p> <p>NOTE: <code>getCurrentEpoch</code> is used when...FIXME</p> <p>=== [[incrementCurrentEpoch]] Advancing (Incrementing) Epoch -- <code>incrementCurrentEpoch</code> Method</p>","location":"EpochTracker/#getcurrentepoch-optionlong"},{"title":"[source, scala]","text":"","location":"EpochTracker/#source-scala_1"},{"title":"incrementCurrentEpoch(): Unit","text":"<p><code>incrementCurrentEpoch</code>...FIXME</p> <p>NOTE: <code>incrementCurrentEpoch</code> is used when...FIXME</p>","location":"EpochTracker/#incrementcurrentepoch-unit"},{"title":"FileContextBasedCheckpointFileManager","text":"<p><code>FileContextBasedCheckpointFileManager</code> is a CheckpointFileManager that uses <code>FileContext</code> (Apache Hadoop).</p> <p><code>FileContextBasedCheckpointFileManager</code> is the default CheckpointFileManager (unless spark.sql.streaming.checkpointFileManagerClass is defined) as HDFS's FileContext.rename() gives atomic renames, which is used for createAtomic.</p>","location":"FileContextBasedCheckpointFileManager/"},{"title":"Creating Instance","text":"<p><code>FileContextBasedCheckpointFileManager</code> takes the following to be created:</p> <ul> <li> <code>Path</code> (Apache Hadoop) <li> <code>Configuration</code> (Apache Hadoop)  <p><code>FileContextBasedCheckpointFileManager</code> is created when:</p> <ul> <li><code>CheckpointFileManager</code> is requested to create a CheckpointFileManager (and spark.sql.streaming.checkpointFileManagerClass is not defined)</li> </ul>","location":"FileContextBasedCheckpointFileManager/#creating-instance"},{"title":"createAtomic <pre><code>createAtomic(\n  path: Path,\n  overwriteIfPossible: Boolean): CancellableFSDataOutputStream\n</code></pre> <p><code>createAtomic</code> is part of the CheckpointFileManager abstraction.</p>  <p><code>createAtomic</code> creates a <code>RenameBasedFSDataOutputStream</code>.</p>","text":"","location":"FileContextBasedCheckpointFileManager/#createatomic"},{"title":"FileSystemBasedCheckpointFileManager","text":"<p>[[CheckpointFileManager]] <code>FileSystemBasedCheckpointFileManager</code> is a CheckpointFileManager that uses Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html[FileSystem] API for managing checkpoint files:</p> <ul> <li> <p>list uses ++https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#listStatus(org.apache.hadoop.fs.Path[],%20org.apache.hadoop.fs.PathFilter)++[FileSystem.listStatus]</p> </li> <li> <p>mkdirs uses ++https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#mkdirs(org.apache.hadoop.fs.Path,%20org.apache.hadoop.fs.permission.FsPermission)++[FileSystem.mkdirs]</p> </li> <li> <p>createTempFile uses ++https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#create(org.apache.hadoop.fs.Path,%20boolean)++[FileSystem.create] (with overwrite enabled)</p> </li> <li> <p>[[createAtomic]] createAtomic uses <code>RenameBasedFSDataOutputStream</code></p> </li> <li> <p>open uses ++https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#open(org.apache.hadoop.fs.Path)++[FileSystem.open]</p> </li> <li> <p>exists uses ++https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#getFileStatus(org.apache.hadoop.fs.Path)++[FileSystem.getFileStatus]</p> </li> <li> <p>renameTempFile uses ++https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#rename(org.apache.hadoop.fs.Path,%20org.apache.hadoop.fs.Path)++[FileSystem.rename]</p> </li> <li> <p>delete uses ++https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#delete(org.apache.hadoop.fs.Path,%20boolean)++[FileSystem.delete] (with recursive enabled)</p> </li> <li> <p>isLocal is <code>true</code> for the &lt;&gt; being <code>LocalFileSystem</code> or <code>RawLocalFileSystem</code>   <p><code>FileSystemBasedCheckpointFileManager</code> is &lt;&gt; exclusively when <code>CheckpointFileManager</code> helper object is requested for a CheckpointFileManager (for HDFSMetadataLog, StreamMetadata and HDFSBackedStateStoreProvider). <p>[[RenameHelperMethods]] <code>FileSystemBasedCheckpointFileManager</code> is a <code>RenameHelperMethods</code> for &lt;&gt; by \"write-to-temp-file-and-rename\". <p>=== [[creating-instance]] Creating FileSystemBasedCheckpointFileManager Instance</p> <p><code>FileSystemBasedCheckpointFileManager</code> takes the following to be created:</p> <ul> <li>[[path]] Checkpoint directory (Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/Path.html[Path])</li> <li>[[hadoopConf]] Configuration (Hadoop's http://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/conf/Configuration.html[Configuration])</li> </ul> <p><code>FileSystemBasedCheckpointFileManager</code> initializes the &lt;&gt;. <p>=== [[internal-properties]] Internal Properties</p> <p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| fs a| [[fs]] Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html[FileSystem] of the &lt;&gt; <p>|===</p>","location":"FileSystemBasedCheckpointFileManager/"},{"title":"GroupState","text":"<p><code>GroupState</code> is an &lt;&gt; of &lt;&gt; (of type <code>S</code>) in Arbitrary Stateful Streaming Aggregation. <p><code>GroupState</code> is used with the following KeyValueGroupedDataset operations:</p> <ul> <li> <p>mapGroupsWithState</p> </li> <li> <p>flatMapGroupsWithState</p> </li> </ul> <p><code>GroupState</code> is created separately for every aggregation key to hold a state as an aggregation state value.</p> <p>[[contract]] .GroupState Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| exists a| [[exists]]</p>","location":"GroupState/"},{"title":"[source, scala]","text":"","location":"GroupState/#source-scala"},{"title":"exists: Boolean","text":"<p>Checks whether the state value exists or not</p> <p>If not exists, &lt;&gt; throws a <code>NoSuchElementException</code>. Use &lt;&gt; instead. <p>| get a| [[get]]</p>","location":"GroupState/#exists-boolean"},{"title":"[source, scala]","text":"","location":"GroupState/#source-scala_1"},{"title":"get: S","text":"<p>Gets the state value if it &lt;&gt; or throws a <code>NoSuchElementException</code> <p>| getCurrentProcessingTimeMs a| [[getCurrentProcessingTimeMs]]</p>","location":"GroupState/#get-s"},{"title":"[source, scala]","text":"","location":"GroupState/#source-scala_2"},{"title":"getCurrentProcessingTimeMs(): Long","text":"<p>Gets the current processing time (as milliseconds in epoch time)</p> <p>| getCurrentWatermarkMs a| [[getCurrentWatermarkMs]]</p>","location":"GroupState/#getcurrentprocessingtimems-long"},{"title":"[source, scala]","text":"","location":"GroupState/#source-scala_3"},{"title":"getCurrentWatermarkMs(): Long","text":"<p>Gets the current event time watermark (as milliseconds in epoch time)</p> <p>| getOption a| [[getOption]]</p>","location":"GroupState/#getcurrentwatermarkms-long"},{"title":"[source, scala]","text":"","location":"GroupState/#source-scala_4"},{"title":"getOption: Option[S]","text":"<p>Gets the state value as a Scala <code>Option</code> (regardless whether it &lt;&gt; or not) <p>Used when:</p> <ul> <li> <p><code>InputProcessor</code> is requested to <code>callFunctionAndUpdateState</code> (when the row iterator is consumed and a state value has been updated, removed or timeout changed)</p> </li> <li> <p><code>GroupStateImpl</code> is requested for the textual representation</p> </li> </ul> <p>| hasTimedOut a| [[hasTimedOut]]</p>","location":"GroupState/#getoption-options"},{"title":"[source, scala]","text":"","location":"GroupState/#source-scala_5"},{"title":"hasTimedOut: Boolean","text":"<p>Whether the state (for a given key) has timed out or not.</p> <p>Can only be <code>true</code> when timeouts are enabled using &lt;&gt; <p>| remove a| [[remove]]</p>","location":"GroupState/#hastimedout-boolean"},{"title":"[source, scala]","text":"","location":"GroupState/#source-scala_6"},{"title":"remove(): Unit","text":"<p>Removes the state</p> <p>| setTimeoutDuration a| [[setTimeoutDuration]]</p>","location":"GroupState/#remove-unit"},{"title":"[source, scala]","text":"<p>setTimeoutDuration(durationMs: Long): Unit setTimeoutDuration(duration: String): Unit</p>  <p>Specifies the timeout duration for the state key (in millis or as a string, e.g. \"10 seconds\", \"1 hour\") for GroupStateTimeout.ProcessingTimeTimeout</p> <p>| setTimeoutTimestamp a| [[setTimeoutTimestamp]]</p>","location":"GroupState/#source-scala_7"},{"title":"[source, scala]","text":"<p>setTimeoutTimestamp(timestamp: java.sql.Date): Unit setTimeoutTimestamp(   timestamp: java.sql.Date,   additionalDuration: String): Unit setTimeoutTimestamp(timestampMs: Long): Unit setTimeoutTimestamp(   timestampMs: Long,   additionalDuration: String): Unit</p>  <p>Specifies the timeout timestamp for the state key for GroupStateTimeout.EventTimeTimeout</p> <p>| update a| [[update]]</p>","location":"GroupState/#source-scala_8"},{"title":"[source, scala]","text":"","location":"GroupState/#source-scala_9"},{"title":"update(newState: S): Unit","text":"<p>Updates the state (sets the state to a new value)</p> <p>|===</p> <p>[[implementations]] GroupStateImpl is the default and only known implementation of the &lt;&gt; in Spark Structured Streaming.","location":"GroupState/#updatenewstate-s-unit"},{"title":"GroupStateImpl","text":"<p><code>GroupStateImpl</code> is the default and only known GroupState in Spark Structured Streaming.</p> <p><code>GroupStateImpl</code> holds per-group &lt;&gt; of type <code>S</code> per group key. <p><code>GroupStateImpl</code> is &lt;&gt; when <code>GroupStateImpl</code> helper object is requested for the following: <ul> <li> <p>&lt;&gt;  <li> <p>&lt;&gt;","location":"GroupStateImpl/"},{"title":"Creating Instance","text":"<p><code>GroupStateImpl</code> takes the following to be created:</p> <ul> <li>[[optionalValue]] State value (of type <code>S</code>)</li> <li>[[batchProcessingTimeMs]] Batch processing time</li> <li>[[eventTimeWatermarkMs]] <code>eventTimeWatermarkMs</code></li> <li>[[timeoutConf]] GroupStateTimeout</li> <li>[[hasTimedOut]] <code>hasTimedOut</code> flag</li> <li>[[watermarkPresent]] <code>watermarkPresent</code> flag</li> </ul> <p>=== [[createForStreaming]] Creating GroupStateImpl for Streaming Query -- <code>createForStreaming</code> Factory Method</p>","location":"GroupStateImpl/#creating-instance"},{"title":"[source, scala]","text":"<p>createForStreamingS: GroupStateImpl[S]</p>  <p><code>createForStreaming</code> simply creates a &lt;&gt; with the given input arguments. <p>NOTE: <code>createForStreaming</code> is used exclusively when <code>InputProcessor</code> is requested to callFunctionAndUpdateState (when <code>InputProcessor</code> is requested to processNewData and processTimedOutState).</p> <p>=== [[createForBatch]] Creating GroupStateImpl for Batch Query -- <code>createForBatch</code> Factory Method</p>","location":"GroupStateImpl/#source-scala"},{"title":"[source, scala]","text":"<p>createForBatch(   timeoutConf: GroupStateTimeout,   watermarkPresent: Boolean): GroupStateImpl[Any]</p>  <p><code>createForBatch</code>...FIXME</p> <p>NOTE: <code>createForBatch</code> is used when...FIXME</p> <p>=== [[toString]] Textual Representation -- <code>toString</code> Method</p>","location":"GroupStateImpl/#source-scala_1"},{"title":"[source, scala]","text":"","location":"GroupStateImpl/#source-scala_2"},{"title":"toString: String","text":"<p>NOTE: <code>toString</code> is part of the ++https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object] contract for the string representation of the object.</p> <p><code>toString</code>...FIXME</p> <p>=== [[setTimeoutDuration]] Specifying Timeout Duration for ProcessingTimeTimeout -- <code>setTimeoutDuration</code> Method</p>","location":"GroupStateImpl/#tostring-string"},{"title":"[source, scala]","text":"","location":"GroupStateImpl/#source-scala_3"},{"title":"setTimeoutDuration(durationMs: Long): Unit","text":"<p><code>setTimeoutDuration</code>...FIXME</p> <p><code>setTimeoutDuration</code> is part of the GroupState abstraction.</p> <p>=== [[setTimeoutTimestamp]] Specifying Timeout Timestamp for EventTimeTimeout -- <code>setTimeoutTimestamp</code> Method</p>","location":"GroupStateImpl/#settimeoutdurationdurationms-long-unit"},{"title":"[source, scala]","text":"","location":"GroupStateImpl/#source-scala_4"},{"title":"setTimeoutTimestamp(durationMs: Long): Unit","text":"<p><code>setTimeoutTimestamp</code>...FIXME</p> <p><code>setTimeoutTimestamp</code> is part of the GroupState abstraction.</p> <p>=== [[getCurrentProcessingTimeMs]] Getting Processing Time -- <code>getCurrentProcessingTimeMs</code> Method</p>","location":"GroupStateImpl/#settimeouttimestampdurationms-long-unit"},{"title":"[source, scala]","text":"","location":"GroupStateImpl/#source-scala_5"},{"title":"getCurrentProcessingTimeMs(): Long","text":"<p><code>getCurrentProcessingTimeMs</code> simply returns the &lt;&gt;. <p><code>getCurrentProcessingTimeMs</code> is part of the GroupState abstraction.</p> <p>=== [[update]] Updating State -- <code>update</code> Method</p>","location":"GroupStateImpl/#getcurrentprocessingtimems-long"},{"title":"[source, scala]","text":"","location":"GroupStateImpl/#source-scala_6"},{"title":"update(newValue: S): Unit","text":"<p><code>update</code>...FIXME</p> <p><code>update</code> is part of the GroupState abstraction.</p> <p>=== [[remove]] Removing State -- <code>remove</code> Method</p>","location":"GroupStateImpl/#updatenewvalue-s-unit"},{"title":"[source, scala]","text":"","location":"GroupStateImpl/#source-scala_7"},{"title":"remove(): Unit","text":"<p><code>remove</code>...FIXME</p> <p><code>remove</code> is part of the GroupState abstraction.</p> <p>=== [[internal-properties]] Internal Properties</p> <p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| value a| [[value]] FIXME</p> <p>Used when...FIXME</p> <p>| defined a| [[defined]] FIXME</p> <p>Used when...FIXME</p> <p>| updated a| [[updated]][[hasUpdated]] Updated flag that says whether the state has been &lt;&gt; or not <p>Default: <code>false</code></p> <p>Disabled (<code>false</code>) when <code>GroupStateImpl</code> is requested to &lt;&gt; <p>Enabled (<code>true</code>) when <code>GroupStateImpl</code> is requested to &lt;&gt; <p>| removed a| [[removed]][[hasRemoved]] Removed flag that says whether the state is marked &lt;&gt; or not <p>Default: <code>false</code></p> <p>Disabled (<code>false</code>) when <code>GroupStateImpl</code> is requested to &lt;&gt; <p>Enabled (<code>true</code>) when <code>GroupStateImpl</code> is requested to &lt;&gt; <p>| timeoutTimestamp a| [[timeoutTimestamp]][[getTimeoutTimestamp]] Current timeout timestamp (in millis) for GroupStateTimeout.EventTimeTimeout or GroupStateTimeout.ProcessingTimeTimeout</p> <p>[[NO_TIMESTAMP]] Default: <code>-1</code></p> <p>Defined using &lt;&gt; (for <code>EventTimeTimeout</code>) and &lt;&gt; (for <code>ProcessingTimeTimeout</code>) |===","location":"GroupStateImpl/#remove-unit"},{"title":"GroupStateTimeout","text":"<p><code>GroupStateTimeout</code> represents an aggregation state timeout that defines when a GroupState can be considered timed-out (expired) in Arbitrary Stateful Streaming Aggregation.</p> <p><code>GroupStateTimeout</code> is used with the following KeyValueGroupedDataset operations:</p> <ul> <li> <p>mapGroupsWithState</p> </li> <li> <p>flatMapGroupsWithState</p> </li> </ul> <p>[[extensions]] .GroupStateTimeouts [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | GroupStateTimeout | Description</p> <p>| EventTimeTimeout | [[EventTimeTimeout]] Timeout based on event time</p> <p>Used when...FIXME</p> <p>| NoTimeout | [[NoTimeout]] No timeout</p> <p>Used when...FIXME</p> <p>| ProcessingTimeTimeout a| [[ProcessingTimeTimeout]] Timeout based on processing time</p> <p>FlatMapGroupsWithStateExec physical operator requires that <code>batchTimestampMs</code> is specified when <code>ProcessingTimeTimeout</code> is used.</p> <p><code>batchTimestampMs</code> is defined when IncrementalExecution is created (with the state). <code>IncrementalExecution</code> is given <code>OffsetSeqMetadata</code> when <code>StreamExecution</code> is requested to run a streaming batch.</p> <p>|===</p>","location":"GroupStateTimeout/"},{"title":"HDFSMetadataLog","text":"<p><code>HDFSMetadataLog</code> is an extension of the MetadataLog abstraction for metadata storage to store batch files in a metadata log directory on Hadoop DFS (for fault-tolerance and reliability).</p>","location":"HDFSMetadataLog/"},{"title":"Extensions","text":"<ul> <li>CommitLog</li> <li>CompactibleFileStreamLog</li> <li>KafkaSourceInitialOffsetWriter</li> <li>OffsetSeqLog</li> </ul>","location":"HDFSMetadataLog/#extensions"},{"title":"Creating Instance","text":"<p><code>HDFSMetadataLog</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> <li> Path of the metadata log directory  <p>While being created, <code>HDFSMetadataLog</code> makes sure that the path exists (and creates it if not).</p>","location":"HDFSMetadataLog/#creating-instance"},{"title":"Metadata Log Directory <p><code>HDFSMetadataLog</code> uses the given path as the metadata log directory with metadata logs (one per batch).</p> <p>The path is immediately converted to a Hadoop Path for file management.</p>","text":"","location":"HDFSMetadataLog/#metadata-log-directory"},{"title":"CheckpointFileManager <p><code>HDFSMetadataLog</code> creates a CheckpointFileManager (with the metadata log directory) when created.</p>","text":"","location":"HDFSMetadataLog/#checkpointfilemanager"},{"title":"Implicit Json4s Formats <p><code>HDFSMetadataLog</code> uses Json4s with the Jackson binding for metadata serialization and deserialization (to and from JSON format).</p>","text":"","location":"HDFSMetadataLog/#implicit-json4s-formats"},{"title":"Latest Committed Batch Id with Metadata (If Available) <pre><code>getLatest(): Option[(Long, T)]\n</code></pre> <p><code>getLatest</code> is a part of MetadataLog abstraction.</p> <p><code>getLatest</code> requests the internal &lt;&gt; for the files in &lt;&gt; that match &lt;&gt;. <p><code>getLatest</code> takes the batch ids (the batch files correspond to) and sorts the ids in reverse order.</p> <p><code>getLatest</code> gives the first batch id with the metadata which &lt;&gt;.  <p>Note</p> <p>It is possible that the batch id could be in the metadata storage, but not available for retrieval.</p>","text":"","location":"HDFSMetadataLog/#latest-committed-batch-id-with-metadata-if-available"},{"title":"Retrieving Metadata of Streaming Batch (if Available) <pre><code>get(\n  batchId: Long): Option[T]\n</code></pre> <p><code>get</code> is part of the MetadataLog abstraction.</p> <p><code>get</code>...FIXME</p>","text":"","location":"HDFSMetadataLog/#retrieving-metadata-of-streaming-batch-if-available"},{"title":"Deserializing Metadata <pre><code>deserialize(\n  in: InputStream): T\n</code></pre> <p><code>deserialize</code> deserializes a metadata (of type <code>T</code>) from a given <code>InputStream</code>.</p> <p><code>deserialize</code> is used to retrieve metadata of a batch.</p>","text":"","location":"HDFSMetadataLog/#deserializing-metadata"},{"title":"Retrieving Metadata of Streaming Batches (if Available) <pre><code>get(\n  startId: Option[Long],\n  endId: Option[Long]): Array[(Long, T)]\n</code></pre> <p><code>get</code> is part of the MetadataLog abstraction.</p> <p><code>get</code>...FIXME</p>","text":"","location":"HDFSMetadataLog/#retrieving-metadata-of-streaming-batches-if-available"},{"title":"Persisting Metadata of Streaming Micro-Batch <pre><code>add(\n  batchId: Long,\n  metadata: T): Boolean\n</code></pre> <p><code>add</code> is part of the MetadataLog abstraction.</p> <p><code>add</code> return <code>true</code> when the metadata of the streaming batch was not available and persisted successfully. Otherwise, <code>add</code> returns <code>false</code>.</p> <p>Internally, <code>add</code> &lt;&gt; (<code>batchId</code>) and returns <code>false</code> when found. <p>Otherwise, when not found, <code>add</code> &lt;&gt; for the given <code>batchId</code> and &lt;&gt;. <code>add</code> returns <code>true</code> if successful.","text":"","location":"HDFSMetadataLog/#persisting-metadata-of-streaming-micro-batch"},{"title":"Writing Batch Metadata to File (Metadata Log) <pre><code>writeBatchToFile(\n  metadata: T,\n  path: Path): Unit\n</code></pre> <p><code>writeBatchToFile</code> requests the &lt;&gt; to createAtomic (for the specified <code>path</code> and the <code>overwriteIfPossible</code> flag disabled). <p><code>writeBatchToFile</code> then &lt;&gt; (to the <code>CancellableFSDataOutputStream</code> output stream) and closes the stream. <p>In case of an exception, <code>writeBatchToFile</code> simply requests the <code>CancellableFSDataOutputStream</code> output stream to <code>cancel</code> (so that the output file is not generated) and re-throws the exception.</p>","text":"","location":"HDFSMetadataLog/#writing-batch-metadata-to-file-metadata-log"},{"title":"Serializing Metadata <pre><code>serialize(\n  metadata: T,\n  out: OutputStream): Unit\n</code></pre> <p><code>serialize</code> simply writes out the log data in a serialized format (using Json4s (with Jackson binding) library).</p>","text":"","location":"HDFSMetadataLog/#serializing-metadata"},{"title":"Purging Expired Metadata <pre><code>purge(\n  thresholdBatchId: Long): Unit\n</code></pre> <p><code>purge</code> is part of the MetadataLog abstraction.</p> <p><code>purge</code>...FIXME</p>","text":"","location":"HDFSMetadataLog/#purging-expired-metadata"},{"title":"Batch Files <p><code>HDFSMetadataLog</code> considers a file a batch file when the name is simply a <code>long</code> number.</p> <p><code>HDFSMetadataLog</code> uses a Hadoop PathFilter to list only batch files.</p>","text":"","location":"HDFSMetadataLog/#batch-files"},{"title":"Verifying Batch Ids <pre><code>verifyBatchIds(\n  batchIds: Seq[Long],\n  startId: Option[Long],\n  endId: Option[Long]): Unit\n</code></pre> <p><code>verifyBatchIds</code>...FIXME</p> <p><code>verifyBatchIds</code> is used when:</p> <ul> <li><code>FileStreamSourceLog</code> is requested to get</li> <li><code>HDFSMetadataLog</code> is requested to get</li> </ul>","text":"","location":"HDFSMetadataLog/#verifying-batch-ids"},{"title":"Path of Metadata File by Batch Id <pre><code>batchIdToPath(\n  batchId: Long): Path\n</code></pre> <p><code>batchIdToPath</code> simply creates a Hadoop Path for the file by the given <code>batchId</code> under the metadata log directory.</p>","text":"","location":"HDFSMetadataLog/#path-of-metadata-file-by-batch-id"},{"title":"Batch Id by Path of Metadata File <pre><code>pathToBatchId(\n  path: Path): Long\n</code></pre> <p><code>pathToBatchId</code>...FIXME</p>","text":"","location":"HDFSMetadataLog/#batch-id-by-path-of-metadata-file"},{"title":"IncrementalExecution","text":"<p><code>IncrementalExecution</code> is the <code>QueryExecution</code> (Spark SQL) of streaming queries.</p>","location":"IncrementalExecution/"},{"title":"Creating Instance","text":"<p><code>IncrementalExecution</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> <code>LogicalPlan</code> (Spark SQL) <li> OutputMode <li> State checkpoint location <li> Query ID <li> Run ID <li> Current Batch ID <li> OffsetSeqMetadata  <p><code>IncrementalExecution</code> is created (and becomes the StreamExecution.lastExecution) when:</p> <ul> <li><code>MicroBatchExecution</code> is requested to run a single streaming micro-batch (in queryPlanning phase)</li> <li><code>ContinuousExecution</code> is requested to run a streaming query in continuous mode (in queryPlanning phase)</li> <li>Dataset.explain operator is executed (on a streaming query)</li> </ul>","location":"IncrementalExecution/#creating-instance"},{"title":"statefulOperatorId <p><code>IncrementalExecution</code> uses the <code>statefulOperatorId</code> internal counter for the IDs of the stateful operators in the optimized logical plan (while applying the preparations rules) when requested to prepare the plan for execution (in executedPlan phase).</p>","text":"","location":"IncrementalExecution/#statefuloperatorid"},{"title":"Preparing Logical Plan (of Streaming Query) for Execution <p>When requested for the optimized logical plan (of the logical plan), <code>IncrementalExecution</code> transforms <code>CurrentBatchTimestamp</code> and <code>ExpressionWithRandomSeed</code> expressions with the timestamp literal and new random seeds, respectively. When transforming <code>CurrentBatchTimestamp</code> expressions, <code>IncrementalExecution</code> prints out the following INFO message to the logs:</p> <pre><code>Current batch timestamp = [timestamp]\n</code></pre> <p>Right after being created, <code>IncrementalExecution</code> is executed (in the queryPlanning phase by the MicroBatchExecution and ContinuousExecution stream execution engines) and so the entire query execution pipeline is executed up to and including executedPlan. That means that the extra planning strategies and the state preparation rule have been applied at this point and the streaming query is ready for execution.</p>","text":"","location":"IncrementalExecution/#preparing-logical-plan-of-streaming-query-for-execution"},{"title":"State Checkpoint Location <p><code>IncrementalExecution</code> is given the checkpoint location when created.</p> <p>For the two available execution engines (MicroBatchExecution and ContinuousExecution), the checkpoint location is actually state directory under the checkpoint root directory.</p> <pre><code>val queryName = \"rate2memory\"\nval checkpointLocation = s\"file:/tmp/checkpoint-$queryName\"\nval query = spark\n  .readStream\n  .format(\"rate\")\n  .load\n  .writeStream\n  .format(\"memory\")\n  .queryName(queryName)\n  .option(\"checkpointLocation\", checkpointLocation)\n  .start\n\n// Give the streaming query a moment (one micro-batch)\n// So lastExecution is available for the checkpointLocation\nimport scala.concurrent.duration._\nquery.awaitTermination(1.second.toMillis)\n\nimport org.apache.spark.sql.execution.streaming.StreamingQueryWrapper\nval stateCheckpointDir = query\n  .asInstanceOf[StreamingQueryWrapper]\n  .streamingQuery\n  .lastExecution\n  .checkpointLocation\nval stateDir = s\"$checkpointLocation/state\"\nassert(stateCheckpointDir equals stateDir)\n</code></pre> <p>State checkpoint location is used when <code>IncrementalExecution</code> is requested for the state info of the next stateful operator (when requested to optimize a streaming physical plan using the state preparation rule that creates the stateful physical operators: StateStoreSaveExec, StateStoreRestoreExec, StreamingDeduplicateExec, FlatMapGroupsWithStateExec, StreamingSymmetricHashJoinExec, and StreamingGlobalLimitExec).</p>","text":"","location":"IncrementalExecution/#state-checkpoint-location"},{"title":"Number of State Stores (spark.sql.shuffle.partitions) <pre><code>numStateStores: Int\n</code></pre> <p><code>numStateStores</code> is the number of state stores which corresponds to <code>spark.sql.shuffle.partitions</code> configuration property (default: <code>200</code>).</p>  <p>Tip</p> <p>Learn more about spark.sql.shuffle.partitions configuration property in The Internals of Spark SQL online book.</p>  <p>Internally, <code>numStateStores</code> requests the OffsetSeqMetadata for the spark.sql.shuffle.partitions configuration property (using the streaming configuration) or simply takes whatever was defined for the given SparkSession (default: <code>200</code>).</p> <p><code>numStateStores</code> is initialized right when <code>IncrementalExecution</code> is created.</p> <p><code>numStateStores</code> is used when <code>IncrementalExecution</code> is requested for the state info of the next stateful operator (when requested to optimize a streaming physical plan using the state preparation rule that creates the stateful physical operators: StateStoreSaveExec, StateStoreRestoreExec, StreamingDeduplicateExec, FlatMapGroupsWithStateExec, StreamingSymmetricHashJoinExec, and StreamingGlobalLimitExec).</p>","text":"","location":"IncrementalExecution/#number-of-state-stores-sparksqlshufflepartitions"},{"title":"Extra Planning Strategies for Streaming Queries <p><code>IncrementalExecution</code> uses a custom <code>SparkPlanner</code> (Spark SQL) with the following extra planning strategies to plan the streaming query for execution:</p> <ol> <li>StreamingJoinStrategy</li> <li>StatefulAggregationStrategy</li> <li>FlatMapGroupsWithStateStrategy</li> <li>StreamingRelationStrategy</li> <li>StreamingDeduplicationStrategy</li> <li>StreamingGlobalLimitStrategy</li> </ol>","text":"","location":"IncrementalExecution/#extra-planning-strategies-for-streaming-queries"},{"title":"State Preparation Rule For Execution-Specific Configuration <pre><code>state: Rule[SparkPlan]\n</code></pre> <p><code>state</code> is a custom physical preparation rule (<code>Rule[SparkPlan]</code>) that can transform a streaming physical plan (<code>SparkPlan</code>) with the following physical operators:</p> <ul> <li> <p>StateStoreSaveExec with any unary physical operator (<code>UnaryExecNode</code>) with a StateStoreRestoreExec</p> </li> <li> <p>StreamingDeduplicateExec</p> </li> <li> <p>FlatMapGroupsWithStateExec</p> </li> <li> <p>StreamingSymmetricHashJoinExec</p> </li> <li> <p>StreamingGlobalLimitExec</p> </li> </ul> <p><code>state</code> simply transforms the physical plan with the above physical operators and fills out the execution-specific configuration:</p> <ul> <li> <p>nextStatefulOperationStateInfo for the state info</p> </li> <li> <p>OutputMode</p> </li> <li> <p>batchWatermarkMs (through the OffsetSeqMetadata) for the event-time watermark</p> </li> <li> <p>batchTimestampMs (through the OffsetSeqMetadata) for the current timestamp</p> </li> <li> <p>getStateWatermarkPredicates for the state watermark predicates (for StreamingSymmetricHashJoinExec)</p> </li> </ul> <p><code>state</code> rule is used (as part of the physical query optimizations) when <code>IncrementalExecution</code> is requested to optimize (prepare) the physical plan of the streaming query (once for ContinuousExecution and every trigger for MicroBatchExecution in queryPlanning phase).</p>  <p>Tip</p> <p>Learn more about Physical Query Optimizations in The Internals of Spark SQL online book.</p>","text":"","location":"IncrementalExecution/#state-preparation-rule-for-execution-specific-configuration"},{"title":"Next StatefulOperationStateInfo <pre><code>nextStatefulOperationStateInfo(): StatefulOperatorStateInfo\n</code></pre> <p><code>nextStatefulOperationStateInfo</code> simply creates a new StatefulOperatorStateInfo with the state checkpoint location, the run ID (of the streaming query), the next statefulOperator ID, the current batch ID, and the number of state stores.</p>  <p>Note</p> <p>The only changing part of <code>StatefulOperatorStateInfo</code> across calls of the <code>nextStatefulOperationStateInfo</code> method is the the next  statefulOperator ID.</p> <p>All the other properties (the state checkpoint location, the run ID, the current batch ID, and the number of state stores) are the same within a single <code>IncrementalExecution</code> instance.</p> <p>The only two properties that may ever change are the run ID (after a streaming query is restarted from the checkpoint) and  the current batch ID (every micro-batch in MicroBatchExecution execution engine).</p>  <p><code>nextStatefulOperationStateInfo</code> is used when <code>IncrementalExecution</code> is requested to optimize a streaming physical plan using the state preparation rule (and creates the stateful physical operators: StateStoreSaveExec, StateStoreRestoreExec, StreamingDeduplicateExec, FlatMapGroupsWithStateExec, StreamingSymmetricHashJoinExec, and StreamingGlobalLimitExec).</p>","text":"","location":"IncrementalExecution/#next-statefuloperationstateinfo"},{"title":"Checking Out Whether Last Execution Requires Another Non-Data Micro-Batch <pre><code>shouldRunAnotherBatch(\n   newMetadata: OffsetSeqMetadata): Boolean\n</code></pre> <p><code>shouldRunAnotherBatch</code> is positive (<code>true</code>) if there is at least one StateStoreWriter operator (in the executedPlan physical query plan) that requires another non-data batch (per the given OffsetSeqMetadata with the event-time watermark and the batch timestamp).</p> <p>Otherwise, <code>shouldRunAnotherBatch</code> is negative (<code>false</code>).</p> <p><code>shouldRunAnotherBatch</code> is used when <code>MicroBatchExecution</code> is requested to construct the next streaming micro-batch (and checks out whether the last batch execution requires another non-data batch).</p>","text":"","location":"IncrementalExecution/#checking-out-whether-last-execution-requires-another-non-data-micro-batch"},{"title":"Demo: State Checkpoint Directory <p>Using <code>setConf(SHUFFLE_PARTITIONS, 1)</code> will make for an easier debugging as the state is then only for one partition and makes monitoring easier.</p> <pre><code>import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS\nspark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, 1)\n\nassert(spark.sessionState.conf.numShufflePartitions == 1)\n</code></pre> <p>Using the <code>rate</code> source as an input.</p> <pre><code>val counts = spark\n  .readStream\n  .format(\"rate\")\n  .load\n  .groupBy(window($\"timestamp\", \"5 seconds\") as \"group\")\n  .agg(count(\"value\") as \"value_count\") // &lt;-- creates an Aggregate logical operator\n  .orderBy(\"group\")  // &lt;-- makes for easier checking\n\nassert(counts.isStreaming, \"This should be a streaming query\")\n</code></pre> <p>Searching for <code>checkpoint = &lt;unknown&gt;</code> in the following output for StateStoreSaveExec and StateStoreRestoreExec physical operators.</p> <pre><code>scala&gt; counts.explain\n== Physical Plan ==\n*(5) Sort [group#5 ASC NULLS FIRST], true, 0\n+- Exchange rangepartitioning(group#5 ASC NULLS FIRST, 1)\n   +- *(4) HashAggregate(keys=[window#11], functions=[count(value#1L)])\n      +- StateStoreSave [window#11], state info [ checkpoint = &lt;unknown&gt;, runId = 558bf725-accb-487d-97eb-f790fa4a6138, opId = 0, ver = 0, numPartitions = 1], Append, 0, 2\n         +- *(3) HashAggregate(keys=[window#11], functions=[merge_count(value#1L)])\n            +- StateStoreRestore [window#11], state info [ checkpoint = &lt;unknown&gt;, runId = 558bf725-accb-487d-97eb-f790fa4a6138, opId = 0, ver = 0, numPartitions = 1], 2\n               +- *(2) HashAggregate(keys=[window#11], functions=[merge_count(value#1L)])\n                  +- Exchange hashpartitioning(window#11, 1)\n                     +- *(1) HashAggregate(keys=[window#11], functions=[partial_count(value#1L)])\n                        +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#11, value#1L]\n                           +- *(1) Filter isnotnull(timestamp#0)\n                              +- StreamingRelation rate, [timestamp#0, value#1L]\n</code></pre> <p>Start the query with the <code>checkpointLocation</code> option.</p> <pre><code>val checkpointLocation = \"/tmp/spark-streams-state-checkpoint-root\"\n\nimport scala.concurrent.duration._\nimport org.apache.spark.sql.streaming.{OutputMode, Trigger}\nval t = Trigger.ProcessingTime(1.hour) // should be enough time for exploration\nval sq = counts\n  .writeStream\n  .format(\"console\")\n  .option(\"truncate\", false)\n  .option(\"checkpointLocation\", checkpointLocation)\n  .trigger(t)\n  .outputMode(OutputMode.Complete)\n  .start\n</code></pre> <p>Wait till the first batch which should happen right after start and access <code>lastExecution</code> that has the checkpoint resolved.</p> <pre><code>import org.apache.spark.sql.execution.streaming._\nval lastExecution = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery.lastExecution\nassert(lastExecution.checkpointLocation == s\"file:${checkpointLocation}/state\")\n</code></pre>","text":"","location":"IncrementalExecution/#demo-state-checkpoint-directory"},{"title":"KeyValueGroupedDataset","text":"<p><code>KeyValueGroupedDataset</code> represents a grouped dataset as a result of Dataset.groupByKey operator (that aggregates records by a grouping function).</p> <pre><code>// Dataset[T]\ngroupByKey(func: T =&gt; K): KeyValueGroupedDataset[K, T]\n</code></pre> <pre><code>import java.sql.Timestamp\nval numGroups = spark.\n  readStream.\n  format(\"rate\").\n  load.\n  as[(Timestamp, Long)].\n  groupByKey { case (time, value) =&gt; value % 2 }\n\nscala&gt; :type numGroups\norg.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)]\n</code></pre> <p><code>KeyValueGroupedDataset</code> is also &lt;&gt; for &lt;&gt; and &lt;&gt; operators. <pre><code>scala&gt; :type numGroups\norg.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)]\n\nscala&gt; :type numGroups.keyAs[String]\norg.apache.spark.sql.KeyValueGroupedDataset[String,(java.sql.Timestamp, Long)]\n</code></pre> <pre><code>scala&gt; :type numGroups\norg.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)]\n\nval mapped = numGroups.mapValues { case (ts, n) =&gt; s\"($ts, $n)\" }\nscala&gt; :type mapped\norg.apache.spark.sql.KeyValueGroupedDataset[Long,String]\n</code></pre> <p><code>KeyValueGroupedDataset</code> works for batch and streaming aggregations, but shines the most when used for Streaming Aggregation.</p> <pre><code>scala&gt; :type numGroups\norg.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)]\n\nimport org.apache.spark.sql.streaming.Trigger\nimport scala.concurrent.duration._\nnumGroups.\n  mapGroups { case(group, values) =&gt; values.size }.\n  writeStream.\n  format(\"console\").\n  trigger(Trigger.ProcessingTime(10.seconds)).\n  start\n\n-------------------------------------------\nBatch: 0\n-------------------------------------------\n+-----+\n|value|\n+-----+\n+-----+\n\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+-----+\n|value|\n+-----+\n|    3|\n|    2|\n+-----+\n\n-------------------------------------------\nBatch: 2\n-------------------------------------------\n+-----+\n|value|\n+-----+\n|    5|\n|    5|\n+-----+\n\n// Eventually...\nspark.streams.active.foreach(_.stop)\n</code></pre> <p>The most prestigious use case of <code>KeyValueGroupedDataset</code> however is Arbitrary Stateful Streaming Aggregation that allows for accumulating streaming state (by means of GroupState) using &lt;&gt; and the more advanced &lt;&gt; operators. <p>[[operators]] .KeyValueGroupedDataset's Operators [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Operator | Description</p> <p>| agg a| [[agg]]</p>","location":"KeyValueGroupedDataset/"},{"title":"[source, scala]","text":"<p>aggU1: Dataset[(K, U1)] aggU1, U2: Dataset[(K, U1, U2)] aggU1, U2, U3: Dataset[(K, U1, U2, U3)] aggU1, U2, U3, U4: Dataset[(K, U1, U2, U3, U4)]</p>  <p>| cogroup a| [[cogroup]]</p>","location":"KeyValueGroupedDataset/#source-scala"},{"title":"[source, scala]","text":"<p>cogroupU, R : Encoder(   f: (K, Iterator[V], Iterator[U]) =&gt; TraversableOnce[R]): Dataset[R]</p>  <p>| count a| [[count]]</p>","location":"KeyValueGroupedDataset/#source-scala_1"},{"title":"[source, scala]","text":"","location":"KeyValueGroupedDataset/#source-scala_2"},{"title":"count(): Dataset[(K, Long)]","text":"<p>| flatMapGroups a| [[flatMapGroups]]</p>","location":"KeyValueGroupedDataset/#count-datasetk-long"},{"title":"[source, scala]","text":"","location":"KeyValueGroupedDataset/#source-scala_3"},{"title":"flatMapGroupsU : Encoder: Dataset[U]","text":"<p>| flatMapGroupsWithState a| [[flatMapGroupsWithState]]</p> <pre><code>flatMapGroupsWithState[S: Encoder, U: Encoder](\n  outputMode: OutputMode,\n  timeoutConf: GroupStateTimeout)(\n  func: (K, Iterator[V], GroupState[S]) =&gt; Iterator[U]): Dataset[U]\n</code></pre> <p>Arbitrary Stateful Streaming Aggregation - streaming aggregation with explicit state and state timeout</p>  <p>Note</p> <p>The difference between this <code>flatMapGroupsWithState</code> and mapGroupsWithState operators is the state function that generates zero or more elements (that are in turn the rows in the result streaming <code>Dataset</code>).</p>  <p>| keyAs a| [[keyAs]]</p>","location":"KeyValueGroupedDataset/#flatmapgroupsu-encoder-datasetu"},{"title":"[source, scala]","text":"<p>keys: Dataset[K] keyAs[L : Encoder]: KeyValueGroupedDataset[L, V]</p>  <p>| mapGroups a| [[mapGroups]]</p>","location":"KeyValueGroupedDataset/#source-scala_4"},{"title":"[source, scala]","text":"","location":"KeyValueGroupedDataset/#source-scala_5"},{"title":"mapGroupsU : Encoder: Dataset[U]","text":"<p>| spark-sql-streaming-KeyValueGroupedDataset-mapGroupsWithState.md[mapGroupsWithState] a| [[mapGroupsWithState]]</p> <pre><code>mapGroupsWithState[S: Encoder, U: Encoder](\n  func: (K, Iterator[V], GroupState[S]) =&gt; U): Dataset[U]\nmapGroupsWithState[S: Encoder, U: Encoder](\n  timeoutConf: GroupStateTimeout)(\n  func: (K, Iterator[V], GroupState[S]) =&gt; U): Dataset[U]\n</code></pre> <p>Creates a new <code>Dataset</code> with FlatMapGroupsWithState logical operator</p>  <p>Note</p> <p>The difference between <code>mapGroupsWithState</code> and flatMapGroupsWithState is the state function that generates exactly one element (that is in turn the row in the result <code>Dataset</code>).</p>  <p>| mapValues a| [[mapValues]]</p>","location":"KeyValueGroupedDataset/#mapgroupsu-encoder-datasetu"},{"title":"[source, scala]","text":"","location":"KeyValueGroupedDataset/#source-scala_6"},{"title":"mapValuesW : Encoder: KeyValueGroupedDataset[K, W]","text":"<p>| reduceGroups a| [[reduceGroups]]</p>","location":"KeyValueGroupedDataset/#mapvaluesw-encoder-keyvaluegroupeddatasetk-w"},{"title":"[source, scala]","text":"","location":"KeyValueGroupedDataset/#source-scala_7"},{"title":"reduceGroups(f: (V, V) =&gt; V): Dataset[(K, V)]","text":"<p>|===</p>","location":"KeyValueGroupedDataset/#reducegroupsf-v-v-v-datasetk-v"},{"title":"MetadataLog","text":"<p><code>MetadataLog</code> is an abstraction of metadata logs that can add, get, getLatest and purge metadata (of type <code>T</code>).</p>  Type Constructor <p><code>MetadataLog[T]</code> is a Scala type constructor with the type parameter <code>T</code></p>","location":"MetadataLog/"},{"title":"Contract","text":"","location":"MetadataLog/#contract"},{"title":"Storing Metadata of Streaming Batch <pre><code>add(\n  batchId: Long,\n  metadata: T): Boolean\n</code></pre> <p>Stores (adds) metadata of a streaming batch</p> <p>Used when:</p> <ul> <li> <p><code>KafkaMicroBatchStream</code> is requested to getOrCreateInitialPartitionOffsets</p> </li> <li> <p><code>KafkaSource</code> is requested for the initialPartitionOffsets</p> </li> <li> <p><code>CompactibleFileStreamLog</code> is requested for the store metadata of a streaming batch and to compact</p> </li> <li> <p><code>FileStreamSource</code> is requested to fetchMaxOffset</p> </li> <li> <p><code>FileStreamSourceLog</code> is requested to store (add) metadata of a streaming batch</p> </li> <li> <p><code>ManifestFileCommitProtocol</code> is requested to commitJob</p> </li> <li> <p><code>MicroBatchExecution</code> stream execution engine is requested to &lt;&gt; and &lt;&gt;  <li> <p><code>ContinuousExecution</code> stream execution engine is requested to &lt;&gt; and &lt;&gt;","text":"","location":"MetadataLog/#storing-metadata-of-streaming-batch"},{"title":"get <pre><code>get(\n  batchId: Long): Option[T]\nget(\n  startId: Option[Long],\n  endId: Option[Long]): Array[(Long, T)]\n</code></pre> <p>Looks up (gets) metadata of one or more streaming batches</p> <p>Used when...FIXME</p>","text":"","location":"MetadataLog/#get"},{"title":"getLatest <pre><code>getLatest(): Option[(Long, T)]\n</code></pre> <p>Looks up the latest-committed metadata (if available)</p> <p>Used when...FIXME</p>","text":"","location":"MetadataLog/#getlatest"},{"title":"purge <pre><code>purge(\n  thresholdBatchId: Long): Unit\n</code></pre> <p>Purging (removing) metadata older than the given threshold</p> <p>Used when...FIXME</p>","text":"","location":"MetadataLog/#purge"},{"title":"Implementations","text":"<ul> <li>HDFSMetadataLog</li> </ul>","location":"MetadataLog/#implementations"},{"title":"MicroBatchStream","text":"<p><code>MicroBatchStream</code>\u00a0is an extension of the SparkDataStream abstraction for streaming sources for Micro-Batch Stream Processing.</p>","location":"MicroBatchStream/"},{"title":"Contract","text":"","location":"MicroBatchStream/#contract"},{"title":"Creating PartitionReaderFactory <pre><code>PartitionReaderFactory createReaderFactory()\n</code></pre> <p><code>PartitionReaderFactory</code> (Spark SQL)</p> <p>Used when:</p> <ul> <li><code>MicroBatchScanExec</code> physical operator is requested for a PartitionReaderFactory</li> </ul>","text":"","location":"MicroBatchStream/#creating-partitionreaderfactory"},{"title":"Latest Offset <pre><code>Offset latestOffset()\n</code></pre> <p>Latest Offset</p> <p>Used when:</p> <ul> <li><code>MicroBatchExecution</code> is requested to constructing or skipping next streaming micro-batch</li> </ul>","text":"","location":"MicroBatchStream/#latest-offset"},{"title":"Input Partitions <pre><code>InputPartition[] planInputPartitions(\n  Offset start,\n  Offset end)\n</code></pre> <p><code>InputPartition</code>s (Spark SQL)</p> <p>Used when:</p> <ul> <li><code>MicroBatchScanExec</code> physical operator is requested for input partitions</li> </ul>","text":"","location":"MicroBatchStream/#input-partitions"},{"title":"Implementations","text":"<ul> <li><code>AvailableNowMicroBatchStreamWrapper</code></li> <li>KafkaMicroBatchStream</li> <li>MemoryStream</li> <li><code>RatePerMicroBatchStream</code></li> <li><code>RateStreamMicroBatchStream</code></li> <li><code>TextSocketMicroBatchStream</code></li> </ul>","location":"MicroBatchStream/#implementations"},{"title":"Offset \u2014 Read Position of Streaming Query","text":"<p><code>Offset</code> is an abstraction of stream positions.</p>  <p>Note</p> <p>There are two <code>Offset</code> abstractions and new streaming data sources should use Data Source v2 API.</p>","location":"Offset/"},{"title":"Contract","text":"","location":"Offset/#contract"},{"title":"JSON Representation <pre><code>String json()\n</code></pre> <p>JSON-encoded representation of the offset</p> <p>Used when:</p> <ul> <li><code>MicroBatchExecution</code> stream execution engine is requested to construct the next streaming micro-batch and run a streaming micro-batch</li> <li><code>OffsetSeq</code> is requested for the textual representation</li> <li><code>OffsetSeqLog</code> is requested to serialize metadata (write metadata in serialized format)</li> <li><code>ProgressReporter</code> is requested to record trigger offsets</li> <li><code>ContinuousExecution</code> stream execution engine is requested to run a streaming query in continuous mode and commit an epoch</li> </ul>","text":"","location":"Offset/#json-representation"},{"title":"Implementations","text":"<ul> <li>ContinuousMemoryStreamOffset</li> <li>FileStreamSourceOffset</li> <li>KafkaSourceOffset</li> <li>LongOffset</li> <li>SerializedOffset</li> <li>TextSocketOffset</li> </ul>","location":"Offset/#implementations"},{"title":"OffsetSeq","text":"<p><code>OffsetSeq</code> is the metadata managed by Hadoop DFS-based metadata storage.</p> <p><code>OffsetSeq</code> is &lt;&gt; (possibly using the &lt;&gt; factory methods) when: <ul> <li> <p><code>OffsetSeqLog</code> is requested to deserialize metadata (retrieve metadata from a persistent storage)</p> </li> <li> <p><code>StreamProgress</code> is requested to convert itself to OffsetSeq (most importantly when <code>MicroBatchExecution</code> stream execution engine is requested to construct the next streaming micro-batch to commit available offsets for a batch to the write-ahead log)</p> </li> <li> <p><code>ContinuousExecution</code> stream execution engine is requested to &lt;&gt; and &lt;&gt;","location":"OffsetSeq/"},{"title":"Creating Instance","text":"<p><code>OffsetSeq</code> takes the following when created:</p> <ul> <li>[[offsets]] Collection of optional Offsets (with <code>None</code> for &lt;&gt;) <li>[[metadata]] Optional OffsetSeqMetadata (default: <code>None</code>)</li>  <p>=== [[toStreamProgress]] Converting to StreamProgress -- <code>toStreamProgress</code> Method</p>","location":"OffsetSeq/#creating-instance"},{"title":"[source, scala]","text":"<p>toStreamProgress(   sources: Seq[BaseStreamingSource]): StreamProgress</p>  <p><code>toStreamProgress</code> creates a new StreamProgress and adds the streaming sources for which there are new offsets available.</p> <p>NOTE: &lt;&gt; is a collection with holes (empty elements) for streaming sources with no new data available. <p><code>toStreamProgress</code> throws an <code>AssertionError</code> if the number of the input <code>sources</code> does not match the &lt;&gt;: <pre><code>There are [[offsets.size]] sources in the checkpoint offsets and now there are [[sources.size]] sources requested by the query. Cannot continue.\n</code></pre> <p><code>toStreamProgress</code> is used when:</p> <ul> <li> <p><code>MicroBatchExecution</code> is requested to &lt;&gt; and &lt;&gt;  <li> <p><code>ContinuousExecution</code> is requested for &lt;&gt;   <p>=== [[toString]] Textual Representation -- <code>toString</code> Method</p>","location":"OffsetSeq/#source-scala"},{"title":"[source, scala]","text":"","location":"OffsetSeq/#source-scala_1"},{"title":"toString: String","text":"<p>NOTE: <code>toString</code> is part of the ++https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object] contract for the string representation of the object.</p> <p><code>toString</code> simply converts the &lt;&gt; to JSON (if an offset is available) or <code>-</code> (a dash if an offset is not available for a streaming source at that position). <p>=== [[fill]] Creating OffsetSeq Instance -- <code>fill</code> Factory Methods</p>","location":"OffsetSeq/#tostring-string"},{"title":"[source, scala]","text":"<p>fill(   offsets: Offset*): OffsetSeq // &lt;1&gt; fill(   metadata: Option[String],   offsets: Offset*): OffsetSeq</p>  <p>&lt;1&gt; Uses no metadata (<code>None</code>)</p> <p><code>fill</code> simply creates an &lt;&gt; for the given variable sequence of Offsets and the optional OffsetSeqMetadata (in JSON format). <p><code>fill</code> is used when:</p> <ul> <li> <p><code>OffsetSeqLog</code> is requested to deserialize metadata</p> </li> <li> <p><code>ContinuousExecution</code> stream execution engine is requested to get start offsets and addOffset</p> </li> </ul>","location":"OffsetSeq/#source-scala_2"},{"title":"OffsetSeqLog \u2014 Hadoop DFS-based Metadata Storage of OffsetSeqs","text":"<p><code>OffsetSeqLog</code> is a Hadoop DFS-based metadata storage for OffsetSeq metadata.</p> <p><code>OffsetSeqLog</code> is created as the write-ahead log (WAL) of offsets of streaming query execution engines.</p> <p>[[OffsetSeq]][[offsets]][[metadata]] <code>OffsetSeqLog</code> uses OffsetSeq for metadata which holds an ordered collection of offsets and optional metadata (as OffsetSeqMetadata for event-time watermark).</p> <p>[[VERSION]] <code>OffsetSeqLog</code> uses <code>1</code> for the version when &lt;&gt; and &lt;&gt; metadata.","location":"OffsetSeqLog/"},{"title":"Creating Instance","text":"<p><code>OffsetSeqLog</code> takes the following to be created:</p> <ul> <li>[[sparkSession]] <code>SparkSession</code></li> <li>[[path]] Path of the metadata log directory</li> </ul> <p>=== [[serialize]] Serializing Metadata (Writing Metadata in Serialized Format) -- <code>serialize</code> Method</p>","location":"OffsetSeqLog/#creating-instance"},{"title":"[source, scala]","text":"<p>serialize(   offsetSeq: OffsetSeq,   out: OutputStream): Unit</p>  <p><code>serialize</code> firstly writes out the &lt;&gt; prefixed with <code>v</code> on a single line (e.g. <code>v1</code>) followed by the optional metadata in JSON format. <p><code>serialize</code> then writes out the offsets in JSON format, one per line.</p> <p>NOTE: No offsets to write in <code>offsetSeq</code> for a streaming source is marked as - (a dash) in the log.</p> <pre><code>$ ls -tr [checkpoint-directory]/offsets\n0 1 2 3 4 5 6\n\n$ cat [checkpoint-directory]/offsets/6\nv1\n{\"batchWatermarkMs\":0,\"batchTimestampMs\":1502872590006,\"conf\":{\"spark.sql.shuffle.partitions\":\"200\",\"spark.sql.streaming.stateStore.providerClass\":\"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\"}}\n51\n</code></pre> <p><code>serialize</code> is part of HDFSMetadataLog abstraction.</p> <p>=== [[deserialize]] Deserializing Metadata (Reading OffsetSeq from Serialized Format) -- <code>deserialize</code> Method</p>","location":"OffsetSeqLog/#source-scala"},{"title":"[source, scala]","text":"","location":"OffsetSeqLog/#source-scala_1"},{"title":"deserialize(in: InputStream): OffsetSeq","text":"<p><code>deserialize</code> firstly parses the &lt;&gt; on the first line. <p><code>deserialize</code> reads the optional metadata (with an empty line for metadata not available).</p> <p><code>deserialize</code> creates a SerializedOffset for every line left.</p> <p>In the end, <code>deserialize</code> creates a OffsetSeq for the optional metadata and the <code>SerializedOffsets</code>.</p> <p>When there are no lines in the <code>InputStream</code>, <code>deserialize</code> throws an <code>IllegalStateException</code>:</p> <pre><code>Incomplete log file\n</code></pre> <p><code>deserialize</code> is part of HDFSMetadataLog abstraction.</p>","location":"OffsetSeqLog/#deserializein-inputstream-offsetseq"},{"title":"OffsetSeqMetadata","text":"<p><code>OffsetSeqMetadata</code> is the metadata of a streaming batch.</p> <p><code>OffsetSeqMetadata</code> is persisted in the write-ahead offset log.</p>","location":"OffsetSeqMetadata/"},{"title":"Creating Instance","text":"<p><code>OffsetSeqMetadata</code> takes the following to be created:</p> <ul> <li>Batch Watermark</li> <li>Batch Timestamp</li> <li> Configuration (default: empty)  <p><code>OffsetSeqMetadata</code> is created using apply.</p>","location":"OffsetSeqMetadata/#creating-instance"},{"title":"Batch Watermark <p><code>OffsetSeqMetadata</code> is given the current batch's event-time watermark when created. Unless given, it is assumed <code>0</code>.</p>","text":"","location":"OffsetSeqMetadata/#batch-watermark"},{"title":"Batch Timestamp <p><code>OffsetSeqMetadata</code> is given the current batch's batch timestamp when created. Unless given, it is assumed <code>0</code>.</p>","text":"","location":"OffsetSeqMetadata/#batch-timestamp"},{"title":"Creating OffsetSeqMetadata <pre><code>apply(\n  batchWatermarkMs: Long,\n  batchTimestampMs: Long,\n  sessionConf: RuntimeConfig): OffsetSeqMetadata\n</code></pre> <p><code>apply</code> find the relevantSQLConfs in the given <code>RuntimeConfig</code> and creates a OffsetSeqMetadata (with the batch watermark and timestamp, and the relevant configs found in the session config).</p>  <p><code>apply</code> is used when:</p> <ul> <li><code>MicroBatchExecution</code> is requested to populateStartOffsets (while restarting a streaming query with a checkpointed offsets)</li> <li><code>StreamExecution</code> is created and requested to runStream</li> </ul>","text":"","location":"OffsetSeqMetadata/#creating-offsetseqmetadata"},{"title":"Checkpointed Properties <p><code>OffsetSeqMetadata</code> allows the following configuration properties to be once-only settable that can only be set once and can never change after a streaming query is started.</p> <ul> <li><code>spark.sql.shuffle.partitions</code> (Spark SQL)</li> <li>spark.sql.streaming.aggregation.stateFormatVersion</li> <li>spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion</li> <li>spark.sql.streaming.join.stateFormatVersion</li> <li>spark.sql.streaming.multipleWatermarkPolicy</li> <li>spark.sql.streaming.statefulOperator.useStrictDistribution</li> <li>spark.sql.streaming.stateStore.compression.codec</li> <li>spark.sql.streaming.stateStore.providerClass</li> <li>spark.sql.streaming.stateStore.rocksdb.formatVersion</li> </ul> <p>The configuration properties are searched for while creating an OffsetSeqMetadata.</p> <p>The values of these configs are persisted into the offset log in the checkpoint position.</p> <p>Once persisted in a checkpoint location, restarting a streaming query will make the persisted values be in effect again (overriding any current values of the properties if set).</p>","text":"","location":"OffsetSeqMetadata/#checkpointed-properties"},{"title":"Updating RuntimeConfig with Metadata Properties <pre><code>setSessionConf(\n  metadata: OffsetSeqMetadata,\n  sessionConf: RuntimeConfig): Unit\n</code></pre> <p>For any relevant SQL property set in the given OffsetSeqMetadata, <code>setSessionConf</code> overrides the value in the given <code>RuntimeConfig</code> if set. <code>setSessionConf</code> prints out the following WARN message to the logs:</p> <pre><code>Updating the value of conf '[confKey]' in current session from '[sessionValue]' to '[metadataValue]'.\n</code></pre> <p>When not set in the given OffsetSeqMetadata, <code>setSessionConf</code> makes the default values effective in the given <code>RuntimeConfig</code>. <code>setSessionConf</code> prints out the following WARN message to the logs:</p> <pre><code>Conf '[confKey]' was not found in the offset log, using default value '[defaultValue]'\n</code></pre> <p>Otherwise, <code>setSessionConf</code> prints out one of the following WARN messages to the logs based on whether a relevant property is set in the given <code>RuntimeConfig</code> or not.</p> <pre><code>Conf '[confKey]' was not found in the offset log. Using existing session conf value '[v]'.\n</code></pre> <pre><code>Conf '[confKey]' was not found in the offset log. No value set in session conf.\n</code></pre>  <p><code>setSessionConf</code> is used when:</p> <ul> <li><code>MicroBatchExecution</code> is requested to populateStartOffsets (while restarting a streaming query with a checkpointed offsets)</li> </ul>","text":"","location":"OffsetSeqMetadata/#updating-runtimeconfig-with-metadata-properties"},{"title":"OutputMode","text":"<p><code>OutputMode</code> of a streaming query described what data is written to a streaming sink.</p> <p><code>OutputMode</code> is specified using DataStreamWriter.outputMode.</p>","location":"OutputMode/"},{"title":"Output Modes","text":"","location":"OutputMode/#output-modes"},{"title":"Append <p>Append (alias: <code>append</code>) is the default output mode that writes \"new\" rows only.</p> <p>In streaming aggregations, a \"new\" row is when the intermediate state becomes final, i.e. when new events for the grouping key can only be considered late which is when watermark moves past the event time of the key.</p> <p><code>Append</code> output mode requires that a streaming query defines event-time watermark (using withWatermark operator) on the event time column that is used in aggregation (directly or using window standard function).</p> <p>Required for datasets with <code>FileFormat</code> format (to create FileStreamSink)</p> <p><code>Append</code> is mandatory when multiple <code>flatMapGroupsWithState</code> operators are used in a structured query.</p>","text":"","location":"OutputMode/#append"},{"title":"Complete <p>Complete (alias: <code>complete</code>) writes all the rows of a Result Table (and corresponds to a traditional batch structured query).</p> <p>Complete mode does not drop old aggregation state and preserves all data in the Result Table.</p> <p>Supported only for streaming aggregations (as asserted by UnsupportedOperationChecker).</p>","text":"","location":"OutputMode/#complete"},{"title":"Update <p>Update (alias: <code>update</code>) writes only the rows that were updated (every time there are updates).</p> <p>For queries that are not streaming aggregations, <code>Update</code> is equivalent to the Append output mode.</p>","text":"","location":"OutputMode/#update"},{"title":"PartitionOffset","text":"<p>== [[PartitionOffset]] PartitionOffset</p> <p><code>PartitionOffset</code> is...FIXME</p>","location":"PartitionOffset/"},{"title":"ReportsSourceMetrics","text":"<p><code>ReportsSourceMetrics</code> is...FIXME</p>","location":"ReportsSourceMetrics/"},{"title":"SQLConf","text":"<p><code>SQLConf</code> is an internal configuration store for parameters and hints used to configure a Spark Structured Streaming application (and Spark SQL applications in general).</p>  <p>Tip</p> <p>Find out more on SQLConf in The Internals of Spark SQL.</p>","location":"SQLConf/"},{"title":"FLATMAPGROUPSWITHSTATE_STATE_FORMAT_VERSION <p>spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion</p>","text":"","location":"SQLConf/#flatmapgroupswithstate_state_format_version"},{"title":"STATEFUL_OPERATOR_USE_STRICT_DISTRIBUTION <p>spark.sql.streaming.statefulOperator.useStrictDistribution</p>","text":"","location":"SQLConf/#stateful_operator_use_strict_distribution"},{"title":"stateStoreProviderClass <p>spark.sql.streaming.stateStore.providerClass</p>","text":"","location":"SQLConf/#statestoreproviderclass"},{"title":"STREAMING_AGGREGATION_STATE_FORMAT_VERSION <p>spark.sql.streaming.aggregation.stateFormatVersion</p>","text":"","location":"SQLConf/#streaming_aggregation_state_format_version"},{"title":"streamingFileCommitProtocolClass <p>spark.sql.streaming.commitProtocolClass configuration property</p> <p>Used when <code>FileStreamSink</code> is requested to \"add\" a batch of data</p>","text":"","location":"SQLConf/#streamingfilecommitprotocolclass"},{"title":"streamingMetricsEnabled <p>spark.sql.streaming.metricsEnabled configuration property</p> <p>Used when:</p> <ul> <li><code>StreamExecution</code> is requested to runStream</li> </ul>","text":"","location":"SQLConf/#streamingmetricsenabled"},{"title":"streamingNoDataMicroBatchesEnabled <p>spark.sql.streaming.noDataMicroBatches.enabled</p> <p>Used when:</p> <ul> <li><code>MicroBatchExecution</code> stream execution engine is requested to run an activated streaming query</li> </ul>","text":"","location":"SQLConf/#streamingnodatamicrobatchesenabled"},{"title":"fileSinkLogCleanupDelay <p>spark.sql.streaming.fileSink.log.cleanupDelay configuration property</p> <p>Used when FileStreamSinkLog is created</p>","text":"","location":"SQLConf/#filesinklogcleanupdelay"},{"title":"fileSinkLogDeletion <p>spark.sql.streaming.fileSink.log.deletion configuration property</p> <p>Used when FileStreamSinkLog is created</p>","text":"","location":"SQLConf/#filesinklogdeletion"},{"title":"fileSinkLogCompactInterval <p>spark.sql.streaming.fileSink.log.compactInterval configuration property</p> <p>Used when FileStreamSinkLog is created</p>","text":"","location":"SQLConf/#filesinklogcompactinterval"},{"title":"minBatchesToRetain <p>spark.sql.streaming.minBatchesToRetain configuration property</p> <p>Used when:</p> <ul> <li> <p><code>CompactibleFileStreamLog</code> is created</p> </li> <li> <p>StreamExecution is created</p> </li> <li> <p><code>StateStoreConf</code> is created</p> </li> </ul> <p>[[accessor-methods]] .SQLConf's Property Accessor Methods [cols=\"1,1\",options=\"header\",width=\"100%\"] |=== | Method Name / Property | Description</p> <p>| <code>continuousStreamingExecutorQueueSize</code></p> <p>spark.sql.streaming.continuous.executorQueueSize</p> <p>a| [[continuousStreamingExecutorQueueSize]] Used when:</p> <ul> <li> <p><code>DataSourceV2ScanExec</code> leaf physical operator is requested for the input RDDs (and creates a &lt;&gt;)  <li> <p><code>ContinuousCoalesceExec</code> unary physical operator is requested to execute</p> </li>  <p>| <code>continuousStreamingExecutorPollIntervalMs</code></p> <p>spark.sql.streaming.continuous.executorPollIntervalMs</p> <p>a| [[continuousStreamingExecutorPollIntervalMs]] Used exclusively when <code>DataSourceV2ScanExec</code> leaf physical operator is requested for the input RDDs (and creates a &lt;&gt;) <p>| <code>disabledV2StreamingMicroBatchReaders</code></p> <p>spark.sql.streaming.disabledV2MicroBatchReaders</p> <p>a| [[disabledV2StreamingMicroBatchReaders]] Used exclusively when <code>MicroBatchExecution</code> is requested for the &lt;&gt; (of a streaming query) <p>| <code>fileSourceLogDeletion</code></p> <p>spark.sql.streaming.fileSource.log.deletion</p> <p>a| [[fileSourceLogDeletion]][[FILE_SOURCE_LOG_DELETION]] Used exclusively when <code>FileStreamSourceLog</code> is requested for the isDeletingExpiredLog</p> <p>| <code>fileSourceLogCleanupDelay</code></p> <p>spark.sql.streaming.fileSource.log.cleanupDelay</p> <p>a| [[fileSourceLogCleanupDelay]][[FILE_SOURCE_LOG_CLEANUP_DELAY]] Used exclusively when <code>FileStreamSourceLog</code> is requested for the fileCleanupDelayMs</p> <p>| <code>fileSourceLogCompactInterval</code></p> <p>spark.sql.streaming.fileSource.log.compactInterval</p> <p>a| [[fileSourceLogCompactInterval]][[FILE_SOURCE_LOG_COMPACT_INTERVAL]] Used exclusively when <code>FileStreamSourceLog</code> is requested for the default compaction interval</p> <p>| <code>stateStoreMinDeltasForSnapshot</code></p> <p>spark.sql.streaming.stateStore.minDeltasForSnapshot</p> <p>a| [[stateStoreMinDeltasForSnapshot]] Used (as StateStoreConf.minDeltasForSnapshot) exclusively when <code>HDFSBackedStateStoreProvider</code> is requested to doSnapshot</p> <p>| <code>STREAMING_CHECKPOINT_FILE_MANAGER_CLASS</code></p> <p>spark.sql.streaming.checkpointFileManagerClass a| [[STREAMING_CHECKPOINT_FILE_MANAGER_CLASS]] Used exclusively when <code>CheckpointFileManager</code> helper object is requested to create a CheckpointFileManager</p> <p>| <code>streamingMetricsEnabled</code></p> <p>spark.sql.streaming.metricsEnabled</p> <p>a| [[streamingMetricsEnabled]] Used exclusively when <code>StreamExecution</code> is requested for runStream (to control whether to register a metrics reporter for a streaming query)</p> <p>| <code>STREAMING_MULTIPLE_WATERMARK_POLICY</code></p> <p>spark.sql.streaming.multipleWatermarkPolicy</p> <p>a| [[STREAMING_MULTIPLE_WATERMARK_POLICY]]</p> <p>| <code>streamingNoDataProgressEventInterval</code></p> <p>spark.sql.streaming.noDataProgressEventInterval</p> <p>a| [[streamingNoDataProgressEventInterval]] Used exclusively for ProgressReporter</p> <p>| <code>streamingPollingDelay</code></p> <p>spark.sql.streaming.pollingDelay</p> <p>a| [[streamingPollingDelay]][[STREAMING_POLLING_DELAY]] Used exclusively when StreamExecution is created</p> <p>| <code>streamingProgressRetention</code></p> <p>spark.sql.streaming.numRecentProgressUpdates</p> <p>a| [[streamingProgressRetention]][[STREAMING_PROGRESS_RETENTION]] Used exclusively when <code>ProgressReporter</code> is requested to update progress of streaming query (and possibly remove an excess)</p> <p>|===</p>","text":"","location":"SQLConf/#minbatchestoretain"},{"title":"Sink","text":"<p><code>Sink</code>\u00a0is an extension of the <code>Table</code> abstraction for streaming sinks that add the batch results of a streaming query in Micro-Batch Stream Processing.</p>  <p>Note</p> <p><code>Sink</code>\u00a0extends <code>Table</code> interface for the only purpose of making it compatible with Data Source V2. All <code>Table</code> methods simply throw an <code>IllegalStateException</code>.</p>","location":"Sink/"},{"title":"Contract","text":"","location":"Sink/#contract"},{"title":"Adding Batch <pre><code>addBatch(\n  batchId: Long,\n  data: DataFrame): Unit\n</code></pre> <p>Adds a batch of data to the sink</p> <p>Used when <code>MicroBatchExecution</code> stream execution engine is requested to add a batch to a sink (addBatch phase) (while running micro-batches of a streaming query)</p>","text":"","location":"Sink/#adding-batch"},{"title":"Implementations","text":"<ul> <li>FileStreamSink</li> <li>ForeachBatchSink</li> <li>KafkaSink</li> </ul>","location":"Sink/#implementations"},{"title":"Source \u2014 Streaming Source in Micro-Batch Stream Processing","text":"<p><code>Source</code> is an extension of the SparkDataStream abstraction for streaming sources for \"streamed reading\" of continually arriving data in a streaming query (identified by offset).</p> <p><code>Source</code> is used in Micro-Batch Stream Processing.</p> <p><code>Source</code> is created using StreamSourceProvider.createSource (and DataSource.createSource).</p> <p>For fault tolerance, <code>Source</code> must be able to replay an arbitrary sequence of past data in a stream using a range of offsets. This is the assumption so Structured Streaming can achieve end-to-end exactly-once guarantees.</p>","location":"Source/"},{"title":"Contract","text":"","location":"Source/#contract"},{"title":"commit <pre><code>commit(\n  end: Offset): Unit\n</code></pre> <p>Commits data up to the given end offset (informs the source that Spark has completed processing all data for offsets less than or equal to the end offset and will only request offsets greater than the end offset in the future).</p> <p>Used when:</p> <ul> <li>MicroBatchExecution stream execution engine is requested to write offsets to a commit log (walCommit phase) while running an activated streaming query</li> </ul>","text":"","location":"Source/#commit"},{"title":"getBatch <pre><code>getBatch(\n  start: Option[Offset],\n  end: Offset): DataFrame\n</code></pre> <p>Generating a streaming <code>DataFrame</code> with data between the start and end offsets</p> <p>Start offset can be undefined (<code>None</code>) to indicate that the batch should begin with the first record</p> <p>Used when MicroBatchExecution stream execution engine is requested to run an activated streaming query, namely:</p> <ul> <li> <p>Populate start offsets from checkpoint (resuming from checkpoint)</p> </li> <li> <p>Request unprocessed data from all sources (getBatch phase)</p> </li> </ul>","text":"","location":"Source/#getbatch"},{"title":"getOffset <pre><code>getOffset: Option[Offset]\n</code></pre> <p>Latest (maximum) offset of the source (or <code>None</code> to denote no data)</p> <p>Used when:</p> <ul> <li>MicroBatchExecution stream execution engine (Micro-Batch Stream Processing) is requested for latest offsets of all sources (getOffset phase) while running activated streaming query</li> </ul>","text":"","location":"Source/#getoffset"},{"title":"schema <pre><code>schema: StructType\n</code></pre> <p>Schema of the data from this source</p>","text":"","location":"Source/#schema"},{"title":"Implementations","text":"<ul> <li>FileStreamSource</li> <li>KafkaSource</li> </ul>","location":"Source/#implementations"},{"title":"initialOffset Method <pre><code>initialOffset(): OffsetV2\n</code></pre> <p><code>initialOffset</code> throws an <code>IllegalStateException</code>.</p> <p><code>initialOffset</code> is part of the SparkDataStream abstraction.</p>","text":"","location":"Source/#initialoffset-method"},{"title":"deserializeOffset Method <pre><code>deserializeOffset(\n  json: String): OffsetV2\n</code></pre> <p><code>deserializeOffset</code> throws an <code>IllegalStateException</code>.</p> <p><code>deserializeOffset</code> is part of the SparkDataStream abstraction.</p>","text":"","location":"Source/#deserializeoffset-method"},{"title":"SparkDataStream","text":"<p><code>SparkDataStream</code> is an abstraction of readable data streams.</p>","location":"SparkDataStream/"},{"title":"Contract","text":"","location":"SparkDataStream/#contract"},{"title":"commit <pre><code>void commit(\n  Offset end)\n</code></pre> <p>Used when:</p> <ul> <li><code>ContinuousExecution</code> stream execution engine is requested to commit</li> <li><code>MicroBatchExecution</code> stream execution engine is requested to constructNextBatch</li> </ul>","text":"","location":"SparkDataStream/#commit"},{"title":"deserializeOffset <pre><code>Offset deserializeOffset(\n  String json)\n</code></pre> <p>Used when:</p> <ul> <li><code>ContinuousExecution</code> stream execution engine is requested to runContinuous and commit</li> <li><code>MicroBatchExecution</code> stream execution engine is requested to constructNextBatch and runBatch</li> </ul>","text":"","location":"SparkDataStream/#deserializeoffset"},{"title":"initialOffset <pre><code>Offset initialOffset()\n</code></pre> <p>Used when:</p> <ul> <li><code>ContinuousExecution</code> stream execution engine is requested to runContinuous</li> <li><code>MicroBatchExecution</code> stream execution engine is requested to constructNextBatch and runBatch</li> </ul>","text":"","location":"SparkDataStream/#initialoffset"},{"title":"stop <pre><code>void stop()\n</code></pre> <p>Used when:</p> <ul> <li><code>StreamExecution</code> is requested to stop sources</li> </ul>","text":"","location":"SparkDataStream/#stop"},{"title":"Implementations","text":"<ul> <li>ContinuousStream</li> <li>MemoryStreamBase</li> <li>MicroBatchStream</li> <li>Source</li> <li>SupportsAdmissionControl</li> </ul>","location":"SparkDataStream/#implementations"},{"title":"StreamExecution","text":"<p><code>StreamExecution</code> is an abstraction of stream execution engines (streaming query processing engines) that can run a structured query (on a stream execution thread).</p> <p></p>  <p>Note</p> <p>Continuous query, streaming query, continuous Dataset, streaming Dataset are all considered high-level synonyms for an executable entity that stream execution engines run using the analyzed logical plan internally.</p>   <p>Important</p> <p><code>StreamExecution</code> does not support adaptive query execution and cost-based optimizer (and turns them off when requested to run stream processing).</p>  <p><code>StreamExecution</code> is the execution environment of a streaming query that is executed every trigger and in the end adds the results to a sink.</p> <p><code>StreamExecution</code> corresponds to a single streaming query with one or more streaming sources and exactly one streaming sink.</p> <pre><code>import org.apache.spark.sql.streaming.Trigger\nimport scala.concurrent.duration._\nval q = spark.\n  readStream.\n  format(\"rate\").\n  load.\n  writeStream.\n  format(\"console\").\n  trigger(Trigger.ProcessingTime(10.minutes)).\n  start\nscala&gt; :type q\norg.apache.spark.sql.streaming.StreamingQuery\n\n// Pull out StreamExecution off StreamingQueryWrapper\nimport org.apache.spark.sql.execution.streaming.{StreamExecution, StreamingQueryWrapper}\nval se = q.asInstanceOf[StreamingQueryWrapper].streamingQuery\nscala&gt; :type se\norg.apache.spark.sql.execution.streaming.StreamExecution\n</code></pre>","location":"StreamExecution/"},{"title":"Contract","text":"","location":"StreamExecution/#contract"},{"title":"Logical Plan <pre><code>logicalPlan: LogicalPlan\n</code></pre> <p>Analyzed logical plan of the streaming query to execute</p> <p>Used when <code>StreamExecution</code> is requested to run stream processing</p> <p><code>logicalPlan</code> is part of the ProgressReporter abstraction.</p>","text":"","location":"StreamExecution/#logical-plan"},{"title":"Running Activated Streaming Query <pre><code>runActivatedStream(\n  sparkSessionForStream: SparkSession): Unit\n</code></pre> <p>Executes (runs) the activated streaming query (that is described by the logical plan)</p> <p>Used when <code>StreamExecution</code> is requested to run the streaming query (when transitioning from <code>INITIALIZING</code> to <code>ACTIVE</code> state)</p>","text":"","location":"StreamExecution/#running-activated-streaming-query"},{"title":"Implementations","text":"<ul> <li>ContinuousExecution</li> <li>MicroBatchExecution</li> </ul>","location":"StreamExecution/#implementations"},{"title":"Creating Instance","text":"<p><code>StreamExecution</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> Name of the streaming query (can be <code>null</code>) <li> Path of the checkpoint directory (metadata directory) <li> Streaming query (not used due to logicalPlan) <li>Sink</li> <li> Trigger <li> <code>Clock</code> <li> OutputMode <li> <code>deleteCheckpointOnStop</code> flag (whether to delete the checkpoint directory on stop)   Abstract Class <p><code>StreamExecution</code>\u00a0is an abstract class and cannot be created directly. It is created indirectly for the concrete StreamExecutions.</p>","location":"StreamExecution/#creating-instance"},{"title":"Sink <pre><code>sink: Table\n</code></pre> <p><code>StreamExecution</code> is given a <code>Table</code> (Spark SQL) when created.</p> <p>The <code>Table</code> represents the sink this streaming query writes to.</p> <p><code>sink</code> is part of the ProgressReporter abstraction.</p>","text":"","location":"StreamExecution/#sink"},{"title":"Starting Streaming Query <pre><code>start(): Unit\n</code></pre> <p><code>start</code> starts a stream execution thread to run stream processing.</p> <p></p>  <p><code>start</code> prints out the following INFO message to the logs (with the prettyId and the checkpointRoot):</p> <pre><code>Starting [prettyId]. Use [checkpointRoot] to store the query checkpoint.\n</code></pre> <p><code>start</code> then starts the stream execution thread as a daemon thread (in its own execution thread on JVM).</p>  <p>Note</p> <p><code>start</code> uses Java's Thread.start to run the streaming query on a separate execution thread.</p>  <p>In the end, <code>start</code> pauses the main thread (using the latch) until <code>StreamExecution</code> is requested to run the streaming query (that sends a QueryStartedEvent to all streaming listeners followed by decrementing the count of the startLatch).</p>  <p><code>start</code> is used when:</p> <ul> <li><code>StreamingQueryManager</code> is requested to start a streaming query (when <code>DataStreamWriter</code> is requested to start an execution of the streaming query)</li> </ul>","text":"","location":"StreamExecution/#starting-streaming-query"},{"title":"Configuration Properties","text":"","location":"StreamExecution/#configuration-properties"},{"title":"s.s.s.minBatchesToRetain <p><code>StreamExecution</code> uses the spark.sql.streaming.minBatchesToRetain configuration property to allow the StreamExecutions to discard old log entries (from the offset and commit logs).</p>","text":"","location":"StreamExecution/#sssminbatchestoretain"},{"title":"s.s.s.pollingDelay <p><code>StreamExecution</code> uses spark.sql.streaming.pollingDelay configuration property to control how long to delay polling for new data (when no data was available to process in a batch).</p>","text":"","location":"StreamExecution/#ssspollingdelay"},{"title":"ProgressReporter <p><code>StreamExecution</code> is a ProgressReporter and reports status of the streaming query (when it starts, progresses and terminates) by posting <code>StreamingQueryListener</code> events.</p> <pre><code>import org.apache.spark.sql.streaming.Trigger\nimport scala.concurrent.duration._\nval sq = spark\n  .readStream\n  .text(\"server-logs\")\n  .writeStream\n  .format(\"console\")\n  .queryName(\"debug\")\n  .trigger(Trigger.ProcessingTime(20.seconds))\n  .start\n\n// Enable the log level to see the INFO and DEBUG messages\n// log4j.logger.org.apache.spark.sql.execution.streaming.StreamExecution=DEBUG\n\n17/06/18 21:21:07 INFO StreamExecution: Starting new streaming query.\n17/06/18 21:21:07 DEBUG StreamExecution: getOffset took 5 ms\n17/06/18 21:21:07 DEBUG StreamExecution: Stream running from {} to {}\n17/06/18 21:21:07 DEBUG StreamExecution: triggerExecution took 9 ms\n17/06/18 21:21:07 DEBUG StreamExecution: Execution stats: ExecutionStats(Map(),List(),Map())\n17/06/18 21:21:07 INFO StreamExecution: Streaming query made progress: {\n  \"id\" : \"8b57b0bd-fc4a-42eb-81a3-777d7ba5e370\",\n  \"runId\" : \"920b227e-6d02-4a03-a271-c62120258cea\",\n  \"name\" : \"debug\",\n  \"timestamp\" : \"2017-06-18T19:21:07.693Z\",\n  \"numInputRows\" : 0,\n  \"processedRowsPerSecond\" : 0.0,\n  \"durationMs\" : {\n    \"getOffset\" : 5,\n    \"triggerExecution\" : 9\n  },\n  \"stateOperators\" : [ ],\n  \"sources\" : [ {\n    \"description\" : \"FileStreamSource[file:/Users/jacek/dev/oss/spark/server-logs]\",\n    \"startOffset\" : null,\n    \"endOffset\" : null,\n    \"numInputRows\" : 0,\n    \"processedRowsPerSecond\" : 0.0\n  } ],\n  \"sink\" : {\n    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSink@2460208a\"\n  }\n}\n17/06/18 21:21:10 DEBUG StreamExecution: Starting Trigger Calculation\n17/06/18 21:21:10 DEBUG StreamExecution: getOffset took 3 ms\n17/06/18 21:21:10 DEBUG StreamExecution: triggerExecution took 3 ms\n17/06/18 21:21:10 DEBUG StreamExecution: Execution stats: ExecutionStats(Map(),List(),Map())\n</code></pre>","text":"","location":"StreamExecution/#progressreporter"},{"title":"Unique Streaming Sources <pre><code>uniqueSources: Map[SparkDataStream, ReadLimit]\n</code></pre> <p><code>StreamExecution</code> tracks the unique streaming data sources of a streaming query in the <code>uniqueSources</code> internal registry.</p> <p></p> <p>Used when <code>StreamExecution</code>:</p> <ul> <li>Constructs the next streaming micro-batch (and gets new offsets for every streaming data source)</li> <li>Stops all streaming data sources</li> </ul>","text":"","location":"StreamExecution/#unique-streaming-sources"},{"title":"Streaming Query Identifiers <p>The name, id and runId are all unique across all active queries (in a StreamingQueryManager). The difference is that:</p> <ul> <li> <p>name is optional and user-defined</p> </li> <li> <p>id is a UUID that is auto-generated at the time <code>StreamExecution</code> is created and persisted to <code>metadata</code> checkpoint file</p> </li> <li> <p>runId is a UUID that is auto-generated every time <code>StreamExecution</code> is created</p> </li> </ul>","text":"","location":"StreamExecution/#streaming-query-identifiers"},{"title":"Id <p><code>StreamExecution</code> is uniquely identified by an ID of the streaming query (which is the <code>id</code> of the StreamMetadata).</p> <p>Since the StreamMetadata is persisted (to the <code>metadata</code> file in the checkpoint directory), the streaming query ID \"survives\" query restarts as long as the checkpoint directory is preserved.</p>","text":"","location":"StreamExecution/#id"},{"title":"Run Id <p><code>StreamExecution</code> is uniquely identified by a run ID of the streaming query. A run ID is a randomly-generated 128-bit universally unique identifier (UUID) that is assigned at the time <code>StreamExecution</code> is created.</p> <p><code>runId</code> does not \"survive\" query restarts and will always be different yet unique (across all active queries).</p>","text":"","location":"StreamExecution/#run-id"},{"title":"StreamMetadata <p><code>StreamExecution</code> uses a StreamMetadata that is persisted in the <code>metadata</code> file in the checkpoint directory.</p> <p>If the <code>metadata</code> file is available it is read and is the way to recover the id of a streaming query when resumed (i.e. restarted after a failure or a planned stop).</p>","text":"","location":"StreamExecution/#streammetadata"},{"title":"Metadata Logs","text":"","location":"StreamExecution/#metadata-logs"},{"title":"Write-Ahead Offset Log <pre><code>offsetLog: OffsetSeqLog\n</code></pre> <p><code>offsetLog</code> is a Hadoop DFS-based metadata storage (of OffsetSeqs) with <code>offsets</code> metadata directory.</p> <p><code>offsetLog</code> is used as a Write-Ahead Log of Offsets to persist offsets of the data about to be processed in every trigger.</p>  <p>Tip</p> <p>Monitor <code>offsets</code> and <code>commits</code> metadata logs to know the progress of a streaming query.</p>  <p>The number of entries in the <code>OffsetSeqLog</code> is controlled using spark.sql.streaming.minBatchesToRetain configuration property.</p> <p><code>offsetLog</code> is used when:</p> <ul> <li> <p><code>ContinuousExecution</code> stream execution engine is requested to commit an epoch, getStartOffsets, and addOffset</p> </li> <li> <p><code>MicroBatchExecution</code> stream execution engine is requested to populate start offsets and construct (or skip) the next streaming micro-batch</p> </li> </ul>","text":"","location":"StreamExecution/#write-ahead-offset-log"},{"title":"Offset Commit Log <p><code>StreamExecution</code> uses offset commit log (CommitLog with <code>commits</code> metadata checkpoint directory) for streaming batches successfully executed (with a single file per batch with a file name being the batch id) or committed epochs.</p>  <p>Note</p> <p>Metadata log or metadata checkpoint are synonyms and are often used interchangeably.</p>  <p><code>commitLog</code> is used by the &lt;&gt; for the following: <ul> <li> <p><code>MicroBatchExecution</code> is requested to &lt;&gt; (that in turn requests to &lt;&gt; at the very beginning of the streaming query execution and later regularly every &lt;&gt;)  <li> <p><code>ContinuousExecution</code> is requested to &lt;&gt; (that in turn requests to &lt;&gt; at the very beginning of the streaming query execution and later regularly every &lt;&gt;)","text":"","location":"StreamExecution/#offset-commit-log"},{"title":"State of Streaming Query <pre><code>state: AtomicReference[State]\n</code></pre> <p><code>state</code> indicates the internal state of execution of the streaming query (as java.util.concurrent.atomic.AtomicReference).</p>","text":"","location":"StreamExecution/#state-of-streaming-query"},{"title":"ACTIVE <p><code>StreamExecution</code> has been requested to &lt;&gt; (and is about to &lt;&gt;)","text":"","location":"StreamExecution/#active"},{"title":"INITIALIZING <p><code>StreamExecution</code> has been created.</p>","text":"","location":"StreamExecution/#initializing"},{"title":"TERMINATED <p>Indicates that:</p> <ul> <li><code>MicroBatchExecution</code> has been requested to stop</li> <li><code>ContinuousExecution</code> has been requested to stop</li> <li><code>StreamExecution</code> has been requested to run stream processing (and has finished running the activated streaming query)</li> </ul>","text":"","location":"StreamExecution/#terminated"},{"title":"RECONFIGURING <p>Used when <code>ContinuousExecution</code> is requested to run a streaming query in continuous mode (and the ContinuousReader indicated a need for reconfiguration)</p>","text":"","location":"StreamExecution/#reconfiguring"},{"title":"Creating StreamingWrite <pre><code>createStreamingWrite(\n  table: SupportsWrite,\n  options: Map[String, String],\n  inputPlan: LogicalPlan): StreamingWrite\n</code></pre> <p><code>createStreamingWrite</code> creates a <code>LogicalWriteInfoImpl</code> (with the query ID, the schema of the input <code>LogicalPlan</code> and the given options).</p> <p><code>createStreamingWrite</code> requests the given <code>SupportsWrite</code> table for a <code>WriteBuilder</code> (for the <code>LogicalWriteInfoImpl</code>).</p>  <p>Tip</p> <p>Learn more about SupportsWrite and WriteBuilder in The Internals of Spark SQL online book.</p>  <p><code>createStreamingWrite</code> branches based on the OutputMode:</p> <ul> <li> <p>For Append output mode, <code>createStreamingWrite</code> requests the <code>WriteBuilder</code> to build a <code>StreamingWrite</code>.</p> </li> <li> <p>For Complete output mode, <code>createStreamingWrite</code> assumes that the <code>WriteBuilder</code> is a <code>SupportsTruncate</code> and requests it to <code>truncate</code> followed by <code>buildForStreaming</code></p> </li> <li> <p>For Update output mode, <code>createStreamingWrite</code> assumes that the <code>WriteBuilder</code> is a <code>SupportsStreamingUpdate</code> and requests it to <code>update</code> followed by <code>buildForStreaming</code></p> </li> </ul>  <p>Tip</p> <p>Learn more about SupportsTruncate and SupportsStreamingUpdate in The Internals of Spark SQL online book.</p>  <p><code>createStreamingWrite</code> is used when MicroBatchExecution and ContinuousExecution stream execution engines are requested for analyzed logical plans.</p>","text":"","location":"StreamExecution/#creating-streamingwrite"},{"title":"Available Offsets (StreamProgress) <pre><code>availableOffsets: StreamProgress\n</code></pre> <p><code>availableOffsets</code> is a registry of offsets per streaming source to track what data (by offset) is available for processing for every streaming source in the streaming query (and have not yet been committed).</p> <p><code>availableOffsets</code> works in tandem with the committedOffsets internal registry.</p> <p><code>availableOffsets</code> is empty when <code>StreamExecution</code> is created (i.e. no offsets are reported for any streaming source in the streaming query).</p> <p><code>availableOffsets</code> is used when:</p> <ul> <li> <p><code>MicroBatchExecution</code> stream execution engine is requested to &lt;&gt;, &lt;&gt;, &lt;&gt; and &lt;&gt;  <li> <p><code>ContinuousExecution</code> stream execution engine is requested to commit an epoch</p> </li> <li> <p><code>StreamExecution</code> is requested for the internal string representation</p> </li>","text":"","location":"StreamExecution/#available-offsets-streamprogress"},{"title":"Committed Offsets (StreamProgress) <pre><code>committedOffsets: StreamProgress\n</code></pre> <p><code>committedOffsets</code> is a registry of offsets per streaming source to track what data (by offset) has already been processed and committed (to the sink or state stores) for every streaming source in the streaming query.</p> <p><code>committedOffsets</code> works in tandem with the availableOffsets internal registry.</p> <p><code>committedOffsets</code> is used when:</p> <ul> <li><code>MicroBatchExecution</code> stream execution engine is requested for the &lt;&gt;, to &lt;&gt; and &lt;&gt; <li><code>ContinuousExecution</code> stream execution engine is requested for the &lt;&gt; and to &lt;&gt; <li><code>StreamExecution</code> is requested for the internal string representation</li>","text":"","location":"StreamExecution/#committed-offsets-streamprogress"},{"title":"Fully-Qualified (Resolved) Path to Checkpoint Root Directory <pre><code>resolvedCheckpointRoot: String\n</code></pre> <p><code>resolvedCheckpointRoot</code> is a fully-qualified path of the given checkpoint root directory.</p> <p>The given checkpoint root directory is defined using checkpointLocation option or the spark.sql.streaming.checkpointLocation configuration property with <code>queryName</code> option.</p> <p><code>checkpointLocation</code> and <code>queryName</code> options are defined when <code>StreamingQueryManager</code> is requested to create a streaming query.</p> <p><code>resolvedCheckpointRoot</code> is used when creating the path to the checkpoint directory and when <code>StreamExecution</code> finishes running streaming batches.</p> <p><code>resolvedCheckpointRoot</code> is used for the logicalPlan (while transforming analyzedPlan and planning <code>StreamingRelation</code> logical operators to corresponding <code>StreamingExecutionRelation</code> physical operators with the streaming data sources created passing in the path to <code>sources</code> directory to store checkpointing metadata).</p> <p><code>resolvedCheckpointRoot</code> is printed out immediately when resolved as a INFO message to the logs:</p> <pre><code>Checkpoint root [checkpointRoot] resolved to [resolvedCheckpointRoot].\n</code></pre> <p><code>resolvedCheckpointRoot</code> is printed out again as a INFO message to the logs when <code>StreamExecution</code> is started:</p> <pre><code>Starting [prettyIdString]. Use [resolvedCheckpointRoot] to store the query checkpoint.\n</code></pre>","text":"","location":"StreamExecution/#fully-qualified-resolved-path-to-checkpoint-root-directory"},{"title":"StreamWriterCommitProgress <pre><code>sinkCommitProgress: Option[StreamWriterCommitProgress]\n</code></pre> <p><code>sinkCommitProgress</code> is part of the ProgressReporter abstraction.</p> <p><code>StreamExecution</code> initializes <code>sinkCommitProgress</code> registry to be <code>None</code> when created.</p>","text":"","location":"StreamExecution/#streamwritercommitprogress"},{"title":"Last Query Execution Of Streaming Query (IncrementalExecution) <pre><code>lastExecution: IncrementalExecution\n</code></pre> <p><code>lastExecution</code> is part of the ProgressReporter abstraction.</p> <p><code>lastExecution</code> is a IncrementalExecution (a <code>QueryExecution</code> of a streaming query) of the most recent (last) execution.</p> <p><code>lastExecution</code> is created when the &lt;&gt; are requested for the following: <ul> <li> <p><code>MicroBatchExecution</code> is requested to &lt;&gt; (when in &lt;&gt;)  <li> <p><code>ContinuousExecution</code> stream execution engine is requested to &lt;&gt; (when in &lt;&gt;)   <p><code>lastExecution</code> is used when:</p> <ul> <li> <p><code>StreamExecution</code> is requested to &lt;&gt; (via &lt;&gt;)  <li> <p><code>ProgressReporter</code> is requested to extractStateOperatorMetrics, extractExecutionStats, and extractSourceToNumInputRows</p> </li> <li> <p><code>MicroBatchExecution</code> stream execution engine is requested to &lt;&gt; (based on StateStoreWriters in a streaming query), &lt;&gt; (when in &lt;&gt; and &lt;&gt;)  <li> <p><code>ContinuousExecution</code> stream execution engine is requested to &lt;&gt; (when in &lt;&gt;)  <li> <p>For debugging query execution of streaming queries (using <code>debugCodegen</code>)</p> </li>","text":"","location":"StreamExecution/#last-query-execution-of-streaming-query-incrementalexecution"},{"title":"Explaining Streaming Query <p><pre><code>explain(): Unit // &lt;1&gt;\nexplain(extended: Boolean): Unit\n</code></pre> &lt;1&gt; Turns the <code>extended</code> flag off (<code>false</code>)</p> <p><code>explain</code> simply prints out &lt;&gt; to the standard output.","text":"","location":"StreamExecution/#explaining-streaming-query"},{"title":"Stopping Streaming Sources and Readers <pre><code>stopSources(): Unit\n</code></pre> <p><code>stopSources</code> requests every streaming source to stop.</p> <p>In case of an non-fatal exception, <code>stopSources</code> prints out the following WARN message to the logs:</p> <pre><code>Failed to stop streaming source: [source]. Resources may have leaked.\n</code></pre> <p><code>stopSources</code> is used when:</p> <ul> <li> <p><code>StreamExecution</code> is requested to &lt;&gt; (and &lt;&gt; successfully or not)  <li> <p><code>ContinuousExecution</code> is requested to &lt;&gt; (and terminates)","text":"","location":"StreamExecution/#stopping-streaming-sources-and-readers"},{"title":"Running Stream Processing <pre><code>runStream(): Unit\n</code></pre> <p><code>runStream</code> simply prepares the environment to execute the activated streaming query.</p> <p><code>runStream</code> is used when the stream execution thread is requested to start (when <code>DataStreamWriter</code> is requested to start an execution of the streaming query).</p> <p>Internally, <code>runStream</code> sets the job group (to all the Spark jobs started by this thread) as follows:</p> <ul> <li> <p>runId for the job group ID</p> </li> <li> <p>getBatchDescriptionString for the job group description (to display in web UI)</p> </li> <li> <p><code>interruptOnCancel</code> flag on</p> </li> </ul>  <p>Note</p> <p><code>runStream</code> uses the SparkSession to access <code>SparkContext</code> and assign the job group id.</p> <p>Learn more about SparkContext.setJobGroup method in The Internals of Apache Spark online book.</p>  <p><code>runStream</code> sets <code>sql.streaming.queryId</code> local property to id.</p> <p><code>runStream</code> requests the <code>MetricsSystem</code> to register the MetricsReporter when spark.sql.streaming.metricsEnabled configuration property is enabled.</p> <p> <code>runStream</code> notifies StreamingQueryListeners that the streaming query has been started (by posting a new QueryStartedEvent event with id, runId, and name).</p> <p></p> <p><code>runStream</code> unblocks the main starting thread (by decrementing the count of the startLatch that when <code>0</code> lets the starting thread continue).</p> <p><code>runStream</code> updates the status message to be Initializing sources.</p> <p> <code>runStream</code> initializes the analyzed logical plan.</p>  Lazy Value <p>The analyzed logical plan is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and cached afterwards.</p>  <p><code>runStream</code> disables adaptive query execution and cost-based join optimization (by turning <code>spark.sql.adaptive.enabled</code> and <code>spark.sql.cbo.enabled</code> configuration properties off, respectively).</p> <p><code>runStream</code> creates a new \"zero\" OffsetSeqMetadata.</p> <p>(when in INITIALIZING state) <code>runStream</code> enters ACTIVE state:</p> <ul> <li> <p>Decrements the count of initializationLatch</p> </li> <li> <p> Executes the activated streaming query    <p>Note</p> <p><code>runBatches</code> does the main work only when first started (when in INITIALIZING state).</p>  <p> <code>runStream</code>...FIXME (describe the failed and stop states)</p> <p>Once TriggerExecutor has finished executing batches, <code>runBatches</code> updates the status message to Stopped.</p> <p>NOTE: TriggerExecutor finishes executing batches when the batch runner returns whether the streaming query is stopped or not (while active).</p>","text":"","location":"StreamExecution/#running-stream-processing"},{"title":"finally Block <p><code>runStream</code> releases the startLatch and initializationLatch latches.</p> <p><code>runStream</code> stopSources.</p> <p><code>runStream</code> enters TERMINATED state.</p> <p><code>runStream</code> sets the StreamingQueryStatus with the <code>isTriggerActive</code> and <code>isDataAvailable</code> flags off (<code>false</code>).</p> <p><code>runStream</code> removes the stream metrics reporter from the application's <code>MetricsSystem</code>.</p> <p><code>runStream</code> requests the <code>StreamingQueryManager</code> to handle termination of a streaming query.</p> <p><code>runStream</code> creates a new QueryTerminatedEvent (with the id and run id of the streaming query) and posts it.</p> <p> With the deleteCheckpointOnStop flag enabled and no <code>StreamingQueryException</code>, <code>runStream</code> deletes the checkpoint directory.</p> <p>In the end, <code>runStream</code> releases the terminationLatch latch.</p>","text":"","location":"StreamExecution/#finally-block"},{"title":"TriggerExecutor's Batch Runner <p>Batch Runner (<code>batchRunner</code>) is an executable block executed by TriggerExecutor in runBatches.</p> <p><code>batchRunner</code> starts trigger calculation.</p> <p>As long as the query is not stopped (i.e. state is not TERMINATED), <code>batchRunner</code> executes the streaming batch for the trigger.</p> <p>In triggerExecution time-tracking section, <code>runBatches</code> branches off per currentBatchId:</p> <ul> <li> <p>For <code>currentBatchId &lt; 0</code>:</p> <ol> <li>populateStartOffsets</li> <li>Setting Job Description as getBatchDescriptionString</li> </ol> <pre><code>Stream running from [committedOffsets] to [availableOffsets]\n</code></pre> </li> <li> <p>For <code>currentBatchId &gt;= 0</code>:</p> <ol> <li>Constructing the next streaming micro-batch</li> </ol> </li> </ul> <p>If there is data available in the sources, <code>batchRunner</code> marks currentStatus with <code>isDataAvailable</code> enabled.</p>  <p>Tip</p> <p>You can check out the status of a streaming query using status method.</p> <pre><code>scala&gt; spark.streams.active(0).status\nres1: org.apache.spark.sql.streaming.StreamingQueryStatus =\n{\n  \"message\" : \"Waiting for next trigger\",\n  \"isDataAvailable\" : false,\n  \"isTriggerActive\" : false\n}\n</code></pre>  <p><code>batchRunner</code> then updates the status message to Processing new data and runs the current streaming batch.</p> <p></p> <p> After triggerExecution section has finished, <code>batchRunner</code> finishes the streaming batch for the trigger (and collects query execution statistics).</p> <p>When there was &lt;&gt; in the sources, <code>batchRunner</code> updates committed offsets (by adding the &lt;&gt; to BatchCommitLog and adding availableOffsets to committedOffsets). <p><code>batchRunner</code> prints out the following DEBUG message to the logs:</p> <pre><code>batch [currentBatchId] committed\n</code></pre> <p><code>batchRunner</code> increments the current batch id and sets the job description for all the following Spark jobs to include the new batch id.</p> <p> When no data was available in the sources to process, <code>batchRunner</code> does the following:</p> <ol> <li> <p>Marks currentStatus with <code>isDataAvailable</code> disabled</p> </li> <li> <p>Updates the status message to Waiting for data to arrive</p> </li> <li> <p>Sleeps the current thread for pollingDelayMs milliseconds.</p> </li> </ol> <p><code>batchRunner</code> updates the status message to Waiting for next trigger and returns whether the query is currently active or not (so TriggerExecutor can decide whether to finish executing the batches or not)</p>","text":"","location":"StreamExecution/#triggerexecutors-batch-runner"},{"title":"Path to Checkpoint Directory <pre><code>checkpointFile(\n  name: String): String\n</code></pre> <p><code>checkpointFile</code> gives the path of a directory with <code>name</code> in checkpoint directory.</p> <p><code>checkpointFile</code> is used for streamMetadata, OffsetSeqLog, BatchCommitLog, and lastExecution (for runBatch).</p>","text":"","location":"StreamExecution/#path-to-checkpoint-directory"},{"title":"Posting StreamingQueryListener Event <pre><code>postEvent(\n  event: StreamingQueryListener.Event): Unit\n</code></pre> <p><code>postEvent</code> is a part of the ProgressReporter abstraction.</p> <p><code>postEvent</code> simply requests the <code>StreamingQueryManager</code> to post the input event (to the StreamingQueryListenerBus in the current <code>SparkSession</code>).</p>  <p>Note</p> <p><code>postEvent</code> uses <code>SparkSession</code> to access the current <code>StreamingQueryManager</code>.</p>  <p><code>postEvent</code> is used when:</p> <ul> <li> <p><code>ProgressReporter</code> is requested to report update progress (while finishing a trigger)</p> </li> <li> <p><code>StreamExecution</code> runs streaming batches (and announces starting a streaming query by posting a QueryStartedEvent and query termination by posting a QueryTerminatedEvent)</p> </li> </ul>","text":"","location":"StreamExecution/#posting-streamingquerylistener-event"},{"title":"Waiting Until No New Data Available in Sources or Query Has Been Terminated <pre><code>processAllAvailable(): Unit\n</code></pre> <p><code>processAllAvailable</code> is a part of the StreamingQuery abstraction.</p> <p><code>processAllAvailable</code> reports the &lt;&gt; if reported (and returns immediately). <p>NOTE: &lt;&gt; is reported exclusively when <code>StreamExecution</code> is requested to &lt;&gt; (that terminated with an exception). <p><code>processAllAvailable</code> returns immediately when <code>StreamExecution</code> is no longer &lt;&gt; (in <code>TERMINATED</code> state). <p><code>processAllAvailable</code> acquires a lock on the &lt;&gt; and turns the &lt;&gt; internal flag off (<code>false</code>). <p><code>processAllAvailable</code> keeps polling with 10-second pauses (locked on &lt;&gt;) until &lt;&gt; flag is turned on (<code>true</code>) or <code>StreamExecution</code> is no longer &lt;&gt; (in <code>TERMINATED</code> state). <p>NOTE: The 10-second pause is hardcoded and cannot be changed.</p> <p>In the end, <code>processAllAvailable</code> releases &lt;&gt; lock. <p><code>processAllAvailable</code> throws an <code>IllegalStateException</code> when executed on the &lt;&gt;: <pre><code>Cannot wait for a query state from the same thread that is running the query\n</code></pre>","text":"","location":"StreamExecution/#waiting-until-no-new-data-available-in-sources-or-query-has-been-terminated"},{"title":"Stream Execution Thread <pre><code>queryExecutionThread: QueryExecutionThread\n</code></pre> <p><code>queryExecutionThread</code> is a Java thread of execution (java.util.Thread) that runs a streaming query.</p> <p><code>queryExecutionThread</code> is started (as a daemon thread) when <code>StreamExecution</code> is requested to &lt;&gt;. At that time, <code>start</code> prints out the following INFO message to the logs (with the &lt;&gt; and the &lt;&gt;): <pre><code>Starting [prettyIdString]. Use [resolvedCheckpointRoot] to store the query checkpoint.\n</code></pre> <p>When started, <code>queryExecutionThread</code> sets the &lt;&gt; and &lt;&gt;. <p><code>queryExecutionThread</code> uses the name stream execution thread for [id] (that uses &lt;&gt; for the id, i.e. <code>queryName [id = [id], runId = [runId]]</code>). <p><code>queryExecutionThread</code> is a <code>QueryExecutionThread</code> that is a custom <code>UninterruptibleThread</code> from Apache Spark with <code>runUninterruptibly</code> method for running a block of code without being interrupted by <code>Thread.interrupt()</code>.</p>  <p>Tip</p> <p>Use Java's jconsole or jstack to monitor stream execution threads.</p> <pre><code>$ jstack &lt;driver-pid&gt; | grep -e \"stream execution thread\"\n\"stream execution thread for kafka-topic1 [id =...\n</code></pre>","text":"","location":"StreamExecution/#stream-execution-thread"},{"title":"Batch Metadata <pre><code>offsetSeqMetadata: OffsetSeqMetadata\n</code></pre> <p><code>offsetSeqMetadata</code> is part of the ProgressReporter abstraction.</p>  <p><code>StreamExecution</code> creates an OffsetSeqMetadata when created.</p> <p>Most importantly, the <code>OffsetSeqMetadata</code> is used to create an IncrementalExecution in the queryPlanning phase of the MicroBatchExecution and ContinuousExecution execution engines.</p> <p><code>offsetSeqMetadata</code> is initialized (with <code>0</code> for <code>batchWatermarkMs</code> and <code>batchTimestampMs</code>) when <code>StreamExecution</code> is requested to &lt;&gt;. <p><code>offsetSeqMetadata</code> is then updated (with the current event-time watermark and timestamp) when <code>MicroBatchExecution</code> is requested to &lt;&gt;. <p>NOTE: <code>MicroBatchExecution</code> uses the &lt;&gt; for the current event-time watermark and the &lt;&gt; for the current batch timestamp. <p><code>offsetSeqMetadata</code> is stored (checkpointed) in &lt;&gt; of <code>MicroBatchExecution</code> (and printed out as INFO message to the logs). <p><code>offsetSeqMetadata</code> is restored (re-created) from a checkpointed state when <code>MicroBatchExecution</code> is requested to &lt;&gt;. <p><code>offsetSeqMetadata</code> is part of the ProgressReporter abstraction.</p>","text":"","location":"StreamExecution/#batch-metadata"},{"title":"isActive <pre><code>isActive: Boolean\n</code></pre> <p><code>isActive</code> is part of the StreamingQuery abstraction.</p> <p><code>isActive</code> is enabled (<code>true</code>) as long as the State is not TERMINATED.</p>","text":"","location":"StreamExecution/#isactive"},{"title":"Human-Readable HTML Description of Spark Jobs (for web UI) <pre><code>getBatchDescriptionString: String\n</code></pre> <p><code>getBatchDescriptionString</code> is a human-readable description (in HTML format) that uses the optional name if defined, the &lt;&gt;, the &lt;&gt; and <code>batchDescription</code> that can be init (for the &lt;&gt; negative) or the current batch ID itself. <p><code>getBatchDescriptionString</code> is of the following format:</p> <pre><code>[name]\nid = [id]\nrunId = [runId]\nbatch = [batchDescription]\n</code></pre> <p></p> <p><code>getBatchDescriptionString</code> is used when:</p> <ul> <li> <p><code>MicroBatchExecution</code> stream execution engine is requested to &lt;&gt; (as the job description of any Spark jobs triggerred as part of query execution)  <li> <p><code>StreamExecution</code> is requested to &lt;&gt; (as the job group description of any Spark jobs triggerred as part of query execution)","text":"","location":"StreamExecution/#human-readable-html-description-of-spark-jobs-for-web-ui"},{"title":"No New Data Available <pre><code>noNewData: Boolean\n</code></pre> <p><code>noNewData</code> is a flag that indicates that a batch has completed with no new data left and processAllAvailable could stop waiting till all streaming data is processed.</p> <p>Default: <code>false</code></p> <p>Turned on (<code>true</code>) when:</p> <ul> <li> <p><code>MicroBatchExecution</code> stream execution engine is requested to &lt;&gt; (while &lt;&gt;)  <li> <p><code>ContinuousExecution</code> stream execution engine is requested to &lt;&gt;   <p>Turned off (<code>false</code>) when:</p> <ul> <li> <p><code>MicroBatchExecution</code> stream execution engine is requested to &lt;&gt; (right after the &lt;&gt; phase)  <li> <p><code>StreamExecution</code> is requested to &lt;&gt;","text":"","location":"StreamExecution/#no-new-data-available"},{"title":"Current Batch ID <ul> <li> <p><code>-1</code> when <code>StreamExecution</code> is created</p> </li> <li> <p><code>0</code> when <code>StreamExecution</code> populates start offsets (and OffsetSeqLog is empty, i.e. no offset files in <code>offsets</code> directory in checkpoint)</p> </li> <li> <p>Incremented when <code>StreamExecution</code> runs streaming batches and finishes a trigger that had data available from sources (right after committing the batch).</p> </li> </ul>","text":"","location":"StreamExecution/#current-batch-id"},{"title":"newData Registry <pre><code>newData: Map[BaseStreamingSource, LogicalPlan]\n</code></pre> <p>Registry of the streaming sources (in the logical query plan) that have new data available in the current batch. The new data is a streaming <code>DataFrame</code>.</p> <p><code>newData</code> is part of the ProgressReporter abstraction.</p> <p>Set when <code>StreamExecution</code> is requested to requests unprocessed data from streaming sources (while running a single streaming batch)</p>","text":"","location":"StreamExecution/#newdata-registry"},{"title":"Streaming Metrics <p><code>StreamExecution</code> uses MetricsReporter for reporting streaming metrics.</p> <p><code>MetricsReporter</code> is created with the following source name (with name if defined or id):</p> <pre><code>spark.streaming.[name or id]\n</code></pre> <p><code>MetricsReporter</code> is registered only when spark.sql.streaming.metricsEnabled configuration property is enabled (when <code>StreamExecution</code> is requested to runStream).</p> <p><code>MetricsReporter</code> is deactivated (removed) when a streaming query is stopped (when <code>StreamExecution</code> is requested to runStream).</p>","text":"","location":"StreamExecution/#streaming-metrics"},{"title":"Latches <p><code>StreamExecution</code> uses java.util.concurrent.CountDownLatches (with count <code>1</code>).</p>","text":"","location":"StreamExecution/#latches"},{"title":"initializationLatch <p>Counted down when requested to runStream:</p> <ul> <li>Changes state from INITIALIZING to ACTIVE just before runActivatedStream</li> <li>In runStream's finally block</li> </ul> <p>Awaited for tests only (which seems to indicate that it is a test-only latch)</p>","text":"","location":"StreamExecution/#initializationlatch"},{"title":"startLatch <p>Counted down when requested to runStream:</p> <ul> <li>Right after posting a QueryStartedEvent</li> <li>In runStream's finally block</li> </ul> <p>Awaited when requested to start (to pause the main thread until <code>StreamExecution</code> was requested to run the streaming query on a separate thread)</p>","text":"","location":"StreamExecution/#startlatch"},{"title":"terminationLatch <p>Counted down at the end of runStream</p> <p>Awaited when requested to awaitTermination (that pauses the thread until the streaming query has finished successfully or not).</p>","text":"","location":"StreamExecution/#terminationlatch"},{"title":"Locks","text":"","location":"StreamExecution/#locks"},{"title":"awaitProgressLock <p><code>StreamExecution</code> uses a fair reentrant mutual exclusion java.util.concurrent.locks.ReentrantLock (that favors granting access to the longest-waiting thread under contention)</p>","text":"","location":"StreamExecution/#awaitprogresslock"},{"title":"__is_continuous_processing Local Property <p><code>StreamExecution</code> uses __is_continuous_processing local property (default: <code>false</code>) to differentiate between &lt;&gt; (<code>true</code>) and &lt;&gt; (<code>false</code>) which is used when <code>StateStoreRDD</code> is requested to compute a partition (and finds a StateStore for a given version).","text":"","location":"StreamExecution/#__is_continuous_processing-local-property"},{"title":"Demo <pre><code>import org.apache.spark.sql.streaming.StreamingQuery\nassert(sq.isInstanceOf[StreamingQuery])\n\nimport org.apache.spark.sql.execution.streaming.StreamingQueryWrapper\nval se = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery\n\nscala&gt; :type se\norg.apache.spark.sql.execution.streaming.StreamExecution\n</code></pre>","text":"","location":"StreamExecution/#demo"},{"title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.streaming.StreamExecution</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.streaming.StreamExecution=ALL\n</code></pre> <p>Refer to Logging.</p>","text":"","location":"StreamExecution/#logging"},{"title":"StreamMetadata","text":"<p><code>StreamMetadata</code> is a metadata associated with a &lt;&gt; (indirectly through StreamExecution). <p>[[creating-instance]] [[id]] <code>StreamMetadata</code> takes an ID to be created.</p> <p><code>StreamMetadata</code> is &lt;&gt; exclusively when StreamExecution is created (with a randomly-generated 128-bit universally unique identifier (UUID)). <p><code>StreamMetadata</code> can be &lt;&gt; to and &lt;&gt; from a JSON file. <code>StreamMetadata</code> uses http://json4s.org/[json4s-jackson] library for JSON persistence. <pre><code>import org.apache.spark.sql.execution.streaming.StreamMetadata\nimport org.apache.hadoop.fs.Path\nval metadataPath = new Path(\"metadata\")\n\nscala&gt; :type spark\norg.apache.spark.sql.SparkSession\n\nval hadoopConf = spark.sessionState.newHadoopConf()\nval sm = StreamMetadata.read(metadataPath, hadoopConf)\n\nscala&gt; :type sm\nOption[org.apache.spark.sql.execution.streaming.StreamMetadata]\n</code></pre> <p>=== [[read]] Unpersisting StreamMetadata (from JSON File) -- <code>read</code> Object Method</p>","location":"StreamMetadata/"},{"title":"[source, scala]","text":"<p>read(   metadataFile: Path,   hadoopConf: Configuration): Option[StreamMetadata]</p>  <p><code>read</code> unpersists <code>StreamMetadata</code> from the given <code>metadataFile</code> file if available.</p> <p><code>read</code> returns a <code>StreamMetadata</code> if the metadata file was available and the content could be read in JSON format. Otherwise, <code>read</code> returns <code>None</code>.</p> <p>NOTE: <code>read</code> uses <code>org.json4s.jackson.Serialization.read</code> for JSON deserialization.</p> <p>NOTE: <code>read</code> is used exclusively when StreamExecution is created&gt; (and tries to read the <code>metadata</code> checkpoint file).</p> <p>=== [[write]] Persisting Metadata -- <code>write</code> Object Method</p>","location":"StreamMetadata/#source-scala"},{"title":"[source, scala]","text":"<p>write(   metadata: StreamMetadata,   metadataFile: Path,   hadoopConf: Configuration): Unit</p>  <p><code>write</code> persists the given <code>StreamMetadata</code> to the given <code>metadataFile</code> file in JSON format.</p> <p>NOTE: <code>write</code> uses <code>org.json4s.jackson.Serialization.write</code> for JSON serialization.</p> <p><code>write</code> is used when StreamExecution is created (and the <code>metadata</code> checkpoint file is not available).</p>","location":"StreamMetadata/#source-scala_1"},{"title":"StreamProgress \u2014 Collection of Offsets per Streaming Source","text":"<p><code>StreamProgress</code> is a collection of Offsets per streaming source.</p> <p><code>StreamProgress</code> is &lt;&gt; when: <ul> <li> <p>StreamExecution is created (and creates committed and available offsets)</p> </li> <li> <p><code>OffsetSeq</code> is requested to convert to StreamProgress</p> </li> </ul> <p><code>StreamProgress</code> is an extension of Scala's scala.collection.immutable.Map with streaming sources as keys and their Offsets as values.</p>","location":"StreamProgress/"},{"title":"Creating Instance","text":"<p><code>StreamProgress</code> takes the following to be created:</p> <ul> <li>[[baseMap]] Offsets per streaming source (<code>Map[BaseStreamingSource, Offset]</code>) (default: empty)</li> </ul> <p>=== [[get]] Looking Up Offset by Streaming Source -- <code>get</code> Method</p>","location":"StreamProgress/#creating-instance"},{"title":"[source, scala]","text":"","location":"StreamProgress/#source-scala"},{"title":"get(key: BaseStreamingSource): Option[Offset]","text":"<p>NOTE: <code>get</code> is part of the Scala's <code>scala.collection.MapLike</code> to...FIXME.</p> <p><code>get</code> simply looks up an Offsets for the given streaming source in the &lt;&gt;. <p>=== [[plusplus]] <code>++</code> Method</p>","location":"StreamProgress/#getkey-basestreamingsource-optionoffset"},{"title":"[source, scala]","text":"<p>++(   updates: GenTraversableOnce[(BaseStreamingSource, Offset)]): StreamProgress</p>  <p><code>++</code> simply creates a new &lt;&gt; with the &lt;&gt; and the given updates. <p><code>++</code> is used exclusively when <code>OffsetSeq</code> is requested to convert to StreamProgress.</p> <p>=== [[toOffsetSeq]] Converting to OffsetSeq -- <code>toOffsetSeq</code> Method</p>","location":"StreamProgress/#source-scala_1"},{"title":"[source, scala]","text":"<p>toOffsetSeq(   sources: Seq[BaseStreamingSource],   metadata: OffsetSeqMetadata): OffsetSeq</p>  <p><code>toOffsetSeq</code> creates a OffsetSeq with offsets that are &lt;&gt; for every streaming source. <p><code>toOffsetSeq</code> is used when:</p> <ul> <li> <p><code>MicroBatchExecution</code> stream execution engine is requested to construct the next streaming micro-batch (to commit available offsets for a batch to the write-ahead log)</p> </li> <li> <p><code>StreamExecution</code> is requested to run stream processing (that failed with a Throwable)</p> </li> </ul>","location":"StreamProgress/#source-scala_2"},{"title":"StreamSinkProvider","text":"<p><code>StreamSinkProvider</code> is the &lt;&gt; of &lt;&gt; that can &lt;&gt; for a file format (e.g. <code>parquet</code>) or system (e.g. <code>kafka</code>). <p>[[contract]] .StreamSinkProvider Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| createSink a| [[createSink]]</p>","location":"StreamSinkProvider/"},{"title":"[source, scala]","text":"<p>createSink(   sqlContext: SQLContext,   parameters: Map[String, String],   partitionColumns: Seq[String],   outputMode: OutputMode): Sink</p>  <p>Creates a streaming sink</p> <p>Used when <code>DataSource</code> is requested for a streaming sink (when <code>DataStreamWriter</code> is requested to start a streaming query)</p> <p>|===</p> <p>[[implementations]] NOTE: KafkaSourceProvider is the only known <code>StreamSinkProvider</code> in Spark Structured Streaming.</p>","location":"StreamSinkProvider/#source-scala"},{"title":"StreamSourceProvider","text":"<p><code>StreamSourceProvider</code> is an abstraction of data source providers that can create a streaming source for a data format or system.</p> <p><code>StreamSourceProvider</code> is part of Data Source API V1 for Micro-Batch Stream Processing.</p>","location":"StreamSourceProvider/"},{"title":"Contract","text":"","location":"StreamSourceProvider/#contract"},{"title":"Creating Streaming Source <pre><code>createSource(\n  sqlContext: SQLContext,\n  metadataPath: String,\n  schema: Option[StructType],\n  providerName: String,\n  parameters: Map[String, String]): Source\n</code></pre> <p>Creates a Streaming Source</p> <p><code>metadataPath</code> is the value of the optional user-specified <code>checkpointLocation</code> option or resolved by StreamingQueryManager.</p> <p>Used when:</p> <ul> <li><code>DataSource</code> is requested to create a streaming source (when <code>MicroBatchExecution</code> is requested to initialize the analyzed logical plan)</li> </ul>","text":"","location":"StreamSourceProvider/#creating-streaming-source"},{"title":"Source Schema <pre><code>sourceSchema(\n  sqlContext: SQLContext,\n  schema: Option[StructType],\n  providerName: String,\n  parameters: Map[String, String]): (String, StructType)\n</code></pre> <p>Name and schema of the Streaming Source</p> <p>Used when:</p> <ul> <li><code>DataSource</code> is requested for metadata of a streaming source (when <code>MicroBatchExecution</code> is requested to initialize the analyzed logical plan)</li> </ul>","text":"","location":"StreamSourceProvider/#source-schema"},{"title":"Implementations","text":"<ul> <li>KafkaSourceProvider</li> </ul>","location":"StreamSourceProvider/#implementations"},{"title":"StreamingQuery","text":"<p><code>StreamingQuery</code> is an abstraction of handles to streaming queries (that are executed continuously and concurrently on a separate thread).</p>","location":"StreamingQuery/"},{"title":"Creating StreamingQuery","text":"<p><code>StreamingQuery</code> is created when a streaming query is started using DataStreamWriter.start operator.</p>  Demo: Deep Dive into FileStreamSink <p>Learn more in Demo: Deep Dive into FileStreamSink.</p>","location":"StreamingQuery/#creating-streamingquery"},{"title":"Managing Active StreamingQueries","text":"<p>StreamingQueryManager manages active <code>StreamingQuery</code> instances and allows to access one (by id) or all active queries (using StreamingQueryManager.get or StreamingQueryManager.active operators, respectively).</p>","location":"StreamingQuery/#managing-active-streamingqueries"},{"title":"States","text":"<p><code>StreamingQuery</code> can be in two states:</p> <ul> <li>Active (started)</li> <li>Inactive (stopped)</li> </ul> <p>If inactive, <code>StreamingQuery</code> may have stopped due to an StreamingQueryException.</p>","location":"StreamingQuery/#states"},{"title":"Implementations","text":"<ul> <li>StreamExecution</li> <li>StreamingQueryWrapper</li> </ul>","location":"StreamingQuery/#implementations"},{"title":"Contract","text":"","location":"StreamingQuery/#contract"},{"title":"awaitTermination <pre><code>awaitTermination(): Unit\nawaitTermination(\n  timeoutMs: Long): Boolean\n</code></pre> <p>Used when...FIXME</p>","text":"","location":"StreamingQuery/#awaittermination"},{"title":"StreamingQueryException <pre><code>exception: Option[StreamingQueryException]\n</code></pre> <p><code>StreamingQueryException</code> if the streaming query has finished due to an exception</p> <p>Used when...FIXME</p>","text":"","location":"StreamingQuery/#streamingqueryexception"},{"title":"Explaining Streaming Query <pre><code>explain(): Unit\nexplain(\n  extended: Boolean): Unit\n</code></pre> <p>Used when...FIXME</p>","text":"","location":"StreamingQuery/#explaining-streaming-query"},{"title":"Id <pre><code>id: UUID\n</code></pre> <p>Unique identifier of the streaming query (that does not change across restarts unlike runId)</p> <p>Used when...FIXME</p>","text":"","location":"StreamingQuery/#id"},{"title":"isActive <pre><code>isActive: Boolean\n</code></pre> <p>Indicates whether the streaming query is active (<code>true</code>) or not (<code>false</code>)</p> <p>Used when...FIXME</p>","text":"","location":"StreamingQuery/#isactive"},{"title":"StreamingQueryProgress <pre><code>lastProgress: StreamingQueryProgress\n</code></pre> <p>The latest StreamingQueryProgress of the streaming query</p> <p>Used when...FIXME</p>","text":"","location":"StreamingQuery/#streamingqueryprogress"},{"title":"Query Name <pre><code>name: String\n</code></pre> <p>Name of the streaming query (unique across all active queries in SparkSession)</p> <p>Used when...FIXME</p>","text":"","location":"StreamingQuery/#query-name"},{"title":"Processing All Available Data <pre><code>processAllAvailable(): Unit\n</code></pre> <p>Pauses (blocks) the current thread until the streaming query has no more data to be processed or has been stopped.</p> <p>Intended for testing</p> <p>Used when...FIXME</p>","text":"","location":"StreamingQuery/#processing-all-available-data"},{"title":"Recent StreamingQueryProgresses <pre><code>recentProgress: Array[StreamingQueryProgress]\n</code></pre> <p>Recent StreamingQueryProgress updates.</p> <p>Used when...FIXME</p>","text":"","location":"StreamingQuery/#recent-streamingqueryprogresses"},{"title":"Run Id <pre><code>runId: UUID\n</code></pre> <p>Unique identifier of the current execution of the streaming query (that is different every restart unlike id)</p> <p>Used when...FIXME</p>","text":"","location":"StreamingQuery/#run-id"},{"title":"SparkSession <pre><code>sparkSession: SparkSession\n</code></pre> <p>Used when...FIXME</p>","text":"","location":"StreamingQuery/#sparksession"},{"title":"StreamingQueryStatus <pre><code>status: StreamingQueryStatus\n</code></pre> <p>StreamingQueryStatus of the streaming query (as <code>StreamExecution</code> has accumulated being a <code>ProgressReporter</code> while running the streaming query)</p> <p>Used when...FIXME</p>","text":"","location":"StreamingQuery/#streamingquerystatus"},{"title":"Stopping Streaming Query <pre><code>stop(): Unit\n</code></pre> <p>Stops the streaming query</p> <p>Used when...FIXME</p>","text":"","location":"StreamingQuery/#stopping-streaming-query"},{"title":"StreamingQueryListenerBus","text":"<p><code>StreamingQueryListenerBus</code> is an event bus for dispatching streaming events (of active streaming queries) to StreamingQueryListeners.</p>  <p>Tip</p> <p>Learn more about event buses in The Internals of Apache Spark online book.</p>","location":"StreamingQueryListenerBus/"},{"title":"Creating Instance","text":"<p><code>StreamingQueryListenerBus</code> takes the following to be created:</p> <ul> <li> <code>LiveListenerBus</code> (Spark Core)  <p>When created, <code>StreamingQueryListenerBus</code> registers itself with the LiveListenerBus to <code>streams</code> event queue.</p> <p><code>StreamingQueryListenerBus</code> is created for StreamingQueryManager (once per <code>SparkSession</code>).</p> <p></p>","location":"StreamingQueryListenerBus/#creating-instance"},{"title":"SparkListener <p><code>StreamingQueryListenerBus</code> is an event listener (<code>SparkListener</code>) and registers itself with the LiveListenerBus to intercept QueryStartedEvents.</p>  <p>Tip</p> <p>Learn more about SparkListener in The Internals of Apache Spark online book.</p>","text":"","location":"StreamingQueryListenerBus/#sparklistener"},{"title":"Run IDs of Active Streaming Queries <pre><code>activeQueryRunIds: Set[UUID]\n</code></pre> <p><code>activeQueryRunIds</code> is an internal registry of run IDs of active streaming queries in the <code>SparkSession</code>.</p> <ul> <li> <p>A <code>runId</code> is added when <code>StreamingQueryListenerBus</code> is requested to post a QueryStartedEvent</p> </li> <li> <p>A <code>runId</code> is removed when <code>StreamingQueryListenerBus</code> is requested to post a QueryTerminatedEvent</p> </li> </ul> <p><code>activeQueryRunIds</code> is used internally to dispatch a streaming event to a StreamingQueryListener (so the events gets sent out to streaming queries in the <code>SparkSession</code>).</p>","text":"","location":"StreamingQueryListenerBus/#run-ids-of-active-streaming-queries"},{"title":"Posting Streaming Event to LiveListenerBus <pre><code>post(\n  event: StreamingQueryListener.Event): Unit\n</code></pre> <p><code>post</code> simply posts the input <code>event</code> directly to the LiveListenerBus unless it is a QueryStartedEvent.</p> <p>For a QueryStartedEvent, <code>post</code> adds the <code>runId</code> (of the streaming query that has been started) to the activeQueryRunIds internal registry first, posts the event to the LiveListenerBus and then postToAll.</p> <p><code>post</code> is used when <code>StreamingQueryManager</code> is requested to post a streaming event.</p>","text":"","location":"StreamingQueryListenerBus/#posting-streaming-event-to-livelistenerbus"},{"title":"Notifying Listener about Event <pre><code>doPostEvent(\n  listener: StreamingQueryListener,\n  event: StreamingQueryListener.Event): Unit\n</code></pre> <p><code>doPostEvent</code> is part of the <code>ListenerBus</code> (Spark Core) abstraction.</p> <p><code>doPostEvent</code> branches per the type of StreamingQueryListener.Event:</p> <ul> <li> <p>For a QueryStartedEvent, requests the StreamingQueryListener to onQueryStarted</p> </li> <li> <p>For a QueryProgressEvent, requests the StreamingQueryListener to onQueryProgress</p> </li> <li> <p>For a QueryTerminatedEvent, requests the StreamingQueryListener to onQueryTerminated</p> </li> </ul> <p>For any other event, <code>doPostEvent</code> simply does nothing (swallows it).</p>","text":"","location":"StreamingQueryListenerBus/#notifying-listener-about-event"},{"title":"Posting Event To All Listeners <pre><code>postToAll(\n  event: Event): Unit\n</code></pre> <p><code>postToAll</code> is part of the <code>ListenerBus</code> (Spark Core) abstraction.</p> <p><code>postToAll</code> first requests the parent <code>ListenerBus</code> to post the event to all registered listeners.</p> <p>For a QueryTerminatedEvent, <code>postToAll</code> simply removes the <code>runId</code> (of the streaming query that has been terminated) from the activeQueryRunIds internal registry.</p>","text":"","location":"StreamingQueryListenerBus/#posting-event-to-all-listeners"},{"title":"StreamingQueryManager","text":"<p><code>StreamingQueryManager</code> is the management interface for active streaming queries of a SparkSession.</p> <p><code>StreamingQueryManager</code> is used (internally) to create a StreamingQuery (and its StreamExecution).</p> <p></p>","location":"StreamingQueryManager/"},{"title":"Creating Instance","text":"<p><code>StreamingQueryManager</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code>  <p><code>StreamingQueryManager</code> is created\u00a0when <code>SessionState</code> is requested for one.</p>  <p>Tip</p> <p>Learn more about SessionState in The Internals of Spark SQL online book.</p>  <p></p>","location":"StreamingQueryManager/#creating-instance"},{"title":"All Active Streaming Queries <pre><code>active: Array[StreamingQuery]\n</code></pre> <p>Active streaming queries</p>","text":"","location":"StreamingQueryManager/#all-active-streaming-queries"},{"title":"Registering StreamingQueryListener <pre><code>addListener(\n  listener: StreamingQueryListener): Unit\n</code></pre> <p>Registers (adds) a StreamingQueryListener</p>","text":"","location":"StreamingQueryManager/#registering-streamingquerylistener"},{"title":"Awaiting Any Termination <pre><code>awaitAnyTermination(): Unit\nawaitAnyTermination(\n  timeoutMs: Long): Boolean\n</code></pre> <p>Waits until any streaming query terminates or <code>timeoutMs</code> elapses</p>","text":"","location":"StreamingQueryManager/#awaiting-any-termination"},{"title":"Getting Active StreamingQuery by ID <pre><code>get(\n  id: String): StreamingQuery\nget(\n  id: UUID): StreamingQuery\n</code></pre> <p>Gets the StreamingQuery by id</p>","text":"","location":"StreamingQueryManager/#getting-active-streamingquery-by-id"},{"title":"Deregistering StreamingQueryListener <pre><code>removeListener(\n  listener: StreamingQueryListener): Unit\n</code></pre> <p>De-registers (removes) the StreamingQueryListener</p>","text":"","location":"StreamingQueryManager/#deregistering-streamingquerylistener"},{"title":"Resetting Terminated Queries <pre><code>resetTerminated(): Unit\n</code></pre> <p>Resets the internal registry of the terminated streaming queries (that lets awaitAnyTermination to be used again)</p>","text":"","location":"StreamingQueryManager/#resetting-terminated-queries"},{"title":"Accessing StreamingQueryManager <p><code>StreamingQueryManager</code> is available using <code>SparkSession.streams</code> property.</p> <pre><code>scala&gt; :type spark\norg.apache.spark.sql.SparkSession\n\nscala&gt; :type spark.streams\norg.apache.spark.sql.streaming.StreamingQueryManager\n</code></pre>","text":"","location":"StreamingQueryManager/#accessing-streamingquerymanager"},{"title":"StreamingQueryListenerBus <pre><code>listenerBus: StreamingQueryListenerBus\n</code></pre> <p><code>listenerBus</code> is a StreamingQueryListenerBus (for the current SparkSession) that is created immediately when <code>StreamingQueryManager</code> is created.</p> <p><code>listenerBus</code> is used for the following:</p> <ul> <li> <p>Register or de-register a given StreamingQueryListener</p> </li> <li> <p>Post a streaming event (and notify registered StreamingQueryListeners about the event)</p> </li> </ul>","text":"","location":"StreamingQueryManager/#streamingquerylistenerbus"},{"title":"Registering StreamingQueryListener <pre><code>addListener(\n  listener: StreamingQueryListener): Unit\n</code></pre> <p><code>addListener</code> requests the StreamingQueryListenerBus to add the input StreamingQueryListener.</p>","text":"","location":"StreamingQueryManager/#registering-streamingquerylistener_1"},{"title":"De-Registering StreamingQueryListener <pre><code>removeListener(\n  listener: StreamingQueryListener): Unit\n</code></pre> <p><code>removeListener</code> requests StreamingQueryListenerBus to remove the input StreamingQueryListener.</p>","text":"","location":"StreamingQueryManager/#de-registering-streamingquerylistener"},{"title":"Creating Streaming Query <pre><code>createQuery(\n  userSpecifiedName: Option[String],\n  userSpecifiedCheckpointLocation: Option[String],\n  df: DataFrame,\n  extraOptions: Map[String, String],\n  sink: BaseStreamingSink,\n  outputMode: OutputMode,\n  useTempCheckpointLocation: Boolean,\n  recoverFromCheckpointLocation: Boolean,\n  trigger: Trigger,\n  triggerClock: Clock): StreamingQueryWrapper\n</code></pre> <p><code>createQuery</code> creates a StreamingQueryWrapper (for a StreamExecution per the input user-defined properties).</p> <p>Internally, <code>createQuery</code> first finds the name of the checkpoint directory of a query (aka checkpoint location) in the following order:</p> <ol> <li> <p>Exactly the input <code>userSpecifiedCheckpointLocation</code> if defined</p> </li> <li> <p>spark.sql.streaming.checkpointLocation Spark property if defined for the parent directory with a subdirectory per the optional <code>userSpecifiedName</code> (or a randomly-generated UUID)</p> </li> <li> <p>(only when <code>useTempCheckpointLocation</code> is enabled) A temporary directory (as specified by <code>java.io.tmpdir</code> JVM property) with a subdirectory with <code>temporary</code> prefix.</p> </li> </ol>  <p>Note</p> <p><code>userSpecifiedCheckpointLocation</code> can be any path that is acceptable by Hadoop's Path.</p>  <p>If the directory name for the checkpoint location could not be found, <code>createQuery</code> reports a <code>AnalysisException</code>.</p> <pre><code>checkpointLocation must be specified either through option(\"checkpointLocation\", ...) or SparkSession.conf.set(\"spark.sql.streaming.checkpointLocation\", ...)\n</code></pre> <p><code>createQuery</code> reports a <code>AnalysisException</code> when the input <code>recoverFromCheckpointLocation</code> flag is turned off but there is offsets directory in the checkpoint location.</p> <p><code>createQuery</code> makes sure that the logical plan of the structured query is analyzed (i.e. no logical errors have been found).</p> <p>Unless spark.sql.streaming.unsupportedOperationCheck configuration property is enabled, <code>createQuery</code> checks the logical plan of the streaming query for unsupported operations.</p> <p>(only when <code>spark.sql.adaptive.enabled</code> Spark property is turned on) <code>createQuery</code> prints out a WARN message to the logs:</p> <pre><code>spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n</code></pre> <p>In the end, <code>createQuery</code> creates a StreamingQueryWrapper with a new MicroBatchExecution.</p> <p><code>createQuery</code> is used when <code>StreamingQueryManager</code> is requested to start a streaming query (when <code>DataStreamWriter</code> is requested to start an execution of a streaming query).</p>","text":"","location":"StreamingQueryManager/#creating-streaming-query"},{"title":"recoverFromCheckpointLocation <p><code>recoverFromCheckpointLocation</code> flag corresponds to <code>recoverFromCheckpointLocation</code> flag that <code>StreamingQueryManager</code> uses to start a streaming query and which is enabled by default (and is in fact the only place where <code>createQuery</code> is used).</p> <ul> <li> <p><code>memory</code> sink has the flag enabled for Complete output mode only</p> </li> <li> <p><code>foreach</code> sink has the flag always enabled</p> </li> <li> <p><code>console</code> sink has the flag always disabled</p> </li> <li> <p>all other sinks have the flag always enabled</p> </li> </ul>","text":"","location":"StreamingQueryManager/#recoverfromcheckpointlocation"},{"title":"userSpecifiedName <p><code>userSpecifiedName</code> corresponds to <code>queryName</code> option (that can be defined using <code>DataStreamWriter</code>'s queryName method) while <code>userSpecifiedCheckpointLocation</code> is <code>checkpointLocation</code> option.</p>","text":"","location":"StreamingQueryManager/#userspecifiedname"},{"title":"Starting Streaming Query Execution <pre><code>startQuery(\n  userSpecifiedName: Option[String],\n  userSpecifiedCheckpointLocation: Option[String],\n  df: DataFrame,\n  extraOptions: Map[String, String],\n  sink: BaseStreamingSink,\n  outputMode: OutputMode,\n  useTempCheckpointLocation: Boolean = false,\n  recoverFromCheckpointLocation: Boolean = true,\n  trigger: Trigger = ProcessingTime(0),\n  triggerClock: Clock = new SystemClock()): StreamingQuery\n</code></pre> <p><code>startQuery</code> starts a streaming query and returns a handle to it.</p> <p>Internally, <code>startQuery</code> first creates a StreamingQueryWrapper, registers it in activeQueries internal registry (by the id), requests it for the underlying StreamExecution and starts it.</p> <p>In the end, <code>startQuery</code> returns the StreamingQueryWrapper (as part of the fluent API so you can chain operators) or throws the exception that was reported when attempting to start the query.</p> <p><code>startQuery</code> throws an <code>IllegalArgumentException</code> when there is another query registered under <code>name</code>. <code>startQuery</code> looks it up in the activeQueries internal registry.</p> <pre><code>Cannot start query with name [name] as a query with that name is already active\n</code></pre> <p><code>startQuery</code> throws an <code>IllegalStateException</code> when a query is started again from checkpoint. <code>startQuery</code> looks it up in activeQueries internal registry.</p> <pre><code>Cannot start query with id [id] as another query with same id is already active. Perhaps you are attempting to restart a query from checkpoint that is already active.\n</code></pre> <p><code>startQuery</code> is used when <code>DataStreamWriter</code> is requested to start an execution of the streaming query.</p>","text":"","location":"StreamingQueryManager/#starting-streaming-query-execution"},{"title":"Posting StreamingQueryListener Event to StreamingQueryListenerBus <pre><code>postListenerEvent(\n  event: StreamingQueryListener.Event): Unit\n</code></pre> <p><code>postListenerEvent</code> simply posts the input <code>event</code> to the internal event bus for streaming events (StreamingQueryListenerBus).</p> <p></p> <p><code>postListenerEvent</code> is used when <code>StreamExecution</code> is requested to post a streaming event.</p>","text":"","location":"StreamingQueryManager/#posting-streamingquerylistener-event-to-streamingquerylistenerbus"},{"title":"Handling Termination of Streaming Query (and Deactivating Query in StateStoreCoordinator) <pre><code>notifyQueryTermination(\n  terminatedQuery: StreamingQuery): Unit\n</code></pre> <p><code>notifyQueryTermination</code> removes the <code>terminatedQuery</code> from activeQueries internal registry (by the query id).</p> <p><code>notifyQueryTermination</code> records the <code>terminatedQuery</code> in lastTerminatedQuery internal registry (when no earlier streaming query was recorded or the <code>terminatedQuery</code> terminated due to an exception).</p> <p><code>notifyQueryTermination</code> notifies others that are blocked on awaitTerminationLock.</p> <p>In the end, <code>notifyQueryTermination</code> requests StateStoreCoordinator to deactivate all active runs of the streaming query.</p> <p></p> <p><code>notifyQueryTermination</code> is used when <code>StreamExecution</code> is requested to run a streaming query and the query has finished (running streaming batches) (with or without an exception).</p>","text":"","location":"StreamingQueryManager/#handling-termination-of-streaming-query-and-deactivating-query-in-statestorecoordinator"},{"title":"Active Streaming Queries by ID <p>Registry of StreamingQuerys per <code>UUID</code></p> <p>Used when <code>StreamingQueryManager</code> is requested for active streaming queries, get a streaming query by id, starts a streaming query and is notified that a streaming query has terminated.</p>","text":"","location":"StreamingQueryManager/#active-streaming-queries-by-id"},{"title":"Last-Terminated Streaming Query <p>StreamingQuery that has recently been terminated (i.e. stopped or due to an exception).</p> <p><code>null</code> when no streaming query has terminated yet or resetTerminated.</p> <ul> <li> <p>Used in awaitAnyTermination to know when a streaming query has terminated</p> </li> <li> <p>Set when <code>StreamingQueryManager</code> is notified that a streaming query has terminated</p> </li> </ul>","text":"","location":"StreamingQueryManager/#last-terminated-streaming-query"},{"title":"StateStoreCoordinatorRef <p>StateStoreCoordinatorRef to the <code>StateStoreCoordinator</code> RPC Endpoint</p> <ul> <li>Created when <code>StreamingQueryManager</code> is created</li> </ul> <p>Used when:</p> <ul> <li> <p><code>StreamingQueryManager</code> is notified that a streaming query has terminated</p> </li> <li> <p>Stateful operators are executed (FlatMapGroupsWithStateExec, StateStoreRestoreExec, StateStoreSaveExec, StreamingDeduplicateExec and StreamingSymmetricHashJoinExec)</p> </li> <li> <p>Creating StateStoreRDD (with storeUpdateFunction aborting StateStore when a task fails)</p> </li> </ul>","text":"","location":"StreamingQueryManager/#statestorecoordinatorref"},{"title":"StreamingQueryWrapper \u2014 Serializable StreamExecution","text":"<p> <code>StreamingQueryWrapper</code> is a serializable interface of a StreamExecution. <p><code>StreamingQueryWrapper</code> has the same StreamExecution API and simply passes all the method calls along to the underlying StreamExecution.</p> <p><code>StreamingQueryWrapper</code> is created when <code>StreamingQueryManager</code> is requested to create a streaming query (when <code>DataStreamWriter</code> is requested to start an execution of the streaming query).</p>","location":"StreamingQueryWrapper/"},{"title":"Demo: Any Streaming Query is StreamingQueryWrapper","text":"<pre><code>import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper\nval query = spark\n  .readStream\n  .format(\"rate\")\n  .load\n  .writeStream\n  .format(\"memory\")\n  .queryName(\"rate2memory\")\n  .start\nassert(query.isInstanceOf[StreamingQueryWrapper])\n</code></pre>","location":"StreamingQueryWrapper/#demo-any-streaming-query-is-streamingquerywrapper"},{"title":"StreamingWrite","text":"<p><code>StreamingWrite</code> is...FIXME</p>","location":"StreamingWrite/"},{"title":"SupportsAdmissionControl","text":"<p><code>SupportsAdmissionControl</code>\u00a0is an extension of the SparkDataStream abstraction for streaming sources that want to control the rate of data ingested in Micro-Batch Stream Processing.</p>","location":"SupportsAdmissionControl/"},{"title":"Contract","text":"","location":"SupportsAdmissionControl/#contract"},{"title":"Default ReadLimit <pre><code>ReadLimit getDefaultReadLimit()\n</code></pre> <p>Default: <code>ReadLimit.allAvailable</code></p> <p>Used when:</p> <ul> <li><code>MicroBatchExecution</code> stream execution engine is requested for the analyzed logical plan (of the streaming query)</li> </ul>","text":"","location":"SupportsAdmissionControl/#default-readlimit"},{"title":"Latest Offset <pre><code>Offset latestOffset(\n  Offset startOffset,\n  ReadLimit limit)\n</code></pre> <p>Used when <code>MicroBatchExecution</code> stream execution engine is requested for the next micro-batch</p>","text":"","location":"SupportsAdmissionControl/#latest-offset"},{"title":"Implementations","text":"<ul> <li>SupportsTriggerAvailableNow</li> </ul>","location":"SupportsAdmissionControl/#implementations"},{"title":"SupportsTriggerAvailableNow","text":"<p><code>SupportsTriggerAvailableNow</code> is an extension of the SupportsAdmissionControl abstraction for streaming sources to support Trigger.AvailableNow mode.</p>","location":"SupportsTriggerAvailableNow/"},{"title":"Contract","text":"","location":"SupportsTriggerAvailableNow/#contract"},{"title":"prepareForTriggerAvailableNow <pre><code>void prepareForTriggerAvailableNow()\n</code></pre> <p>Lets a streaming source to prepare for the default ReadLimit (in Trigger.AvailableNow mode)</p> <p>Used when:</p> <ul> <li><code>MicroBatchExecution</code> is requested for the logicalPlan (and the uniqueSources for MultiBatchExecutor)</li> </ul>","text":"","location":"SupportsTriggerAvailableNow/#preparefortriggeravailablenow"},{"title":"Implementations","text":"<ul> <li><code>AvailableNowDataStreamWrapper</code></li> <li>FileStreamSource</li> <li>KafkaMicroBatchStream</li> <li>KafkaSource</li> <li>RatePerMicroBatchStream</li> </ul>","location":"SupportsTriggerAvailableNow/#implementations"},{"title":"Trigger","text":"<p><code>Trigger</code> is an abstraction of policies that indicate how often a StreamingQuery should be executed (triggered) and possibly emit a new data.</p> <p><code>Trigger</code> is used to determine a TriggerExecutor in MicroBatchExecution and ContinuousExecution.</p>    Trigger TriggerExecutor Factory Method     AvailableNowTrigger MultiBatchExecutor Trigger.AvailableNow   ContinuousTrigger ProcessingTimeExecutor Trigger.Continuous   OneTimeTrigger SingleBatchExecutor Trigger.Once   ProcessingTimeTrigger ProcessingTimeExecutor Trigger.ProcessingTime","location":"Trigger/"},{"title":"Implementations","text":"","location":"Trigger/#implementations"},{"title":"AvailableNowTrigger <p>Processes all available data in multiple batches then terminates a query. Best for streaming sources that are SupportsTriggerAvailableNow.</p> <p>Created by Trigger.AvailableNow</p>  <p>SPARK-36533</p> <p><code>AvailableNowTrigger</code> is a new feature in 3.3.0 (tracked under SPARK-36533).</p>","text":"","location":"Trigger/#availablenowtrigger"},{"title":"ContinuousTrigger <p>Continuously processes streaming data, asynchronously checkpointing at the specified interval</p>","text":"","location":"Trigger/#continuoustrigger"},{"title":"OneTimeTrigger <p>Processes all available data in one batch then terminates the query</p>","text":"","location":"Trigger/#onetimetrigger"},{"title":"ProcessingTimeTrigger","text":"","location":"Trigger/#processingtimetrigger"},{"title":"Static Methods","text":"<p><code>Trigger</code> is also a factory object with static methods to create the policies.</p> <pre><code>import org.apache.spark.sql.streaming.Trigger\n</code></pre>","location":"Trigger/#static-methods"},{"title":"AvailableNow <pre><code>Trigger AvailableNow()\n</code></pre> <p>Creates an AvailableNowTrigger</p> <p>Supported by SupportsTriggerAvailableNow data sources (e.g., files, kafka and rate-micro-batch)</p>","text":"","location":"Trigger/#availablenow"},{"title":"Continuous <pre><code>Trigger Continuous(\n  Duration interval)\nTrigger Continuous(\n  long intervalMs)\nTrigger Continuous(\n  long interval,\n  TimeUnit timeUnit)\nTrigger Continuous(\n  Duration interval)\n</code></pre> <p>Creates a ContinuousTrigger</p>","text":"","location":"Trigger/#continuous"},{"title":"Once <pre><code>Trigger Once()\n</code></pre> <p>Creates a OneTimeTrigger</p>","text":"","location":"Trigger/#once"},{"title":"ProcessingTime <pre><code>Trigger ProcessingTime(\n  Duration interval)\nTrigger ProcessingTime(\n  long intervalMs)\nTrigger ProcessingTime(\n  long interval,\n  TimeUnit timeUnit)\nTrigger ProcessingTime(\n  String interval)\n</code></pre> <p>Creates a ProcessingTimeTrigger</p>","text":"","location":"Trigger/#processingtime"},{"title":"DataStreamWriter <p>A <code>Trigger</code> of a streaming query is defined using DataStreamWriter.trigger.</p>","text":"","location":"Trigger/#datastreamwriter"},{"title":"Demo: Trigger.Once <pre><code>import org.apache.spark.sql.streaming.Trigger\nval query = spark.\n  readStream.\n  format(\"rate\").\n  load.\n  writeStream.\n  format(\"console\").\n  option(\"truncate\", false).\n  trigger(Trigger.Once). // &lt;-- execute once and stop\n  queryName(\"rate-once\").\n  start\nassert(query.isActive == false)\nprintln(query.lastProgress)\n</code></pre> <pre><code>{\n  \"id\" : \"2ae4b0a4-434f-4ca7-a523-4e859c07175b\",\n  \"runId\" : \"24039ce5-906c-4f90-b6e7-bbb3ec38a1f5\",\n  \"name\" : \"rate-once\",\n  \"timestamp\" : \"2017-07-04T18:39:35.998Z\",\n  \"numInputRows\" : 0,\n  \"processedRowsPerSecond\" : 0.0,\n  \"durationMs\" : {\n    \"addBatch\" : 1365,\n    \"getBatch\" : 29,\n    \"getOffset\" : 0,\n    \"queryPlanning\" : 285,\n    \"triggerExecution\" : 1742,\n    \"walCommit\" : 40\n  },\n  \"stateOperators\" : [ ],\n  \"sources\" : [ {\n    \"description\" : \"RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8]\",\n    \"startOffset\" : null,\n    \"endOffset\" : 0,\n    \"numInputRows\" : 0,\n    \"processedRowsPerSecond\" : 0.0\n  } ],\n  \"sink\" : {\n    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSink@7dbf277\"\n  }\n}\n</code></pre>","text":"","location":"Trigger/#demo-triggeronce"},{"title":"IllegalStateException: Custom Triggers Not Supported <p>Although <code>Trigger</code> allows for custom implementations, <code>StreamExecution</code> refuses such attempts and reports an <code>IllegalStateException</code>.</p> <pre><code>import org.apache.spark.sql.streaming.Trigger\ncase object MyTrigger extends Trigger\nval sq = spark\n  .readStream\n  .format(\"rate\")\n  .load\n  .writeStream\n  .format(\"console\")\n  .trigger(MyTrigger) // &lt;-- use custom trigger\n  .queryName(\"rate-custom-trigger\")\n  .start\n</code></pre> <pre><code>java.lang.IllegalStateException: Unknown type of trigger: MyTrigger\n  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.&lt;init&gt;(MicroBatchExecution.scala:56)\n  at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:279)\n  at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:326)\n  at org.apache.spark.sql.streaming.DataStreamWriter.startQuery(DataStreamWriter.scala:427)\n  at org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:406)\n  at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:249)\n  ... 47 elided\n</code></pre>","text":"","location":"Trigger/#illegalstateexception-custom-triggers-not-supported"},{"title":"TriggerExecutor","text":"<p><code>TriggerExecutor</code> is an abstraction of trigger executors.</p>","location":"TriggerExecutor/"},{"title":"Contract","text":"","location":"TriggerExecutor/#contract"},{"title":"Executing Batches <pre><code>execute(\n  batchRunner: () =&gt; Boolean): Unit\n</code></pre> <p>Executes batches using a batch runner (trigger handler). <code>batchRunner</code> is assumed to return <code>false</code> to indicate to terminate execution</p> <p>Used when:</p> <ul> <li><code>MicroBatchExecution</code> is requested to run an activated streaming query</li> </ul>","text":"","location":"TriggerExecutor/#executing-batches"},{"title":"Implementations","text":"","location":"TriggerExecutor/#implementations"},{"title":"MultiBatchExecutor <p>Executes the batch runner until it returns <code>false</code></p> <p>Handles AvailableNowTrigger in MicroBatchExecution</p> <p>Used when:</p> <ul> <li><code>MicroBatchExecution</code> is requested for the analyzed logical plan (and extracting unique streaming sources)</li> </ul>","text":"","location":"TriggerExecutor/#multibatchexecutor"},{"title":"ProcessingTimeExecutor <p>Executes the batch runner at regular intervals (as defined using ProcessingTime and DataStreamWriter.trigger method)</p> <p>Processing terminates when <code>batchRunner</code> returns <code>false</code>.</p>","text":"","location":"TriggerExecutor/#processingtimeexecutor"},{"title":"SingleBatchExecutor <p>Executes the batch runner exactly once</p>","text":"","location":"TriggerExecutor/#singlebatchexecutor"},{"title":"UnsupportedOperationChecker","text":"<p><code>UnsupportedOperationChecker</code> checks whether the logical plan of a streaming query uses supported operations only.</p> <p><code>UnsupportedOperationChecker</code> is used when the internal spark.sql.streaming.unsupportedOperationCheck Spark property is enabled.</p>  <p>Note</p> <p><code>UnsupportedOperationChecker</code> comes actually with two methods, i.e. <code>checkForBatch</code> and &lt;&gt;, whose names reveal the different flavours of Spark SQL (as of 2.0), i.e. batch and streaming, respectively. <p>The Spark Structured Streaming gitbook is solely focused on &lt;&gt; method.","location":"UnsupportedOperationChecker/"},{"title":"checkForStreaming Method <pre><code>checkForStreaming(\n  plan: LogicalPlan,\n  outputMode: OutputMode): Unit\n</code></pre> <p><code>checkForStreaming</code> asserts that the following requirements hold:</p> <ol> <li> <p>&lt;&gt;  <li> <p>&lt;&gt; (on the grouping expressions)  <li> <p>&lt;&gt;   <p><code>checkForStreaming</code>...FIXME</p> <p><code>checkForStreaming</code> finds all streaming aggregates (i.e. <code>Aggregate</code> logical operators with streaming sources).</p>  <p>Note</p> <p><code>Aggregate</code> logical operator represents Dataset.groupBy and Dataset.groupByKey operators (and SQL's <code>GROUP BY</code> clause) in a logical query plan.</p>  <p>[[only-one-streaming-aggregation-allowed]] <code>checkForStreaming</code> asserts that there is exactly one streaming aggregation in a streaming query.</p> <p>Otherwise, <code>checkForStreaming</code> reports a <code>AnalysisException</code>:</p> <pre><code>Multiple streaming aggregations are not supported with streaming DataFrames/Datasets\n</code></pre> <p>[[streaming-aggregation-append-mode-requires-watermark]] <code>checkForStreaming</code> asserts that watermark was defined for a streaming aggregation with Append output mode (on at least one of the grouping expressions).</p> <p>Otherwise, <code>checkForStreaming</code> reports a <code>AnalysisException</code>:</p> <pre><code>Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark\n</code></pre> <p>CAUTION: FIXME</p> <p><code>checkForStreaming</code> counts all FlatMapGroupsWithState logical operators (on streaming Datasets with <code>isMapGroupsWithState</code> flag disabled).</p>  <p>Note</p> <p>FlatMapGroupsWithState.isMapGroupsWithState flag is disabled when...FIXME</p>  <p>[[multiple-flatMapGroupsWithState]] <code>checkForStreaming</code> asserts that multiple FlatMapGroupsWithState logical operators are only used when:</p> <ul> <li> <p><code>outputMode</code> is Append output mode</p> </li> <li> <p>outputMode of the <code>FlatMapGroupsWithState</code> logical operators is also Append output mode</p> </li> </ul> <p>CAUTION: FIXME Reference to an example in <code>flatMapGroupsWithState</code></p> <p>Otherwise, <code>checkForStreaming</code> reports a <code>AnalysisException</code>:</p> <pre><code>Multiple flatMapGroupsWithStates are not supported when they are not all in append mode or the output mode is not append on a streaming DataFrames/Datasets\n</code></pre> <p>CAUTION: FIXME</p> <p><code>checkForStreaming</code> is used when <code>StreamingQueryManager</code> is requested to create a StreamingQueryWrapper (for starting a streaming query), but only when the internal spark.sql.streaming.unsupportedOperationCheck configuration property is enabled.</p>","text":"","location":"UnsupportedOperationChecker/#checkforstreaming-method"},{"title":"WatermarkTracker","text":"<p><code>WatermarkTracker</code> tracks the event-time watermark of a streaming query (across EventTimeWatermarkExec operators in a physical query plan) based on a given MultipleWatermarkPolicy.</p> <p><code>WatermarkTracker</code> is used in MicroBatchExecution.</p>","location":"WatermarkTracker/"},{"title":"Creating Instance","text":"<p><code>WatermarkTracker</code> takes the following to be created:</p> <ul> <li>MultipleWatermarkPolicy</li> </ul> <p><code>WatermarkTracker</code> is created (using apply) when <code>MicroBatchExecution</code> is requested to populate start offsets at start or restart (from a checkpoint).</p>","location":"WatermarkTracker/#creating-instance"},{"title":"MultipleWatermarkPolicy <p><code>WatermarkTracker</code> is given a <code>MultipleWatermarkPolicy</code> when created that can be one of the following:</p> <ul> <li><code>MaxWatermark</code> (alias: <code>min</code>)</li> <li><code>MinWatermark</code> (alias: <code>max</code>)</li> </ul>","text":"","location":"WatermarkTracker/#multiplewatermarkpolicy"},{"title":"Creating WatermarkTracker <pre><code>apply(\n  conf: RuntimeConfig): WatermarkTracker\n</code></pre> <p><code>apply</code> uses the spark.sql.streaming.multipleWatermarkPolicy configuration property for the global watermark policy (default: <code>min</code>) and creates a <code>WatermarkTracker</code>.</p> <p><code>apply</code> is used when <code>MicroBatchExecution</code> is requested to populate start offsets at start or restart (from a checkpoint).</p>","text":"","location":"WatermarkTracker/#creating-watermarktracker"},{"title":"Global Event-Time Watermark <pre><code>globalWatermarkMs: Long\n</code></pre> <p><code>WatermarkTracker</code> uses <code>globalWatermarkMs</code> internal registry to keep track of global event-time watermark (based on MultipleWatermarkPolicy across all EventTimeWatermarkExec operators in a physical query plan).</p> <p>Default: <code>0</code></p> <p><code>globalWatermarkMs</code> is used when <code>WatermarkTracker</code> is requested to updateWatermark.</p> <p>The event-time watermark can be updated in setWatermark and updateWatermark.</p> <p>The event-time watermark is used (as <code>currentWatermark</code> method) when <code>MicroBatchExecution</code> stream execution engine is requested to populateStartOffsets and constructNextBatch and runBatch.</p>","text":"","location":"WatermarkTracker/#global-event-time-watermark"},{"title":"Setting Watermark <pre><code>setWatermark(\n  newWatermarkMs: Long): Unit\n</code></pre> <p><code>setWatermark</code> sets the global event-time watermark to the given <code>newWatermarkMs</code> value.</p>  <p><code>setWatermark</code> is used when:</p> <ul> <li><code>MicroBatchExecution</code> is requested to populate start offsets at startup or restart (from a checkpoint)</li> </ul>","text":"","location":"WatermarkTracker/#setting-watermark"},{"title":"Updating Watermark (at Execution) <pre><code>updateWatermark(\n  executedPlan: SparkPlan): Unit\n</code></pre> <p><code>updateWatermark</code> requests the given <code>SparkPlan</code> physical operator to collect all EventTimeWatermarkExec unary physical operators.</p> <p><code>updateWatermark</code> simply exits when no <code>EventTimeWatermarkExec</code> was found.</p> <p><code>updateWatermark</code>...FIXME</p>  <p><code>updateWatermark</code> is used when:</p> <ul> <li><code>MicroBatchExecution</code> is requested to run a single streaming batch</li> </ul>","text":"","location":"WatermarkTracker/#updating-watermark-at-execution"},{"title":"Watermarks by EventTimeWatermarkExec Operator Registry <pre><code>operatorToWatermarkMap: Map[Int, Long]\n</code></pre> <p><code>WatermarkTracker</code> uses <code>operatorToWatermarkMap</code> internal registry to keep track of event-time watermarks of every EventTimeWatermarkExec physical operator in a streaming query plan.</p> <p><code>operatorToWatermarkMap</code> is used when <code>WatermarkTracker</code> is requested to updateWatermark.</p>","text":"","location":"WatermarkTracker/#watermarks-by-eventtimewatermarkexec-operator-registry"},{"title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.streaming.WatermarkTracker</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.streaming.WatermarkTracker=ALL\n</code></pre> <p>Refer to Logging.</p>","text":"","location":"WatermarkTracker/#logging"},{"title":"Configuration Properties","text":"<p>Configuration properties (aka settings) allow you to fine-tune a Spark Structured Streaming application.</p>  The Internals of Spark SQL <p>Learn more about Configuration Properties in The Internals of Spark SQL.</p>","location":"configuration-properties/"},{"title":"aggregation.stateFormatVersion <p>spark.sql.streaming.aggregation.stateFormatVersion</p> <p>(internal) Version of the state format (and a StreamingAggregationStateManagerBaseImpl)</p> <p>Default: <code>2</code></p> <p>Supported values:</p> <ul> <li><code>1</code> (for the legacy <code>StreamingAggregationStateManagerImplV1</code>)</li> <li><code>2</code> (for the default StreamingAggregationStateManagerImplV2)</li> </ul> <p>Checkpointed property</p> <p>Used when:</p> <ul> <li>StatefulAggregationStrategy execution planning strategy is executed (and plans a streaming query with a non-windowed aggregate)</li> </ul>","text":"","location":"configuration-properties/#aggregationstateformatversion"},{"title":"checkpointFileManagerClass <p>spark.sql.streaming.checkpointFileManagerClass</p> <p>(internal) CheckpointFileManager to use to write checkpoint files atomically</p> <p>Default: (undefined)</p> <p>Unless defined, FileContextBasedCheckpointFileManager is considered first, followed by FileSystemBasedCheckpointFileManager in case of unsupported file system used for storing metadata files</p> <p>Used when:</p> <ul> <li><code>CheckpointFileManager</code> is requested to create a CheckpointFileManager</li> </ul>","text":"","location":"configuration-properties/#checkpointfilemanagerclass"},{"title":"checkpointLocation <p>spark.sql.streaming.checkpointLocation</p> <p>Default checkpoint directory for storing checkpoint data</p> <p>Default: <code>(empty)</code></p>","text":"","location":"configuration-properties/#checkpointlocation"},{"title":"commitProtocolClass <p>spark.sql.streaming.commitProtocolClass</p> <p>(internal) <code>FileCommitProtocol</code> to use for writing out micro-batches in FileStreamSink.</p> <p>Default: org.apache.spark.sql.execution.streaming.ManifestFileCommitProtocol</p> <p>Use SQLConf.streamingFileCommitProtocolClass to access the current value.</p>  The Internals of Apache Spark <p>Learn more on FileCommitProtocol in The Internals of Apache Spark.</p>","text":"","location":"configuration-properties/#commitprotocolclass"},{"title":"continuous.executorQueueSize <p>spark.sql.streaming.continuous.executorQueueSize</p> <p>(internal) The size (measured in number of rows) of the queue used in continuous execution to buffer the results of a ContinuousDataReader.</p> <p>Default: <code>1024</code></p>","text":"","location":"configuration-properties/#continuousexecutorqueuesize"},{"title":"continuous.executorPollIntervalMs <p>spark.sql.streaming.continuous.executorPollIntervalMs</p> <p>(internal) The interval (in millis) at which continuous execution readers will poll to check whether the epoch has advanced on the driver.</p> <p>Default: <code>100</code> (ms)</p>","text":"","location":"configuration-properties/#continuousexecutorpollintervalms"},{"title":"disabledV2MicroBatchReaders <p>spark.sql.streaming.disabledV2MicroBatchReaders</p> <p>(internal) A comma-separated list of fully-qualified class names of data source providers for which MicroBatchStream is disabled. Reads from these sources will fall back to the V1 Sources.</p> <p>Default: <code>(empty)</code></p> <p>Use SQLConf.disabledV2StreamingMicroBatchReaders to get the current value.</p>","text":"","location":"configuration-properties/#disabledv2microbatchreaders"},{"title":"fileSink.log.cleanupDelay <p>spark.sql.streaming.fileSink.log.cleanupDelay</p> <p>(internal) How long (in millis) that a file is guaranteed to be visible for all readers.</p> <p>Default: <code>10 minutes</code></p> <p>Use SQLConf.fileSinkLogCleanupDelay to access the current value.</p>","text":"","location":"configuration-properties/#filesinklogcleanupdelay"},{"title":"fileSink.log.deletion <p>spark.sql.streaming.fileSink.log.deletion</p> <p>(internal) Whether to delete the expired log files in file stream sink</p> <p>Default: <code>true</code></p> <p>Use SQLConf.fileSinkLogDeletion to access the current value.</p>","text":"","location":"configuration-properties/#filesinklogdeletion"},{"title":"fileSink.log.compactInterval <p>spark.sql.streaming.fileSink.log.compactInterval</p> <p>(internal) Number of log files after which all the previous files are compacted into the next log file</p> <p>Default: <code>10</code></p> <p>Use SQLConf.fileSinkLogCompactInterval to access the current value.</p>","text":"","location":"configuration-properties/#filesinklogcompactinterval"},{"title":"fileSource.log.cleanupDelay <p>spark.sql.streaming.fileSource.log.cleanupDelay</p> <p>(internal) How long (in millis) a file is guaranteed to be visible for all readers.</p> <p>Default: <code>10</code> (minutes)</p> <p>Use SQLConf.fileSourceLogCleanupDelay to get the current value.</p>","text":"","location":"configuration-properties/#filesourcelogcleanupdelay"},{"title":"fileSource.log.compactInterval <p>spark.sql.streaming.fileSource.log.compactInterval</p> <p>(internal) Number of log files after which all the previous files are compacted into the next log file.</p> <p>Default: <code>10</code></p> <p>Must be a positive value (greater than <code>0</code>)</p> <p>Use SQLConf.fileSourceLogCompactInterval to get the current value.</p>","text":"","location":"configuration-properties/#filesourcelogcompactinterval"},{"title":"fileSource.log.deletion <p>spark.sql.streaming.fileSource.log.deletion</p> <p>(internal) Whether to delete the expired log files in file stream source</p> <p>Default: <code>true</code></p> <p>Use SQLConf.fileSourceLogDeletion to get the current value.</p>","text":"","location":"configuration-properties/#filesourcelogdeletion"},{"title":"flatMapGroupsWithState.stateFormatVersion <p>spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion</p> <p>(internal) State format version used to create a StateManager for FlatMapGroupsWithStateExec physical operator</p> <p>Default: <code>2</code></p> <p>Supported values:</p> <ul> <li><code>1</code></li> <li><code>2</code></li> </ul> <p>Checkpointed property</p> <p>Used when:</p> <ul> <li>FlatMapGroupsWithStateStrategy execution planning strategy is requested to plan a streaming query (and creates a FlatMapGroupsWithStateExec physical operator for every FlatMapGroupsWithState logical operator)</li> </ul>","text":"","location":"configuration-properties/#flatmapgroupswithstatestateformatversion"},{"title":"join.stateFormatVersion <p>spark.sql.streaming.join.stateFormatVersion</p> <p>(internal) State format version used by streaming join operations in a streaming query. State between versions tend to be incompatible, so state format version shouldn't be modified after running.</p> <p>Default: <code>2</code></p> <p>Supported values:</p> <ul> <li><code>1</code></li> <li><code>2</code></li> </ul>","text":"","location":"configuration-properties/#joinstateformatversion"},{"title":"maxBatchesToRetainInMemory <p>spark.sql.streaming.maxBatchesToRetainInMemory</p> <p>(internal) The maximum number of batches which will be retained in memory to avoid loading from files.</p> <p>Default: <code>2</code></p> <p>Maximum count of versions a State Store implementation should retain in memory.</p> <p>The value adjusts a trade-off between memory usage vs cache miss:</p> <ul> <li><code>2</code> covers both success and direct failure cases</li> <li><code>1</code> covers only success case</li> <li><code>0</code> or negative value disables cache to maximize memory size of executors</li> </ul> <p>Used when <code>HDFSBackedStateStoreProvider</code> is requested to initialize.</p>","text":"","location":"configuration-properties/#maxbatchestoretaininmemory"},{"title":"metricsEnabled <p>spark.sql.streaming.metricsEnabled</p> <p>Enables streaming metrics</p> <p>Default: <code>false</code></p> <p>Use SQLConf.streamingMetricsEnabled to access the current value.</p>","text":"","location":"configuration-properties/#metricsenabled"},{"title":"minBatchesToRetain <p>spark.sql.streaming.minBatchesToRetain</p> <p>(internal) Minimum number of batches that must be retained and made recoverable</p> <p>Stream execution engines discard (purge) offsets from the <code>offsets</code> metadata log when the current batch ID (in MicroBatchExecution) or the epoch committed (in ContinuousExecution) is above the threshold.</p> <p>Default: <code>100</code></p> <p>Use SQLConf.minBatchesToRetain to access the current value.</p>","text":"","location":"configuration-properties/#minbatchestoretain"},{"title":"multipleWatermarkPolicy <p>spark.sql.streaming.multipleWatermarkPolicy</p> <p>Global watermark policy that is the policy to calculate the global watermark value when there are multiple watermark operators in a streaming query</p> <p>Default: <code>min</code></p> <p>Supported values:</p> <ul> <li><code>min</code> - chooses the minimum watermark reported across multiple operators</li> <li><code>max</code> - chooses the maximum across multiple operators</li> </ul> <p>Cannot be changed between query restarts from the same checkpoint location.</p>","text":"","location":"configuration-properties/#multiplewatermarkpolicy"},{"title":"noDataMicroBatches.enabled <p>spark.sql.streaming.noDataMicroBatches.enabled</p> <p>Controls whether the streaming micro-batch engine should execute batches with no data to process for eager state management for stateful streaming queries (<code>true</code>) or not (<code>false</code>).</p> <p>Default: <code>true</code></p> <p>Use SQLConf.streamingNoDataMicroBatchesEnabled to get the current value</p>","text":"","location":"configuration-properties/#nodatamicrobatchesenabled"},{"title":"noDataProgressEventInterval <p>spark.sql.streaming.noDataProgressEventInterval</p> <p>(internal) How long to wait between two progress events when there is no data (in millis) when <code>ProgressReporter</code> is requested to finish a trigger</p> <p>Default: <code>10000L</code></p> <p>Use SQLConf.streamingNoDataProgressEventInterval to get the current value</p>","text":"","location":"configuration-properties/#nodataprogresseventinterval"},{"title":"numRecentProgressUpdates <p>spark.sql.streaming.numRecentProgressUpdates</p> <p>Number of StreamingQueryProgresses to retain in progressBuffer internal registry when <code>ProgressReporter</code> is requested to update progress of streaming query</p> <p>Default: <code>100</code></p> <p>Use SQLConf.streamingProgressRetention to get the current value</p>","text":"","location":"configuration-properties/#numrecentprogressupdates"},{"title":"pollingDelay <p>spark.sql.streaming.pollingDelay</p> <p>(internal) How long (in millis) to delay <code>StreamExecution</code> before polls for new data when no data was available in a batch</p> <p>Default: <code>10</code> (milliseconds)</p>","text":"","location":"configuration-properties/#pollingdelay"},{"title":"statefulOperator.useStrictDistribution <p>spark.sql.streaming.statefulOperator.useStrictDistribution</p> <p>The purpose of this config is only compatibility; DO NOT MANUALLY CHANGE THIS!!!</p> <p>When <code>true</code>, the stateful operator for streaming query will use StatefulOpClusteredDistribution which guarantees stable state partitioning as long as the operator provides consistent grouping keys across the lifetime of query.</p> <p>When <code>false</code>, the stateful operator for streaming query will use ClusteredDistribution which is not sufficient to guarantee stable state partitioning despite the operator provides consistent grouping keys across the lifetime of query.</p> <p>This config will be set to <code>true</code> for new streaming queries to guarantee stable state partitioning, and set to false for existing streaming queries to not break queries which are restored from existing checkpoints.</p> <p>Please refer SPARK-38204 for details.</p> <p>Default: <code>true</code></p> <p>Checkpointed property</p> <p>Used when:</p> <ul> <li><code>StatefulOperatorPartitioning</code> is requested to getCompatibleDistribution</li> </ul>","text":"","location":"configuration-properties/#statefuloperatorusestrictdistribution"},{"title":"stateStore.compression.codec <p>spark.sql.streaming.stateStore.compression.codec</p> <p>(internal) The codec used to compress delta and snapshot files generated by StateStore. By default, Spark provides four codecs: lz4, lzf, snappy, and zstd. You can also use fully-qualified class names to specify the codec.</p> <p>Default: <code>lz4</code></p>","text":"","location":"configuration-properties/#statestorecompressioncodec"},{"title":"stateStore.maintenanceInterval <p>spark.sql.streaming.stateStore.maintenanceInterval</p> <p>The initial delay and how often to execute StateStore's maintenance task.</p> <p>Default: <code>60s</code></p>","text":"","location":"configuration-properties/#statestoremaintenanceinterval"},{"title":"stateStore.minDeltasForSnapshot <p>spark.sql.streaming.stateStore.minDeltasForSnapshot</p> <p>(internal) Minimum number of state store delta files that need to be generated before <code>HDFSBackedStateStore</code> will consider generating a snapshot (consolidate the deltas into a snapshot)</p> <p>Default: <code>10</code></p> <p>Use SQLConf.stateStoreMinDeltasForSnapshot to get the current value.</p>","text":"","location":"configuration-properties/#statestoremindeltasforsnapshot"},{"title":"stateStore.providerClass <p>spark.sql.streaming.stateStore.providerClass</p> <p>(internal) The fully-qualified class name of a StateStoreProvider implementation</p> <p>Default: HDFSBackedStateStoreProvider</p> <p>Use SQLConf.stateStoreProviderClass to get the current value.</p> <p>Checkpointed property</p> <p>Used when:</p> <ul> <li><code>StateStoreConf</code> is requested for providerClass</li> <li><code>StateStoreWriter</code> is requested to stateStoreCustomMetrics</li> <li><code>StreamingQueryStatisticsPage</code> is requested for the supportedCustomMetrics</li> </ul>","text":"","location":"configuration-properties/#statestoreproviderclass"},{"title":"stateStore.rocksdb.formatVersion <p>spark.sql.streaming.stateStore.rocksdb.formatVersion</p>","text":"","location":"configuration-properties/#statestorerocksdbformatversion"},{"title":"ui.enabled <p>spark.sql.streaming.ui.enabled</p> <p>Enables Structured Streaming Web UI for a Spark application (with Spark Web UI enabled)</p> <p>Default: <code>true</code></p> <p>Used when:</p> <ul> <li><code>SharedState</code> (Spark SQL) is created</li> </ul>","text":"","location":"configuration-properties/#uienabled"},{"title":"unsupportedOperationCheck <p>spark.sql.streaming.unsupportedOperationCheck</p> <p>(internal) When enabled (<code>true</code>), <code>StreamingQueryManager</code> makes sure that the logical plan of a streaming query uses supported operations only</p> <p>Default: <code>true</code></p>","text":"","location":"configuration-properties/#unsupportedoperationcheck"},{"title":"Offsets and Metadata Checkpointing","text":"<p>A streaming query can be started from scratch or from checkpoint (that gives fault-tolerance as the state is preserved even when a failure happens).</p> <p>Stream execution engines use checkpoint location to resume stream processing and get start offsets to start query processing from.</p> <p><code>StreamExecution</code> resumes (populates the start offsets) from the latest checkpointed offsets from the Write-Ahead Log (WAL) of Offsets that may have already been processed (and, if so, committed to the Offset Commit Log).</p> <ul> <li> <p>Hadoop DFS-based metadata storage of OffsetSeqs</p> </li> <li> <p>OffsetSeq and StreamProgress</p> </li> <li> <p>StreamProgress and StreamExecutions (committed and available offsets)</p> </li> </ul>","location":"offsets-and-metadata-checkpointing/"},{"title":"Micro-Batch Stream Processing","text":"<p>In Micro-Batch Stream Processing, the available offsets registry is populated with the latest offsets from the Write-Ahead Log (WAL) when <code>MicroBatchExecution</code> stream processing engine is requested to populate start offsets from checkpoint (if available) when <code>MicroBatchExecution</code> is requested to run an activated streaming query (before the first \"zero\" micro-batch).</p> <p>The available offsets are then added to the committed offsets when the latest batch ID available (as described above) is exactly the latest batch ID committed to the Offset Commit Log when <code>MicroBatchExecution</code> stream processing engine is requested to populate start offsets from checkpoint.</p> <p>When a streaming query is started from scratch (with no checkpoint that has offsets in the Offset Write-Ahead Log), <code>MicroBatchExecution</code> prints out the following INFO message:</p> <pre><code>Starting new streaming query.\n</code></pre> <p>When a streaming query is resumed (restarted) from a checkpoint with offsets in the Offset Write-Ahead Log, <code>MicroBatchExecution</code> prints out the following INFO message:</p> <pre><code>Resuming at batch [currentBatchId] with committed offsets [committedOffsets] and available offsets [availableOffsets]\n</code></pre> <p>Every time <code>MicroBatchExecution</code> is requested to check whether a new data is available (in any of the streaming sources)...FIXME</p> <p>When <code>MicroBatchExecution</code> is requested to construct the next streaming micro-batch (when <code>MicroBatchExecution</code> requested to run the activated streaming query), every streaming source is requested for the latest offset available that are added to the availableOffsets registry. Streaming sources report some offsets or none at all (if this source has never received any data). Streaming sources with no data are excluded (filtered out).</p> <p><code>MicroBatchExecution</code> prints out the following TRACE message to the logs:</p> <pre><code>noDataBatchesEnabled = [noDataBatchesEnabled], lastExecutionRequiresAnotherBatch = [lastExecutionRequiresAnotherBatch], isNewDataAvailable = [isNewDataAvailable], shouldConstructNextBatch = [shouldConstructNextBatch]\n</code></pre> <p>With shouldConstructNextBatch internal flag enabled, <code>MicroBatchExecution</code> commits (adds) the available offsets for the batch to the Write-Ahead Log (WAL) and prints out the following INFO message to the logs:</p> <pre><code>Committed offsets for batch [currentBatchId]. Metadata [offsetSeqMetadata]\n</code></pre> <p>When running a single streaming micro-batch, <code>MicroBatchExecution</code>...FIXME</p> <p>In the end (of running a single streaming micro-batch), <code>MicroBatchExecution</code> commits (adds) the available offsets (to the committedOffsets registry) so they are considered processed already.</p> <p><code>MicroBatchExecution</code> prints out the following DEBUG message to the logs:</p> <pre><code>Completed batch [currentBatchId]\n</code></pre>","location":"offsets-and-metadata-checkpointing/#micro-batch-stream-processing"},{"title":"Limitations (Assumptions)","text":"<p>It is assumed that the order of streaming sources in a streaming query matches the order of the offsets of OffsetSeq (in offsetLog) and availableOffsets.</p> <p>In other words, a streaming query can be modified and then restarted from a checkpoint (to maintain stream processing state) only when the number of streaming sources and their order are preserved across restarts.</p>","location":"offsets-and-metadata-checkpointing/#limitations-assumptions"},{"title":"Spark Structured Streaming and Streaming Queries","text":"<p>Spark Structured Streaming (Structured Streaming or Spark Streams) is the module of Apache Spark for stream processing using streaming queries.</p> <p>Streaming queries can be expressed using a high-level declarative streaming API (Dataset API) or good ol' SQL (SQL over stream / streaming SQL). The declarative streaming Dataset API and SQL are executed on the underlying highly-optimized Spark SQL engine.</p> <p>The semantics of the Structured Streaming model is as follows (see the article Structured Streaming In Apache Spark):</p>  <p>At any time, the output of a continuous application is equivalent to executing a batch job on a prefix of the data.</p>  <p>As of Spark 2.2.0, Structured Streaming has been marked stable and ready for production use. With that the other older streaming module Spark Streaming is considered obsolete and not for developing new streaming applications with Apache Spark.</p> <p>Spark Structured Streaming comes with two stream execution engines for executing streaming queries:</p> <ul> <li> <p>MicroBatchExecution for Micro-Batch Stream Processing</p> </li> <li> <p>ContinuousExecution for Continuous Stream Processing</p> </li> </ul> <p>The goal of Spark Structured Streaming is to unify streaming, interactive, and batch queries over structured datasets for developing end-to-end stream processing applications dubbed continuous applications using Spark SQL's Datasets API with additional support for the following features:</p> <ul> <li> <p>Streaming Aggregation</p> </li> <li> <p>Streaming Join</p> </li> <li> <p>Streaming Watermark</p> </li> <li> <p>Arbitrary Stateful Streaming Aggregation</p> </li> <li> <p>Stateful Stream Processing</p> </li> </ul> <p>In Structured Streaming, Spark developers describe custom streaming computations in the same way as with Spark SQL. Internally, Structured Streaming applies the user-defined structured query to the continuously and indefinitely arriving data to analyze real-time streaming data.</p> <p>Structured Streaming introduces the concept of streaming datasets that are infinite datasets with primitives like input streaming data sources and output streaming data sinks.</p> <p>A <code>Dataset</code> is streaming when its logical plan is streaming.</p> <pre><code>val batchQuery = spark.\n  read. // &lt;-- batch non-streaming query\n  csv(\"sales\")\n\nassert(batchQuery.isStreaming == false)\n\nval streamingQuery = spark.\n  readStream. // &lt;-- streaming query\n  format(\"rate\").\n  load\n\nassert(streamingQuery.isStreaming)\n</code></pre>  <p>Note</p> <p>Read up on Spark SQL, Datasets and logical plans in The Internals of Spark SQL online book.</p>  <p>Structured Streaming models a stream of data as an infinite (and hence continuous) table that could be changed every streaming batch.</p> <p>You can specify output mode of a streaming dataset which is what gets written to a streaming sink (i.e. the infinite result table) when there is a new data available.</p> <p>Streaming Datasets use streaming query plans (as opposed to regular batch Datasets that are based on batch query plans).</p>  <p>Note</p> <p>From this perspective, batch queries can be considered streaming Datasets executed once only (and is why some batch queries, e.g. KafkaSource, can easily work in batch mode).</p> <pre><code>val batchQuery = spark.read.format(\"rate\").load\n\nassert(batchQuery.isStreaming == false)\n\nval streamingQuery = spark.readStream.format(\"rate\").load\n\nassert(streamingQuery.isStreaming)\n</code></pre>  <p>With Structured Streaming, Spark 2 aims at simplifying streaming analytics with little to no need to reason about effective data streaming (trying to hide the unnecessary complexity in your streaming analytics architectures).</p> <p>Structured streaming is defined by the following data abstractions in <code>org.apache.spark.sql.streaming</code> package:</p> <ul> <li>StreamingQuery</li> <li>Streaming Source</li> <li>Streaming Sink</li> <li>StreamingQueryManager</li> </ul> <p>Structured Streaming follows micro-batch model and periodically fetches data from the data source (and uses the <code>DataFrame</code> data abstraction to represent the fetched data for a certain batch).</p> <p>With Datasets as Spark SQL's view of structured data, structured streaming checks input sources for new data every trigger (time) and executes the (continuous) queries.</p>  <p>Note</p> <p>The feature has also been called Streaming Spark SQL Query, Streaming DataFrames, Continuous DataFrame or Continuous Query. There have been lots of names before the Spark project settled on Structured Streaming.</p>","location":"overview/"},{"title":"References","text":"","location":"overview/#references"},{"title":"Articles","text":"<ul> <li>SPARK-8360 Structured Streaming (aka Streaming DataFrames)</li> <li>The official Structured Streaming Programming Guide</li> <li>Structured Streaming In Apache Spark</li> <li>What Spark's Structured Streaming really means</li> </ul>","location":"overview/#articles"},{"title":"Videos","text":"<ul> <li>The Future of Real Time in Spark from Spark Summit East 2016 in which Reynold Xin presents the concept of Streaming DataFrames</li> <li>Structuring Spark: DataFrames, Datasets, and Streaming</li> <li>A Deep Dive Into Structured Streaming by Tathagata \"TD\" Das from Spark Summit 2016</li> <li>Arbitrary Stateful Aggregations in Structured Streaming in Apache Spark by Burak Yavuz</li> </ul>","location":"overview/#videos"},{"title":"Logging","text":"<p>Tip</p> <p>Find out more on Logging in The Internals of Spark SQL online book.</p>","location":"spark-logging/"},{"title":"flatMapGroupsWithState Operator -- Arbitrary Stateful Streaming Aggregation (with Explicit State Logic)","text":"<pre><code>KeyValueGroupedDataset[K, V].flatMapGroupsWithState[S: Encoder, U: Encoder](\n  outputMode: OutputMode,\n  timeoutConf: GroupStateTimeout)(\n  func: (K, Iterator[V], GroupState[S]) =&gt; Iterator[U]): Dataset[U]\n</code></pre> <p><code>flatMapGroupsWithState</code> operator is used for Arbitrary Stateful Streaming Aggregation (with Explicit State Logic).</p> <p><code>flatMapGroupsWithState</code> requires that the given OutputMode is either Append or Update (and reports an <code>IllegalArgumentException</code> at runtime).</p>  <p>Note</p> <p>An <code>OutputMode</code> is a required argument, but does not seem to be used at all. Check out the question What's the purpose of OutputMode in flatMapGroupsWithState? How/where is it used? on StackOverflow.</p>  <p>Every time the state function <code>func</code> is executed for a key, the state (as <code>GroupState[S]</code>) is for this key only.</p>  <p>Note</p> <ul> <li> <p><code>K</code> is the type of the keys in <code>KeyValueGroupedDataset</code></p> </li> <li> <p><code>V</code> is the type of the values (per key) in <code>KeyValueGroupedDataset</code></p> </li> <li> <p><code>S</code> is the user-defined type of the state as maintained for each group</p> </li> <li> <p><code>U</code> is the type of rows in the result <code>Dataset</code></p> </li> </ul>  <p><code>flatMapGroupsWithState</code> creates a new <code>Dataset</code> with FlatMapGroupsWithState unary logical operator.</p>","location":"spark-sql-streaming-KeyValueGroupedDataset-flatMapGroupsWithState/"},{"title":"mapGroupsWithState","text":"<p>== [[mapGroupsWithState]] mapGroupsWithState Operator -- Stateful Streaming Aggregation (with Explicit State Logic)</p>","location":"spark-sql-streaming-KeyValueGroupedDataset-mapGroupsWithState/"},{"title":"[source, scala]","text":"<p>mapGroupsWithStateS: Encoder, U: Encoder: Dataset[U] // &lt;1&gt; mapGroupsWithStateS: Encoder, U: Encoder(   func: (K, Iterator[V], GroupState[S]) =&gt; U): Dataset[U]</p>  <p>&lt;1&gt; Uses <code>GroupStateTimeout.NoTimeout</code> for <code>timeoutConf</code></p> <p><code>mapGroupsWithState</code> operator...FIXME</p>  <p>Note</p> <p><code>mapGroupsWithState</code> is a special case of flatMapGroupsWithState operator with the following:</p> <ul> <li> <p><code>func</code> being transformed to return a single-element <code>Iterator</code></p> </li> <li> <p>Update output mode</p> </li> </ul> <p><code>mapGroupsWithState</code> also creates a <code>FlatMapGroupsWithState</code> with isMapGroupsWithState internal flag enabled.</p>  <pre><code>// numGroups defined at the beginning\nscala&gt; :type numGroups\norg.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)]\n\nimport org.apache.spark.sql.streaming.GroupState\ndef mappingFunc(key: Long, values: Iterator[(java.sql.Timestamp, Long)], state: GroupState[Long]): Long = {\n  println(s\"&gt;&gt;&gt; key: $key =&gt; state: $state\")\n  val newState = state.getOption.map(_ + values.size).getOrElse(0L)\n  state.update(newState)\n  key\n}\n\nimport org.apache.spark.sql.streaming.GroupStateTimeout\nval longs = numGroups.mapGroupsWithState(\n    timeoutConf = GroupStateTimeout.ProcessingTimeTimeout)(\n    func = mappingFunc)\n\nimport org.apache.spark.sql.streaming.{OutputMode, Trigger}\nimport scala.concurrent.duration._\nval q = longs.\n  writeStream.\n  format(\"console\").\n  trigger(Trigger.ProcessingTime(10.seconds)).\n  outputMode(OutputMode.Update). // &lt;-- required for mapGroupsWithState\n  start\n\n// Note GroupState\n\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n&gt;&gt;&gt; key: 0 =&gt; state: GroupState(&lt;undefined&gt;)\n&gt;&gt;&gt; key: 1 =&gt; state: GroupState(&lt;undefined&gt;)\n+-----+\n|value|\n+-----+\n|    0|\n|    1|\n+-----+\n\n-------------------------------------------\nBatch: 2\n-------------------------------------------\n&gt;&gt;&gt; key: 0 =&gt; state: GroupState(0)\n&gt;&gt;&gt; key: 1 =&gt; state: GroupState(0)\n+-----+\n|value|\n+-----+\n|    0|\n|    1|\n+-----+\n\n-------------------------------------------\nBatch: 3\n-------------------------------------------\n&gt;&gt;&gt; key: 0 =&gt; state: GroupState(4)\n&gt;&gt;&gt; key: 1 =&gt; state: GroupState(4)\n+-----+\n|value|\n+-----+\n|    0|\n|    1|\n+-----+\n\n// in the end\nspark.streams.active.foreach(_.stop)\n</code></pre>","location":"spark-sql-streaming-KeyValueGroupedDataset-mapGroupsWithState/#source-scala"},{"title":"Extending Structured Streaming with New Data Sources","text":"<p>Spark Structured Streaming uses Spark SQL for planning streaming queries (preparing for execution).</p> <p>Structured Streaming supports two stream execution engines (i.e. Micro-Batch and Continuous) with their own APIs.</p> <p>Micro-Batch Stream Processing supports the old Data Source API V1 and the new modern Data Source API V2 with micro-batch-specific APIs for streaming sources and sinks.</p> <p>Continuous Stream Processing supports the new modern Data Source API V2 only with continuous-specific APIs for streaming sources and sinks.</p> <p>The following are the questions to think of (and answer) while considering development of a new data source for Structured Streaming. They are supposed to give you a sense of how much work and time it takes as well as what Spark version to support (e.g. 2.2 vs 2.4).</p> <ul> <li>Data Source API V1</li> <li>Data Source API V2</li> <li>Micro-Batch Stream Processing</li> <li>Continuous Stream Processing</li> </ul>","location":"spark-sql-streaming-extending-new-data-sources/"},{"title":"Batch Processing Time","text":"<p>Batch Processing Time (aka Batch Timeout Threshold) is the processing time (processing timestamp) of the current streaming batch.</p> <p>The following standard functions (and their Catalyst expressions) allow accessing the batch processing time in Micro-Batch Stream Processing:</p> <ul> <li> <p><code>now</code>, <code>current_timestamp</code>, and <code>unix_timestamp</code> functions (<code>CurrentTimestamp</code>)</p> </li> <li> <p><code>current_date</code> function (<code>CurrentDate</code>)</p> </li> </ul>  <p>Note</p> <p><code>CurrentTimestamp</code> or <code>CurrentDate</code> expressions are not supported in Continuous Stream Processing.</p>","location":"spark-structured-streaming-batch-processing-time/"},{"title":"Internals","text":"<p>GroupStateImpl is given the batch processing time when created for a streaming query (that is actually the batch processing time of the FlatMapGroupsWithStateExec physical operator).</p> <p>When created, <code>FlatMapGroupsWithStateExec</code> physical operator has the processing time undefined and set to the current timestamp in the state preparation rule every streaming batch.</p> <p>The current timestamp (and other batch-specific configurations) is given as the OffsetSeqMetadata (as part of the query planning phase) when a stream execution engine does the following:</p> <ul> <li> <p><code>MicroBatchExecution</code> is requested to construct a next streaming micro-batch in Micro-Batch Stream Processing</p> </li> <li> <p>In Continuous Stream Processing the base <code>StreamExecution</code> is requested to run stream processing and initializes <code>OffsetSeqMetadata</code> to <code>0</code>s.</p> </li> </ul>","location":"spark-structured-streaming-batch-processing-time/#internals"},{"title":"Internals of Streaming Queries","text":"<p>The page is to keep notes about how to guide readers through the codebase and may disappear if merged with the other pages or become an intro page.</p> <ul> <li>DataStreamReader and Streaming Data Source</li> <li>Data Source Resolution, Streaming Dataset and Logical Query Plan</li> <li>Dataset API \u2014 High-Level DSL to Build Logical Query Plan</li> <li>DataStreamWriter and Streaming Data Sink</li> <li>StreamingQuery</li> <li>StreamingQueryManager</li> </ul>","location":"spark-structured-streaming-internals/"},{"title":"DataStreamReader and Streaming Data Source <p>It all starts with <code>SparkSession.readStream</code> method which lets you define a streaming source in a stream processing pipeline (streaming processing graph or dataflow graph).</p> <pre><code>import org.apache.spark.sql.SparkSession\nassert(spark.isInstanceOf[SparkSession])\n\nval reader = spark.readStream\n\nimport org.apache.spark.sql.streaming.DataStreamReader\nassert(reader.isInstanceOf[DataStreamReader])\n</code></pre> <p><code>SparkSession.readStream</code> method creates a DataStreamReader.</p> <p>The fluent API of <code>DataStreamReader</code> allows you to describe the input data source (e.g. DataStreamReader.format and DataStreamReader.options) using method chaining (with the goal of making the readability of the source code close to that of ordinary written prose, essentially creating a domain-specific language within the interface. See Fluent interface article in Wikipedia).</p> <pre><code>reader\n  .format(\"csv\")\n  .option(\"delimiter\", \"|\")\n</code></pre> <p>There are a couple of built-in data source formats. Their names are the names of the corresponding <code>DataStreamReader</code> methods and so act like shortcuts of <code>DataStreamReader.format</code> (where you have to specify the format by name), i.e. csv, json, orc, parquet and text, followed by DataStreamReader.load.</p> <p>You may also want to use DataStreamReader.schema method to specify the schema of the streaming data source.</p> <pre><code>reader.schema(\"a INT, b STRING\")\n</code></pre> <p>In the end, you use DataStreamReader.load method that simply creates a streaming Dataset (the good ol' Dataset that you may have already used in Spark SQL).</p> <pre><code>val input = reader\n  .format(\"csv\")\n  .option(\"delimiter\", \"\\t\")\n  .schema(\"word STRING, num INT\")\n  .load(\"data/streaming\")\n\nimport org.apache.spark.sql.DataFrame\nassert(input.isInstanceOf[DataFrame])\n</code></pre> <p>The Dataset has the <code>isStreaming</code> property enabled that is basically the only way you could distinguish streaming Datasets from regular, batch Datasets.</p> <pre><code>assert(input.isStreaming)\n</code></pre> <p>In other words, Spark Structured Streaming is designed to extend the features of Spark SQL and let your structured queries be streaming queries.</p>","text":"","location":"spark-structured-streaming-internals/#datastreamreader-and-streaming-data-source"},{"title":"Data Source Resolution, Streaming Dataset and Logical Query Plan <p>Whenever you create a Dataset (be it batch in Spark SQL or streaming in Spark Structured Streaming) is when you create a logical query plan using the High-Level Dataset DSL.</p> <p>A logical query plan is made up of logical operators.</p> <p>Spark Structured Streaming gives you two logical operators to represent streaming sources (StreamingRelationV2 and StreamingRelation).</p> <p>When DataStreamReader.load method is executed, <code>load</code> first looks up the requested data source (that you specified using DataStreamReader.format) and creates an instance of it (instantiation). That'd be data source resolution step (that I described in...FIXME).</p> <p><code>DataStreamReader.load</code> is where you can find the intersection of the former Micro-Batch Stream Processing V1 API with the new Continuous Stream Processing V2 API.</p>","text":"","location":"spark-structured-streaming-internals/#data-source-resolution-streaming-dataset-and-logical-query-plan"},{"title":"V2 Code Path","text":"<p>For MicroBatchStream or ContinuousReadSupport data sources, <code>DataStreamReader.load</code> creates a logical query plan with a StreamingRelationV2 leaf logical operator. That is the new V2 code path.</p> <pre><code>// rate data source is V2\nval rates = spark.readStream.format(\"rate\").load\nval plan = rates.queryExecution.logical\nscala&gt; println(plan.numberedTreeString)\n00 StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2ed03b1a, rate, [timestamp#12, value#13L]\n</code></pre>","location":"spark-structured-streaming-internals/#v2-code-path"},{"title":"V1 Code Path","text":"<p>For all other types of streaming data sources, <code>DataStreamReader.load</code> creates a logical query plan with a StreamingRelation leaf logical operator. That is the former V1 code path.</p> <pre><code>// text data source is V1\nval texts = spark.readStream.format(\"text\").load(\"data/streaming\")\nval plan = texts.queryExecution.logical\nscala&gt; println(plan.numberedTreeString)\n00 StreamingRelation DataSource(org.apache.spark.sql.SparkSession@35edd886,text,List(),None,List(),None,Map(path -&gt; data/streaming),None), FileSource[data/streaming], [value#18]\n</code></pre>","location":"spark-structured-streaming-internals/#v1-code-path"},{"title":"Dataset API \u2014 High-Level DSL to Build Logical Query Plan <p>With a streaming Dataset created, you can now use all the methods of <code>Dataset</code> API, including but not limited to the following operators:</p> <ul> <li> <p>Dataset.dropDuplicates for streaming deduplication</p> </li> <li> <p>Dataset.groupBy and Dataset.groupByKey for streaming aggregation</p> </li> <li> <p>Dataset.withWatermark for event time watermark</p> </li> </ul> <p>Please note that a streaming Dataset is a regular Dataset (with some streaming-related limitations).</p> <pre><code>val rates = spark\n  .readStream\n  .format(\"rate\")\n  .load\nval countByTime = rates\n  .withWatermark(\"timestamp\", \"10 seconds\")\n  .groupBy($\"timestamp\")\n  .agg(count(\"*\") as \"count\")\n\nimport org.apache.spark.sql.Dataset\nassert(countByTime.isInstanceOf[Dataset[_]])\n</code></pre> <p>The point is to understand that the Dataset API is a domain-specific language (DSL) to build a more sophisticated stream processing pipeline that you could also build using the low-level logical operators directly.</p> <p>Use Dataset.explain to learn the underlying logical and physical query plans.</p> <pre><code>assert(countByTime.isStreaming)\n\nscala&gt; countByTime.explain(extended = true)\n== Parsed Logical Plan ==\n'Aggregate ['timestamp], [unresolvedalias('timestamp, None), count(1) AS count#131L]\n+- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds\n   +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2fcb3082, rate, [timestamp#88, value#89L]\n\n== Analyzed Logical Plan ==\ntimestamp: timestamp, count: bigint\nAggregate [timestamp#88-T10000ms], [timestamp#88-T10000ms, count(1) AS count#131L]\n+- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds\n   +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2fcb3082, rate, [timestamp#88, value#89L]\n\n== Optimized Logical Plan ==\nAggregate [timestamp#88-T10000ms], [timestamp#88-T10000ms, count(1) AS count#131L]\n+- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds\n   +- Project [timestamp#88]\n      +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2fcb3082, rate, [timestamp#88, value#89L]\n\n== Physical Plan ==\n*(5) HashAggregate(keys=[timestamp#88-T10000ms], functions=[count(1)], output=[timestamp#88-T10000ms, count#131L])\n+- StateStoreSave [timestamp#88-T10000ms], state info [ checkpoint = &lt;unknown&gt;, runId = 28606ba5-9c7f-4f1f-ae41-e28d75c4d948, opId = 0, ver = 0, numPartitions = 200], Append, 0, 2\n   +- *(4) HashAggregate(keys=[timestamp#88-T10000ms], functions=[merge_count(1)], output=[timestamp#88-T10000ms, count#136L])\n      +- StateStoreRestore [timestamp#88-T10000ms], state info [ checkpoint = &lt;unknown&gt;, runId = 28606ba5-9c7f-4f1f-ae41-e28d75c4d948, opId = 0, ver = 0, numPartitions = 200], 2\n         +- *(3) HashAggregate(keys=[timestamp#88-T10000ms], functions=[merge_count(1)], output=[timestamp#88-T10000ms, count#136L])\n            +- Exchange hashpartitioning(timestamp#88-T10000ms, 200)\n               +- *(2) HashAggregate(keys=[timestamp#88-T10000ms], functions=[partial_count(1)], output=[timestamp#88-T10000ms, count#136L])\n                  +- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds\n                     +- *(1) Project [timestamp#88]\n                        +- StreamingRelation rate, [timestamp#88, value#89L]\n</code></pre> <p>Or go pro and talk to <code>QueryExecution</code> directly.</p> <pre><code>val plan = countByTime.queryExecution.logical\nscala&gt; println(plan.numberedTreeString)\n00 'Aggregate ['timestamp], [unresolvedalias('timestamp, None), count(1) AS count#131L]\n01 +- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds\n02    +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2fcb3082, rate, [timestamp#88, value#89L]\n</code></pre> <p>Please note that most of the stream processing operators you may also have used in batch structured queries in Spark SQL. Again, the distinction between Spark SQL and Spark Structured Streaming is very thin from a developer's point of view.</p>","text":"","location":"spark-structured-streaming-internals/#dataset-api-high-level-dsl-to-build-logical-query-plan"},{"title":"DataStreamWriter and Streaming Data Sink <p>Once you're satisfied with building a stream processing pipeline (using the APIs of DataStreamReader, Dataset, <code>RelationalGroupedDataset</code> and <code>KeyValueGroupedDataset</code>), you should define how and when the result of the streaming query is persisted in (sent out to) an external data system using a streaming sink.</p> <p>You should use Dataset.writeStream method that simply creates a DataStreamWriter.</p> <pre><code>// Not only is this a Dataset, but it is also streaming\nassert(countByTime.isStreaming)\n\nval writer = countByTime.writeStream\n\nimport org.apache.spark.sql.streaming.DataStreamWriter\nassert(writer.isInstanceOf[DataStreamWriter[_]])\n</code></pre> <p>The fluent API of <code>DataStreamWriter</code> allows you to describe the output data sink (DataStreamWriter.format and DataStreamWriter.options) using method chaining (with the goal of making the readability of the source code close to that of ordinary written prose, essentially creating a domain-specific language within the interface. See Fluent interface article in Wikipedia).</p> <pre><code>writer\n  .format(\"csv\")\n  .option(\"delimiter\", \"\\t\")\n</code></pre> <p>Like in DataStreamReader data source formats, there are a couple of built-in data sink formats. Unlike data source formats, their names do not have corresponding <code>DataStreamWriter</code> methods. The reason is that you will use DataStreamWriter.start to create and immediately start a StreamingQuery.</p> <p>There are however two special output formats that do have corresponding <code>DataStreamWriter</code> methods, i.e. DataStreamWriter.foreach and DataStreamWriter.foreachBatch, that allow for persisting query results to external data systems that do not have streaming sinks available. They give you a trade-off between developing a full-blown streaming sink and simply using the methods (that lay the basis of what a custom sink would have to do anyway).</p> <p><code>DataStreamWriter</code> API defines two new concepts (that are not available in the \"base\" Spark SQL):</p> <ul> <li> <p>OutputMode that you specify using DataStreamWriter.outputMode method</p> </li> <li> <p>Trigger that you specify using DataStreamWriter.trigger method</p> </li> </ul> <p>You may also want to give a streaming query a name using DataStreamWriter.queryName method.</p> <p>In the end, you use DataStreamWriter.start method to create and immediately start a StreamingQuery.</p> <pre><code>import org.apache.spark.sql.streaming.OutputMode\nimport org.apache.spark.sql.streaming.Trigger\nimport scala.concurrent.duration._\nval sq = writer\n  .format(\"console\")\n  .option(\"truncate\", false)\n  .option(\"checkpointLocation\", \"/tmp/csv-to-csv-checkpoint\")\n  .outputMode(OutputMode.Append)\n  .trigger(Trigger.ProcessingTime(30.seconds))\n  .queryName(\"csv-to-csv\")\n  .start(\"/tmp\")\n\nimport org.apache.spark.sql.streaming.StreamingQuery\nassert(sq.isInstanceOf[StreamingQuery])\n</code></pre> <p>When <code>DataStreamWriter</code> is requested to start a streaming query, it allows for the following data source formats:</p> <ul> <li> <p>memory with MemorySinkV2 (with ContinuousTrigger) or MemorySink</p> </li> <li> <p>foreach with ForeachWriterProvider sink</p> </li> <li> <p>foreachBatch with ForeachBatchSink sink (that does not support ContinuousTrigger)</p> </li> <li> <p>Any <code>DataSourceRegister</code> data source</p> </li> <li> <p>Custom data sources specified by their fully-qualified class names or [name].DefaultSource</p> </li> <li> <p>avro, kafka and some others (see <code>DataSource.lookupDataSource</code> object method)</p> </li> <li> <p><code>DataSource</code> is requested to create a streaming sink that accepts StreamSinkProvider or <code>FileFormat</code> data sources only</p> </li> </ul> <p>With a streaming sink, <code>DataStreamWriter</code> requests the StreamingQueryManager to start a streaming query.</p>","text":"","location":"spark-structured-streaming-internals/#datastreamwriter-and-streaming-data-sink"},{"title":"StreamingQuery <p>When a stream processing pipeline is started (using DataStreamWriter.start method), <code>DataStreamWriter</code> creates a StreamingQuery and requests the StreamingQueryManager to start a streaming query.</p>","text":"","location":"spark-structured-streaming-internals/#streamingquery"},{"title":"StreamingQueryManager <p>StreamingQueryManager is used to manage streaming queries.</p>","text":"","location":"spark-structured-streaming-internals/#streamingquerymanager"},{"title":"Arbitrary Stateful Streaming Aggregation","text":"<p>Arbitrary Stateful Streaming Aggregation is a streaming aggregation query that uses the following high-level operators of <code>KeyValueGroupedDataset</code> (Spark SQL):</p> <ul> <li> <p>mapGroupsWithState for implicit state logic</p> </li> <li> <p>flatMapGroupsWithState for explicit state logic</p> </li> </ul> <p><code>KeyValueGroupedDataset</code> represents a grouped dataset as a result of Dataset.groupByKey operator.</p> <p><code>mapGroupsWithState</code> and <code>flatMapGroupsWithState</code> operators use GroupState as group streaming aggregation state that is created separately for every aggregation key with an aggregation state value (of a user-defined type).</p> <p><code>mapGroupsWithState</code> and <code>flatMapGroupsWithState</code> operators use GroupStateTimeout as an aggregation state timeout that defines when a GroupState is considered timed-out (expired).</p>","location":"arbitrary-stateful-streaming-aggregation/"},{"title":"Demos","text":"<p>Use the following demos and complete applications to learn more:</p> <ul> <li> <p>Demo: Internals of FlatMapGroupsWithStateExec Physical Operator</p> </li> <li> <p>Demo: Arbitrary Stateful Streaming Aggregation with KeyValueGroupedDataset.flatMapGroupsWithState Operator</p> </li> <li> <p>groupByKey Streaming Aggregation in Update Mode</p> </li> <li> <p>FlatMapGroupsWithStateApp</p> </li> </ul>","location":"arbitrary-stateful-streaming-aggregation/#demos"},{"title":"Performance Metrics <p>Arbitrary Stateful Streaming Aggregation uses performance metrics (of the StateStoreWriter through FlatMapGroupsWithStateExec physical operator).</p>","text":"","location":"arbitrary-stateful-streaming-aggregation/#performance-metrics"},{"title":"Internals <p>One of the most important internal execution components of Arbitrary Stateful Streaming Aggregation is FlatMapGroupsWithStateExec physical operator.</p> <p>When executed, <code>FlatMapGroupsWithStateExec</code> first validates a selected GroupStateTimeout:</p> <ul> <li> <p>For ProcessingTimeTimeout, batch timeout threshold has to be defined</p> </li> <li> <p>For EventTimeTimeout, event-time watermark has to be defined and the input schema has the watermark attribute</p> </li> </ul>  <p>Note</p> <p>FIXME When are the above requirements met?</p>  <p><code>FlatMapGroupsWithStateExec</code> physical operator then mapPartitionsWithStateStore with a custom <code>storeUpdateFunction</code> of the following signature:</p> <pre><code>(StateStore, Iterator[T]) =&gt; Iterator[U]\n</code></pre> <p>While generating the recipe, <code>FlatMapGroupsWithStateExec</code> uses StateStoreOps extension method object to register a listener that is executed on a task completion. The listener makes sure that a given StateStore has all state changes either committed or aborted.</p> <p>In the end, <code>FlatMapGroupsWithStateExec</code> creates a new StateStoreRDD and adds it to the RDD lineage.</p> <p><code>StateStoreRDD</code> is used to properly distribute tasks across executors (per preferred locations) with help of StateStoreCoordinator (that runs on the driver).</p> <p><code>StateStoreRDD</code> uses <code>StateStore</code> helper to look up a StateStore by StateStoreProviderId and store version.</p> <p><code>FlatMapGroupsWithStateExec</code> physical operator uses state managers that are different than state managers for Streaming Aggregation. StateStore abstraction is the same as in Streaming Aggregation.</p> <p>One of the important execution steps is when <code>InputProcessor</code> (of FlatMapGroupsWithStateExec physical operator) is requested to callFunctionAndUpdateState. That executes the user-defined state function on a per-group state key object, value objects, and a GroupStateImpl.</p>","text":"","location":"arbitrary-stateful-streaming-aggregation/#internals"},{"title":"FlatMapGroupsWithStateExecHelper","text":"<p><code>FlatMapGroupsWithStateExecHelper</code> utility is mainly used to creating a StateManager for FlatMapGroupsWithStateExec physical operator.</p> <p>=== [[createStateManager]] Creating StateManager</p>","location":"arbitrary-stateful-streaming-aggregation/FlatMapGroupsWithStateExecHelper/"},{"title":"[source, scala]","text":"<p>createStateManager(   stateEncoder: ExpressionEncoder[Any],   shouldStoreTimestamp: Boolean,   stateFormatVersion: Int): StateManager</p>  <p><code>createStateManager</code> simply creates a &lt;&gt; (with the <code>stateEncoder</code> and <code>shouldStoreTimestamp</code> flag) based on <code>stateFormatVersion</code>: <ul> <li> <p>&lt;&gt; for <code>1</code>  <li> <p>&lt;&gt; for <code>2</code>   <p><code>createStateManager</code> throws an <code>IllegalArgumentException</code> for <code>stateFormatVersion</code> not <code>1</code> or <code>2</code>:</p> <pre><code>Version [stateFormatVersion] is invalid\n</code></pre> <p><code>createStateManager</code> is used for the StateManager for FlatMapGroupsWithStateExec physical operator.</p>","location":"arbitrary-stateful-streaming-aggregation/FlatMapGroupsWithStateExecHelper/#source-scala"},{"title":"InputProcessor","text":"<p><code>InputProcessor</code> is a helper class that is used to update state (in the state store) of a single partition of a FlatMapGroupsWithStateExec physical operator.</p>","location":"arbitrary-stateful-streaming-aggregation/InputProcessor/"},{"title":"Creating Instance","text":"<p><code>InputProcessor</code> takes the following to be created:</p> <ul> <li>StateStore</li> </ul> <p><code>InputProcessor</code> is created when FlatMapGroupsWithStateExec physical operator is executed (for storeUpdateFunction while processing rows per partition with a corresponding per-partition state store).</p>","location":"arbitrary-stateful-streaming-aggregation/InputProcessor/#creating-instance"},{"title":"StateStore <p><code>InputProcessor</code> is given a StateStore when created.</p> <p>The <code>StateStore</code> manages the per-group state (and is used when processing new data and timed-out state data, and in the \"all rows processed\" callback).</p>","text":"","location":"arbitrary-stateful-streaming-aggregation/InputProcessor/#statestore"},{"title":"Processing New Data <pre><code>processNewData(\n  dataIter: Iterator[InternalRow]): Iterator[InternalRow]\n</code></pre> <p><code>processNewData</code> creates a grouped iterator of (of pairs of) per-group state keys and the row values from the given data iterator (<code>dataIter</code>) with the grouping attributes and the output schema of the child operator (of the parent <code>FlatMapGroupsWithStateExec</code> physical operator).</p> <p>For every per-group state key (in the grouped iterator), <code>processNewData</code> requests the StateManager (of the parent <code>FlatMapGroupsWithStateExec</code> physical operator) to get the state (from the StateStore) and callFunctionAndUpdateState (with the <code>hasTimedOut</code> flag off).</p> <p><code>processNewData</code> is used when FlatMapGroupsWithStateExec physical operator is executed.</p>","text":"","location":"arbitrary-stateful-streaming-aggregation/InputProcessor/#processing-new-data"},{"title":"Processing Timed-Out State Data <pre><code>processTimedOutState(): Iterator[InternalRow]\n</code></pre> <p><code>processTimedOutState</code> does nothing and simply returns an empty iterator for GroupStateTimeout.NoTimeout.</p> <p>With timeout enabled, <code>processTimedOutState</code> gets the current timeout threshold per GroupStateTimeout:</p> <ul> <li> <p>batchTimestampMs for ProcessingTimeTimeout</p> </li> <li> <p>eventTimeWatermark for EventTimeTimeout</p> </li> </ul> <p><code>processTimedOutState</code> creates an iterator of timed-out state data by requesting the StateManager for all the available state data (in the StateStore) and takes only the state data with timeout defined and below the current timeout threshold.</p> <p>In the end, for every timed-out state data, <code>processTimedOutState</code> callFunctionAndUpdateState (with the <code>hasTimedOut</code> flag on).</p> <p><code>processTimedOutState</code> is used when FlatMapGroupsWithStateExec physical operator is executed.</p>","text":"","location":"arbitrary-stateful-streaming-aggregation/InputProcessor/#processing-timed-out-state-data"},{"title":"callFunctionAndUpdateState Internal Method <pre><code>callFunctionAndUpdateState(\n  stateData: StateData,\n  valueRowIter: Iterator[InternalRow],\n  hasTimedOut: Boolean): Iterator[InternalRow]\n</code></pre> <p><code>callFunctionAndUpdateState</code> is used when <code>InputProcessor</code> is requested to process new data and timed-out state data with the given <code>hasTimedOut</code> flag is off and on, respectively.</p> <p><code>callFunctionAndUpdateState</code> creates a key object by requesting the given <code>StateData</code> for the <code>UnsafeRow</code> of the key (keyRow) and converts it to an object (using the internal state key converter).</p> <p><code>callFunctionAndUpdateState</code> creates value objects by taking every value row (from the given <code>valueRowIter</code> iterator) and converts them to objects (using the internal state value converter).</p> <p><code>callFunctionAndUpdateState</code> creates a new GroupStateImpl with the following:</p> <ul> <li> <p>The current state value (of the given <code>StateData</code>) that could possibly be <code>null</code></p> </li> <li> <p>The batchTimestampMs of the parent <code>FlatMapGroupsWithStateExec</code> operator (that could possibly be -1)</p> </li> <li> <p>The event-time watermark of the parent <code>FlatMapGroupsWithStateExec</code> operator (that could possibly be -1)</p> </li> <li> <p>The GroupStateTimeout of the parent <code>FlatMapGroupsWithStateExec</code> operator</p> </li> <li> <p>The watermarkPresent flag of the parent <code>FlatMapGroupsWithStateExec</code> operator</p> </li> <li> <p>The given <code>hasTimedOut</code> flag</p> </li> </ul> <p><code>callFunctionAndUpdateState</code> then executes the user-defined state function (of the parent <code>FlatMapGroupsWithStateExec</code> operator) on the key object, value objects, and the newly-created <code>GroupStateImpl</code>.</p> <p>For every output value from the user-defined state function, <code>callFunctionAndUpdateState</code> updates numOutputRows performance metric and wraps the values to an internal row (using the internal output value converter).</p> <p>In the end, <code>callFunctionAndUpdateState</code> returns a <code>Iterator[InternalRow]</code> which calls the completion function right after rows have been processed (so the iterator is considered fully consumed).</p>","text":"","location":"arbitrary-stateful-streaming-aggregation/InputProcessor/#callfunctionandupdatestate-internal-method"},{"title":"\"All Rows Processed\" Callback <pre><code>onIteratorCompletion: Unit\n</code></pre> <p><code>onIteratorCompletion</code> branches off per whether the <code>GroupStateImpl</code> has been marked removed and no timeout timestamp is specified or not.</p> <p>When the <code>GroupStateImpl</code> has been marked removed and no timeout timestamp is specified, <code>onIteratorCompletion</code> does the following:</p> <p>. Requests the StateManager (of the parent <code>FlatMapGroupsWithStateExec</code> operator) to remove the state (from the StateStore for the key row of the given <code>StateData</code>)</p> <p>. Increments the numUpdatedStateRows performance metric</p> <p>Otherwise, when the <code>GroupStateImpl</code> has not been marked removed or the timeout timestamp is specified, <code>onIteratorCompletion</code> checks whether the timeout timestamp has changed by comparing the timeout timestamps of the GroupStateImpl and the given <code>StateData</code>.</p> <p>(only when the <code>GroupStateImpl</code> has been updated, removed or the timeout timestamp changed) <code>onIteratorCompletion</code> does the following:</p> <p>. Requests the StateManager (of the parent <code>FlatMapGroupsWithStateExec</code> operator) to persist the state (in the StateStore with the key row, updated state object, and the timeout timestamp of the given <code>StateData</code>)</p> <p>. Increments the numUpdatedStateRows performance metrics</p> <p><code>onIteratorCompletion</code> is used when <code>InputProcessor</code> is requested to callFunctionAndUpdateState (right after rows have been processed)</p>","text":"","location":"arbitrary-stateful-streaming-aggregation/InputProcessor/#all-rows-processed-callback"},{"title":"Converters","text":"","location":"arbitrary-stateful-streaming-aggregation/InputProcessor/#converters"},{"title":"Output Value Converter <p>An output value converter (of type <code>Any =&gt; InternalRow</code>) to wrap a given output value (from the user-defined state function) to a row</p> <ul> <li>The data type of the row is specified as the data type of the output object attribute when the parent <code>FlatMapGroupsWithStateExec</code> operator is created</li> </ul> <p>Used when <code>InputProcessor</code> is requested to callFunctionAndUpdateState.</p>","text":"","location":"arbitrary-stateful-streaming-aggregation/InputProcessor/#output-value-converter"},{"title":"State Key Converter <p>A state key converter (of type <code>InternalRow =&gt; Any</code>) to deserialize a given row (for a per-group state key) to the current state value</p> <ul> <li> <p>The deserialization expression for keys is specified as the key deserializer expression when the parent <code>FlatMapGroupsWithStateExec</code> operator is created</p> </li> <li> <p>The data type of state keys is specified as the grouping attributes when the parent <code>FlatMapGroupsWithStateExec</code> operator is created</p> </li> </ul> <p>Used when <code>InputProcessor</code> is requested to callFunctionAndUpdateState.</p>","text":"","location":"arbitrary-stateful-streaming-aggregation/InputProcessor/#state-key-converter"},{"title":"State Value Converter <p>A state value converter (of type <code>InternalRow =&gt; Any</code>) to deserialize a given row (for a per-group state value) to a Scala value</p> <ul> <li> <p>The deserialization expression for values is specified as the value deserializer expression when the parent <code>FlatMapGroupsWithStateExec</code> operator is created</p> </li> <li> <p>The data type of state values is specified as the data attributes when the parent <code>FlatMapGroupsWithStateExec</code> operator is created</p> </li> </ul> <p>Used when <code>InputProcessor</code> is requested to callFunctionAndUpdateState.</p>","text":"","location":"arbitrary-stateful-streaming-aggregation/InputProcessor/#state-value-converter"},{"title":"StateManager","text":"<p><code>StateManager</code> is the &lt;&gt; of &lt;&gt; that act as middlemen between state stores and the FlatMapGroupsWithStateExec physical operator used in Arbitrary Stateful Streaming Aggregation. <p>[[contract]] .StateManager Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| getAllState a| [[getAllState]]</p>","location":"arbitrary-stateful-streaming-aggregation/StateManager/"},{"title":"[source, scala]","text":"","location":"arbitrary-stateful-streaming-aggregation/StateManager/#source-scala"},{"title":"getAllState(store: StateStore): Iterator[StateData]","text":"<p>Retrieves all state data (for all keys) from the StateStore</p> <p>Used when <code>InputProcessor</code> is requested to processTimedOutState</p> <p>| getState a| [[getState]]</p>","location":"arbitrary-stateful-streaming-aggregation/StateManager/#getallstatestore-statestore-iteratorstatedata"},{"title":"[source, scala]","text":"<p>getState(   store: StateStore,   keyRow: UnsafeRow): StateData</p>  <p>Gets the state data for the key from the StateStore</p> <p>Used exclusively when <code>InputProcessor</code> is requested to processNewData</p> <p>| putState a| [[putState]]</p>","location":"arbitrary-stateful-streaming-aggregation/StateManager/#source-scala_1"},{"title":"[source, scala]","text":"<p>putState(   store: StateStore,   keyRow: UnsafeRow,   state: Any,   timeoutTimestamp: Long): Unit</p>  <p>Persists (puts) the state value for the key in the StateStore</p> <p>Used exclusively when <code>InputProcessor</code> is requested to callFunctionAndUpdateState (right after all rows have been processed)</p> <p>| removeState a| [[removeState]]</p>","location":"arbitrary-stateful-streaming-aggregation/StateManager/#source-scala_2"},{"title":"[source, scala]","text":"<p>removeState(   store: StateStore,   keyRow: UnsafeRow): Unit</p>  <p>Removes the state for the key from the StateStore</p> <p>Used exclusively when <code>InputProcessor</code> is requested to callFunctionAndUpdateState (right after all rows have been processed)</p> <p>| stateSchema a| [[stateSchema]]</p>","location":"arbitrary-stateful-streaming-aggregation/StateManager/#source-scala_3"},{"title":"[source, scala]","text":"","location":"arbitrary-stateful-streaming-aggregation/StateManager/#source-scala_4"},{"title":"stateSchema: StructType","text":"<p>State schema</p>  <p>Note</p> <p>It looks like (in StateManager of the FlatMapGroupsWithStateExec physical operator) <code>stateSchema</code> is used for the schema of state value objects (not state keys as they are described by the grouping attributes instead).</p>  <p>Used when:</p> <ul> <li> <p>FlatMapGroupsWithStateExec physical operator is executed</p> </li> <li> <p><code>StateManagerImplBase</code> is requested for the stateDeserializerFunc</p> </li> </ul> <p>|===</p> <p>[[implementations]] NOTE: &lt;&gt; is the one and only known direct implementation of the &lt;&gt; in Spark Structured Streaming. <p>NOTE: <code>StateManager</code> is a Scala sealed trait which means that all the &lt;&gt; are in the same compilation unit (a single file).","location":"arbitrary-stateful-streaming-aggregation/StateManager/#stateschema-structtype"},{"title":"StateManagerImplBase","text":"<p><code>StateManagerImplBase</code> is the &lt;&gt; of the &lt;&gt; for &lt;&gt; of FlatMapGroupsWithStateExec physical operator with the following features: <ul> <li> <p>Use Catalyst expressions for &lt;&gt; and &lt;&gt;  <li> <p>Use &lt;&gt; when &lt;&gt; with the &lt;&gt; flag on   <p>[[contract]] .StateManagerImplBase Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| stateDeserializerExpr a| [[stateDeserializerExpr]]</p>","location":"arbitrary-stateful-streaming-aggregation/StateManagerImplBase/"},{"title":"[source, scala]","text":"","location":"arbitrary-stateful-streaming-aggregation/StateManagerImplBase/#source-scala"},{"title":"stateDeserializerExpr: Expression","text":"<p>State deserializer, i.e. a Catalyst expression to deserialize a state object from a row (<code>UnsafeRow</code>)</p> <p>Used exclusively for the &lt;&gt; <p>| stateSerializerExprs a| [[stateSerializerExprs]]</p>","location":"arbitrary-stateful-streaming-aggregation/StateManagerImplBase/#statedeserializerexpr-expression"},{"title":"[source, scala]","text":"","location":"arbitrary-stateful-streaming-aggregation/StateManagerImplBase/#source-scala_1"},{"title":"stateSerializerExprs: Seq[Expression]","text":"<p>State serializer, i.e. Catalyst expressions to serialize a state object to a row (<code>UnsafeRow</code>)</p> <p>Used exclusively for the &lt;&gt; <p>| timeoutTimestampOrdinalInRow a| [[timeoutTimestampOrdinalInRow]]</p>","location":"arbitrary-stateful-streaming-aggregation/StateManagerImplBase/#stateserializerexprs-seqexpression"},{"title":"[source, scala]","text":"","location":"arbitrary-stateful-streaming-aggregation/StateManagerImplBase/#source-scala_2"},{"title":"timeoutTimestampOrdinalInRow: Int","text":"<p>Position of the timeout timestamp in a state row</p> <p>Used when <code>StateManagerImplBase</code> is requested to &lt;&gt; and &lt;&gt; <p>|===</p> <p>[[implementations]] .StateManagerImplBases [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | StateManagerImplBase | Description</p> <p>| &lt;&gt; | [[StateManagerImplV1]] Legacy &lt;&gt; <p>| &lt;&gt; | [[StateManagerImplV2]] Default &lt;&gt; <p>|===</p> <p>=== [[creating-instance]][[shouldStoreTimestamp]] Creating StateManagerImplBase Instance</p> <p><code>StateManagerImplBase</code> takes a single <code>shouldStoreTimestamp</code> flag to be created (that is set when the &lt;&gt; are created). <p>NOTE: <code>StateManagerImplBase</code> is a Scala abstract class and cannot be &lt;&gt; directly. It is created indirectly for the &lt;&gt;. <p><code>StateManagerImplBase</code> initializes the &lt;&gt;. <p>=== [[getState]] Getting State Data for Key from StateStore -- <code>getState</code> Method</p>","location":"arbitrary-stateful-streaming-aggregation/StateManagerImplBase/#timeouttimestampordinalinrow-int"},{"title":"[source, scala]","text":"<p>getState(   store: StateStore,   keyRow: UnsafeRow): StateData</p>  <p><code>getState</code> is part of the StateManager abstraction.</p> <p><code>getState</code>...FIXME</p> <p>=== [[putState]] Persisting State Value for Key in StateStore -- <code>putState</code> Method</p>","location":"arbitrary-stateful-streaming-aggregation/StateManagerImplBase/#source-scala_3"},{"title":"[source, scala]","text":"<p>putState(   store: StateStore,   key: UnsafeRow,   state: Any,   timestamp: Long): Unit</p>  <p><code>putState</code> is part of the StateManager abstraction.</p> <p><code>putState</code>...FIXME</p> <p>=== [[removeState]] Removing State for Key from StateStore -- <code>removeState</code> Method</p>","location":"arbitrary-stateful-streaming-aggregation/StateManagerImplBase/#source-scala_4"},{"title":"[source, scala]","text":"<p>removeState(   store: StateStore,   keyRow: UnsafeRow): Unit</p>  <p><code>removeState</code> is part of the StateManager abstraction.</p> <p><code>removeState</code>...FIXME</p> <p>=== [[getAllState]] Getting All State Data (for All Keys) from StateStore -- <code>getAllState</code> Method</p>","location":"arbitrary-stateful-streaming-aggregation/StateManagerImplBase/#source-scala_5"},{"title":"[source, scala]","text":"","location":"arbitrary-stateful-streaming-aggregation/StateManagerImplBase/#source-scala_6"},{"title":"getAllState(store: StateStore): Iterator[StateData]","text":"<p><code>getAllState</code> is part of the StateManager abstraction.</p> <p><code>getAllState</code>...FIXME</p> <p>=== [[getStateObject]] <code>getStateObject</code> Internal Method</p>","location":"arbitrary-stateful-streaming-aggregation/StateManagerImplBase/#getallstatestore-statestore-iteratorstatedata"},{"title":"[source, scala]","text":"","location":"arbitrary-stateful-streaming-aggregation/StateManagerImplBase/#source-scala_7"},{"title":"getStateObject(row: UnsafeRow): Any","text":"<p><code>getStateObject</code>...FIXME</p> <p><code>getStateObject</code> is used when...FIXME</p> <p>=== [[getStateRow]] <code>getStateRow</code> Internal Method</p>","location":"arbitrary-stateful-streaming-aggregation/StateManagerImplBase/#getstateobjectrow-unsaferow-any"},{"title":"[source, scala]","text":"","location":"arbitrary-stateful-streaming-aggregation/StateManagerImplBase/#source-scala_8"},{"title":"getStateRow(obj: Any): UnsafeRow","text":"<p><code>getStateRow</code>...FIXME</p> <p><code>getStateRow</code> is used when...FIXME</p> <p>=== [[getTimestamp]] Getting Timeout Timestamp (from State Row) -- <code>getTimestamp</code> Internal Method</p>","location":"arbitrary-stateful-streaming-aggregation/StateManagerImplBase/#getstaterowobj-any-unsaferow"},{"title":"[source, scala]","text":"","location":"arbitrary-stateful-streaming-aggregation/StateManagerImplBase/#source-scala_9"},{"title":"getTimestamp(stateRow: UnsafeRow): Long","text":"<p><code>getTimestamp</code>...FIXME</p> <p><code>getTimestamp</code> is used when...FIXME</p> <p>=== [[setTimestamp]] Setting Timeout Timestamp (to State Row) -- <code>setTimestamp</code> Internal Method</p>","location":"arbitrary-stateful-streaming-aggregation/StateManagerImplBase/#gettimestampstaterow-unsaferow-long"},{"title":"[source, scala]","text":"<p>setTimestamp(   stateRow: UnsafeRow,   timeoutTimestamps: Long): Unit</p>  <p><code>setTimestamp</code>...FIXME</p> <p><code>setTimestamp</code> is used when...FIXME</p> <p>=== [[internal-properties]] Internal Properties</p> <p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| stateSerializerFunc a| [[stateSerializerFunc]] State object serializer (of type <code>Any =&gt; UnsafeRow</code>) to serialize a state object (for a per-group state key) to a row (<code>UnsafeRow</code>)</p> <ul> <li>The serialization expression (incl. the type) is specified as the &lt;&gt;  <p>Used exclusively in &lt;&gt; <p>| stateDeserializerFunc a| [[stateDeserializerFunc]] State object deserializer (of type <code>InternalRow =&gt; Any</code>) to deserialize a row (for a per-group state value) to a Scala value</p> <ul> <li>The deserialization expression (incl. the type) is specified as the &lt;&gt;  <p>Used exclusively in &lt;&gt; <p>| stateDataForGets a| [[stateDataForGets]] Empty <code>StateData</code> to share (reuse) between &lt;&gt; calls (to avoid high use of memory with many <code>StateData</code> objects) <p>|===</p>","location":"arbitrary-stateful-streaming-aggregation/StateManagerImplBase/#source-scala_10"},{"title":"StateManagerImplV1","text":"<p><code>StateManagerImplV1</code> is...FIXME</p>","location":"arbitrary-stateful-streaming-aggregation/StateManagerImplV1/"},{"title":"StateManagerImplV2","text":"<p><code>StateManagerImplV2</code> is a concrete StateManager (as a StateManagerImplBase) that is used by default in FlatMapGroupsWithStateExec physical operator (per spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion internal configuration property).</p>","location":"arbitrary-stateful-streaming-aggregation/StateManagerImplV2/"},{"title":"Creating Instance","text":"<p><code>StateManagerImplV2</code> takes the following to be created:</p> <ul> <li>[[stateEncoder]] State encoder (<code>ExpressionEncoder[Any]</code>)</li> <li>[[shouldStoreTimestamp]] <code>shouldStoreTimestamp</code> flag</li> </ul> <p><code>StateManagerImplV2</code> is created when:</p> <ul> <li><code>FlatMapGroupsWithStateExecHelper</code> utility is requested for a StateManager (when the <code>stateFormatVersion</code> is <code>2</code>)</li> </ul>","location":"arbitrary-stateful-streaming-aggregation/StateManagerImplV2/#creating-instance"},{"title":"Continuous Stream Processing","text":"<p>Continuous Stream Processing is a stream processing engine in Spark Structured Streaming used for execution of structured streaming queries with Trigger.Continuous trigger.</p> <p>Continuous Stream Processing execution engine uses the novel Data Source API V2 (Spark SQL) and for the very first time makes stream processing truly continuous (not micro-batch).</p>  <p>Tip</p> <p>Read up on Data Source API V2 in The Internals of Spark SQL online book.</p>  <p>Because of the two innovative changes Continuous Stream Processing is often referred to as Structured Streaming V2.</p> <p>Under the covers, Continuous Stream Processing uses ContinuousExecution stream execution engine. When requested to run an activated streaming query, <code>ContinuousExecution</code> adds WriteToContinuousDataSourceExec physical operator as the top-level operator in the physical query plan of the streaming query.</p> <pre><code>scala&gt; :type sq\norg.apache.spark.sql.streaming.StreamingQuery\n\nscala&gt; sq.explain\n== Physical Plan ==\nWriteToContinuousDataSource ConsoleWriter[numRows=20, truncate=false]\n+- *(1) Project [timestamp#758, value#759L]\n   +- *(1) ScanV2 rate[timestamp#758, value#759L]\n</code></pre> <p>From now on, you may think of a streaming query as a soon-to-be-generated ContinuousWriteRDD - an RDD data structure that Spark developers use to describe a distributed computation.</p> <p>When the streaming query is started (and the top-level <code>WriteToContinuousDataSourceExec</code> physical operator is requested to execute), it simply requests the underlying <code>ContinuousWriteRDD</code> to collect.</p> <p>That collect operator is how a Spark job is run (as tasks over all partitions of the RDD) as described by the ContinuousWriteRDD.compute \"protocol\" (a recipe for the tasks to be scheduled to run on Spark executors).</p> <p>.Creating Instance of StreamExecution image::images/webui-spark-job-streaming-query-started.png[align=\"center\"]</p> <p>While the tasks are computing partitions (of the <code>ContinuousWriteRDD</code>), they keep running until killed or completed. And that's the ingenious design trick of how the streaming query (as a Spark job with the distributed tasks running on executors) runs continuously and indefinitely.</p> <p>When <code>DataStreamReader</code> is requested to create a streaming query for a ContinuousReadSupport data source, it creates...FIXME</p>","location":"continuous-execution/"},{"title":"Demo","text":"<pre><code>import org.apache.spark.sql.streaming.Trigger\nimport scala.concurrent.duration._\nval sq = spark\n  .readStream\n  .format(\"rate\")\n  .load\n  .writeStream\n  .format(\"console\")\n  .option(\"truncate\", false)\n  .trigger(Trigger.Continuous(15.seconds)) // &lt;-- Uses ContinuousExecution for execution\n  .queryName(\"rate2console\")\n  .start\n\nscala&gt; :type sq\norg.apache.spark.sql.streaming.StreamingQuery\n\nassert(sq.isActive)\n\n// sq.stop\n</code></pre>","location":"continuous-execution/#demo"},{"title":"ContinuousExecution","text":"<p><code>ContinuousExecution</code> is the stream execution engine of Continuous Stream Processing.</p> <p><code>ContinuousExecution</code> can only run streaming queries with StreamingRelationV2 leaf logical operators with ContinuousReadSupport data source.</p> <p>When created (for a streaming query), <code>ContinuousExecution</code> is given the &lt;&gt;. The analyzed logical plan is immediately transformed to include a ContinuousExecutionRelation for every StreamingRelationV2 leaf logical operator with ContinuousReadSupport data source (and is the logical plan internally).  <p>Note</p> <p><code>ContinuousExecution</code> uses the same instance of <code>ContinuousExecutionRelation</code> for the same instances of StreamingRelationV2 with ContinuousReadSupport data source.</p>  <p>When requested to &lt;&gt;, <code>ContinuousExecution</code> collects ContinuousReadSupport data sources (inside ContinuousExecutionRelation) from the &lt;&gt; and requests each and every <code>ContinuousReadSupport</code> to create a ContinuousReader (that are stored in &lt;&gt; internal registry).","location":"continuous-execution/ContinuousExecution/"},{"title":"Local Properties","text":"","location":"continuous-execution/ContinuousExecution/#local-properties"},{"title":"__epoch_coordinator_id <p><code>ContinuousExecution</code> uses __epoch_coordinator_id local property for...FIXME</p>","text":"","location":"continuous-execution/ContinuousExecution/#__epoch_coordinator_id"},{"title":"__continuous_start_epoch <p><code>ContinuousExecution</code> uses __continuous_start_epoch local property for...FIXME</p>","text":"","location":"continuous-execution/ContinuousExecution/#__continuous_start_epoch"},{"title":"__continuous_epoch_interval <p><code>ContinuousExecution</code> uses __continuous_epoch_interval local property for...FIXME</p>","text":"","location":"continuous-execution/ContinuousExecution/#__continuous_epoch_interval"},{"title":"TriggerExecutor <p>TriggerExecutor for the Trigger:</p> <ul> <li><code>ProcessingTimeExecutor</code> for ContinuousTrigger</li> </ul> <p>Used when...FIXME</p>  <p>Note</p> <p><code>StreamExecution</code> throws an <code>IllegalStateException</code> when the Trigger is not a ContinuousTrigger.</p>","text":"","location":"continuous-execution/ContinuousExecution/#triggerexecutor"},{"title":"Running Activated Streaming Query <pre><code>runActivatedStream(\n  sparkSessionForStream: SparkSession): Unit\n</code></pre> <p><code>runActivatedStream</code> simply runs the streaming query in continuous mode as long as the state is <code>ACTIVE</code>.</p> <p><code>runActivatedStream</code> is part of StreamExecution abstraction.</p>","text":"","location":"continuous-execution/ContinuousExecution/#running-activated-streaming-query"},{"title":"Running Streaming Query in Continuous Mode <pre><code>runContinuous(\n  sparkSessionForQuery: SparkSession): Unit\n</code></pre> <p><code>runContinuous</code> initializes the continuousSources internal registry by traversing the analyzed logical plan to find ContinuousExecutionRelation leaf logical operators and requests their ContinuousReadSupport data sources to create a ContinuousReader (with the sources metadata directory under the checkpoint directory).</p> <p><code>runContinuous</code> initializes the uniqueSources internal registry to be the continuousSources distinct.</p> <p><code>runContinuous</code> gets the start offsets (they may or may not be available).</p> <p><code>runContinuous</code> transforms the analyzed logical plan. For every ContinuousExecutionRelation <code>runContinuous</code> finds the corresponding ContinuousReader (in the continuousSources), requests it to deserialize the start offsets (from their JSON representation), and then setStartOffset. In the end, <code>runContinuous</code> creates a StreamingDataSourceV2Relation (with the read schema of the <code>ContinuousReader</code> and the <code>ContinuousReader</code> itself).</p> <p><code>runContinuous</code> rewires the transformed plan (with the <code>StreamingDataSourceV2Relation</code>) to use the new attributes from the source (the reader).</p>  <p>Important</p> <p><code>CurrentTimestamp</code> and <code>CurrentDate</code> expressions are not supported for continuous processing.</p>  <p><code>runContinuous</code>...FIXME</p> <p><code>runContinuous</code> finds the only ContinuousReader (of the only <code>StreamingDataSourceV2Relation</code>) in the query plan with the <code>WriteToContinuousDataSource</code>.</p>","text":"","location":"continuous-execution/ContinuousExecution/#running-streaming-query-in-continuous-mode"},{"title":"queryPlanning Phase <p>In queryPlanning time-tracking section, <code>runContinuous</code> creates an IncrementalExecution (that becomes the lastExecution) that is immediately executed (the entire query execution pipeline is executed up to and including executedPlan).</p> <p><code>runContinuous</code> sets the following local properties:</p> <ul> <li> <p>__is_continuous_processing as <code>true</code></p> </li> <li> <p>__continuous_start_epoch as the currentBatchId</p> </li> <li> <p>__epoch_coordinator_id as the currentEpochCoordinatorId, i.e. runId followed by <code>--</code> with a random UUID</p> </li> <li> <p>__continuous_epoch_interval as the interval of the ContinuousTrigger</p> </li> </ul> <p><code>runContinuous</code> uses the <code>EpochCoordinatorRef</code> helper to create a remote reference to the EpochCoordinator RPC endpoint (with the ContinuousReader, the currentEpochCoordinatorId, and the currentBatchId).</p> <p><code>runContinuous</code> creates a daemon epoch update thread and starts it immediately.</p>","text":"","location":"continuous-execution/ContinuousExecution/#queryplanning-phase"},{"title":"runContinuous Phase <p>In runContinuous time-tracking section, <code>runContinuous</code> requests the physical query plan (of the IncrementalExecution) to execute (that simply requests the physical operator to <code>doExecute</code> and generate an <code>RDD[InternalRow]</code>).</p> <p><code>runContinuous</code> is used when <code>ContinuousExecution</code> is requested to run an activated streaming query.</p> <p>==== [[runContinuous-epoch-update-thread]] Epoch Update Thread</p> <p><code>runContinuous</code> creates an epoch update thread that...FIXME</p> <p>==== [[getStartOffsets]] Getting Start Offsets From Checkpoint -- <code>getStartOffsets</code> Internal Method</p>","text":"","location":"continuous-execution/ContinuousExecution/#runcontinuous-phase"},{"title":"[source, scala]","text":"","location":"continuous-execution/ContinuousExecution/#source-scala"},{"title":"getStartOffsets(sparkSessionToRunBatches: SparkSession): OffsetSeq <p><code>getStartOffsets</code>...FIXME</p> <p>NOTE: <code>getStartOffsets</code> is used exclusively when <code>ContinuousExecution</code> is requested to &lt;&gt;.","text":"","location":"continuous-execution/ContinuousExecution/#getstartoffsetssparksessiontorunbatches-sparksession-offsetseq"},{"title":"Committing Epoch <pre><code>commit(\n  epoch: Long): Unit\n</code></pre> <p>In essence, <code>commit</code> adds the given epoch to commit log and the committedOffsets, and requests the &lt;&gt; to commit the corresponding offset. In the end, <code>commit</code> removes old log entries from the offset and commit logs (to keep spark.sql.streaming.minBatchesToRetain entries only). <p>Internally, <code>commit</code> recordTriggerOffsets (with the from and to offsets as the committedOffsets and availableOffsets, respectively).</p> <p>At this point, <code>commit</code> may simply return when the stream execution thread is no longer alive (died).</p> <p><code>commit</code> requests the commit log to store a metadata for the epoch.</p> <p><code>commit</code> requests the single &lt;&gt; to deserialize the offset for the epoch (from the offset write-ahead log). <p><code>commit</code> adds the single &lt;&gt; and the offset (for the epoch) to the committedOffsets registry. <p><code>commit</code> requests the single &lt;&gt; to commit the offset. <p><code>commit</code> requests the offset and commit logs to remove log entries to keep spark.sql.streaming.minBatchesToRetain only.</p> <p><code>commit</code> then acquires the awaitProgressLock, wakes up all threads waiting for the awaitProgressLockCondition and in the end releases the awaitProgressLock.</p> <p>NOTE: <code>commit</code> supports only one continuous source (registered in the &lt;&gt; internal registry). <p><code>commit</code> asserts that the given epoch is available in the offsetLog internal registry (i.e. the offset for the given epoch has been reported before).</p> <p><code>commit</code> is used when <code>EpochCoordinator</code> is requested to commitEpoch.</p> <p>=== [[addOffset]] <code>addOffset</code> Method</p>","text":"","location":"continuous-execution/ContinuousExecution/#committing-epoch"},{"title":"[source, scala] <p>addOffset(   epoch: Long,   reader: ContinuousReader,   partitionOffsets: Seq[PartitionOffset]): Unit</p>  <p>In essense, <code>addOffset</code> requests the given ContinuousReader to mergeOffsets (with the given <code>PartitionOffsets</code>) and then requests the OffsetSeqLog to register the offset with the given epoch.</p> <p></p> <p>Internally, <code>addOffset</code> requests the given ContinuousReader to mergeOffsets (with the given <code>PartitionOffsets</code>) and to get the current \"global\" offset back.</p> <p><code>addOffset</code> then requests the OffsetSeqLog to add the current \"global\" offset for the given <code>epoch</code>.</p> <p><code>addOffset</code> requests the OffsetSeqLog for the offset at the previous epoch.</p> <p>If the offsets at the current and previous epochs are the same, <code>addOffset</code> turns the noNewData internal flag on.</p> <p><code>addOffset</code> then acquires the awaitProgressLock, wakes up all threads waiting for the awaitProgressLockCondition and in the end releases the awaitProgressLock.</p> <p>NOTE: <code>addOffset</code> supports exactly one continuous source.</p> <p><code>addOffset</code> is used when <code>EpochCoordinator</code> is requested to handle a ReportPartitionOffset message.</p>","text":"","location":"continuous-execution/ContinuousExecution/#source-scala_1"},{"title":"Analyzed Logical Plan of Streaming Query <pre><code>logicalPlan: LogicalPlan\n</code></pre> <p><code>logicalPlan</code> resolves StreamingRelationV2 leaf logical operators (with a ContinuousReadSupport source) to ContinuousExecutionRelation leaf logical operators.</p> <p>Internally, <code>logicalPlan</code> transforms the &lt;&gt; as follows: <p>. For every StreamingRelationV2 leaf logical operator with a ContinuousReadSupport source, <code>logicalPlan</code> looks it up for the corresponding ContinuousExecutionRelation (if available in the internal lookup registry) or creates a <code>ContinuousExecutionRelation</code> (with the <code>ContinuousReadSupport</code> source, the options and the output attributes of the <code>StreamingRelationV2</code> operator)</p> <p>. For any other <code>StreamingRelationV2</code>, <code>logicalPlan</code> throws an <code>UnsupportedOperationException</code>: + <pre><code>Data source [name] does not support continuous processing.\n</code></pre></p> <p><code>logicalPlan</code> is part of the StreamExecution abstraction.</p>","text":"","location":"continuous-execution/ContinuousExecution/#analyzed-logical-plan-of-streaming-query"},{"title":"Creating Instance <p><code>ContinuousExecution</code> takes the following when created:</p> <ul> <li>[[sparkSession]] <code>SparkSession</code></li> <li>[[name]] The name of the structured query</li> <li>[[checkpointRoot]] Path to the checkpoint directory (aka metadata directory)</li> <li>[[analyzedPlan]] Analyzed logical query plan (<code>LogicalPlan</code>)</li> <li>[[trigger]] Trigger</li> <li>[[triggerClock]] <code>Clock</code></li> <li>[[outputMode]] OutputMode</li> <li>[[extraOptions]] Options (<code>Map[String, String]</code>)</li> <li>[[deleteCheckpointOnStop]] <code>deleteCheckpointOnStop</code> flag to control whether to delete the checkpoint directory on stop</li> </ul> <p><code>ContinuousExecution</code> is created when <code>StreamingQueryManager</code> is requested to create a streaming query with a StreamWriteSupport sink and a ContinuousTrigger (when <code>DataStreamWriter</code> is requested to start an execution of the streaming query).</p>","text":"","location":"continuous-execution/ContinuousExecution/#creating-instance"},{"title":"Stopping Stream Processing <pre><code>stop(): Unit\n</code></pre> <p><code>stop</code> is part of the StreamingQuery abstraction.</p> <p><code>stop</code> transitions the streaming query to <code>TERMINATED</code> state.</p> <p>If the queryExecutionThread is alive (i.e. it has been started and has not yet died), <code>stop</code> interrupts it and waits for this thread to die.</p> <p>In the end, <code>stop</code> prints out the following INFO message to the logs:</p> <pre><code>Query [prettyIdString] was stopped\n</code></pre>  <p>Note</p> <p>prettyIdString is in the format of <code>queryName [id = [id], runId = [runId]]</code>.</p>  <p>=== [[awaitEpoch]] <code>awaitEpoch</code> Internal Method</p>","text":"","location":"continuous-execution/ContinuousExecution/#stopping-stream-processing"},{"title":"[source, scala]","text":"","location":"continuous-execution/ContinuousExecution/#source-scala_2"},{"title":"awaitEpoch(epoch: Long): Unit <p><code>awaitEpoch</code>...FIXME</p> <p>NOTE: <code>awaitEpoch</code> seems to be used exclusively in tests.</p>","text":"","location":"continuous-execution/ContinuousExecution/#awaitepochepoch-long-unit"},{"title":"continuousSources <pre><code>continuousSources: Seq[ContinuousReader]\n</code></pre> <p>Registry of ContinuousReaders (in the analyzed logical plan of the streaming query)</p> <p>As asserted in &lt;&gt; and &lt;&gt; there could only be exactly one <code>ContinuousReaders</code> registered. <p>Used when <code>ContinuousExecution</code> is requested to &lt;&gt;, &lt;&gt;, and &lt;&gt; <p>Use &lt;&gt; to access the current value","text":"","location":"continuous-execution/ContinuousExecution/#continuoussources"},{"title":"sources <p><code>ContinuousExecution</code> supports one &lt;&gt; only in a &lt;&gt; (and asserts it when &lt;&gt; and &lt;&gt;). When requested for available streaming sources, <code>ContinuousExecution</code> simply gives the &lt;&gt;. <pre><code>import org.apache.spark.sql.streaming.Trigger\nimport scala.concurrent.duration._\nval sq = spark\n  .readStream\n  .format(\"rate\")\n  .load\n  .writeStream\n  .format(\"console\")\n  .option(\"truncate\", false)\n  .trigger(../Trigger.Continuous(1.minute)) // &lt;-- Gives ContinuousExecution\n  .queryName(\"rate2console\")\n  .start\n\nimport org.apache.spark.sql.streaming.StreamingQuery\nassert(sq.isInstanceOf[StreamingQuery])\n\n// The following gives access to the internals\n// And to ContinuousExecution\nimport org.apache.spark.sql.execution.streaming.StreamingQueryWrapper\nval engine = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery\nimport org.apache.spark.sql.execution.streaming.StreamExecution\nassert(engine.isInstanceOf[StreamExecution])\n\nimport org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution\nval continuousEngine = engine.asInstanceOf[ContinuousExecution]\nassert(continuousEngine.trigger == Trigger.Continuous(1.minute))\n</code></pre>","text":"","location":"continuous-execution/ContinuousExecution/#sources"},{"title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution=ALL\n</code></pre> <p>Refer to Logging.</p>","text":"","location":"continuous-execution/ContinuousExecution/#logging"},{"title":"ContinuousReadSupport","text":"<p><code>ContinuousReadSupport</code> is the &lt;&gt; of the <code>DataSourceV2</code> for &lt;&gt; with a &lt;&gt; for Continuous Stream Processing. <p>[[contract]][[createContinuousReader]] <code>ContinuousReadSupport</code> defines a single <code>createContinuousReader</code> method to create a ContinuousReader.</p> <pre><code>ContinuousReader createContinuousReader(\n  Optional&lt;StructType&gt; schema,\n  String checkpointLocation,\n  DataSourceOptions options)\n</code></pre> <p><code>createContinuousReader</code> is used when:</p> <ul> <li> <p><code>ContinuousExecution</code> is requested to run a streaming query (and finds ContinuousExecutionRelations in the analyzed logical plan)</p> </li> <li> <p><code>DataStreamReader</code> is requested to create a streaming query for a ContinuousReadSupport data source</p> </li> </ul> <p>[[implementations]] .ContinuousReadSupports [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | ContinuousReadSupport | Description</p> <p>| ContinuousMemoryStream | [[ContinuousMemoryStream]] Data source provider for <code>memory</code> format</p> <p>| KafkaSourceProvider | [[KafkaSourceProvider]] Data source provider for <code>kafka</code> format</p> <p>| RateStreamProvider | [[RateStreamProvider]] Data source provider for <code>rate</code> format</p> <p>| TextSocketSourceProvider | [[TextSocketSourceProvider]] Data source provider for <code>socket</code> format</p> <p>|===</p>","location":"continuous-execution/ContinuousReadSupport/"},{"title":"ContinuousReader \u2014 Data Source Readers in Continuous Stream Processing","text":"<p><code>ContinuousReader</code> is the &lt;&gt; of Spark SQL's <code>DataSourceReader</code> abstraction for &lt;&gt; in Continuous Stream Processing. <p><code>ContinuousReader</code> is part of the novel Data Source API V2 in Spark SQL.</p>  <p>Tip</p> <p>Read up on Data Source API V2 in The Internals of Spark SQL online book.</p>  <p>[[contract]] .ContinuousReader Contract [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| commit a| [[commit]]</p>","location":"continuous-execution/ContinuousReader/"},{"title":"[source, java]","text":"","location":"continuous-execution/ContinuousReader/#source-java"},{"title":"void commit(Offset end)","text":"<p>Commits the specified offset</p> <p>Used exclusively when <code>ContinuousExecution</code> is requested to commit an epoch</p> <p>| deserializeOffset a| [[deserializeOffset]]</p>","location":"continuous-execution/ContinuousReader/#void-commitoffset-end"},{"title":"[source, java]","text":"","location":"continuous-execution/ContinuousReader/#source-java_1"},{"title":"Offset deserializeOffset(String json)","text":"<p>Deserializes an offset from JSON representation</p> <p>Used when <code>ContinuousExecution</code> is requested to run a streaming query and commit an epoch</p> <p>| getStartOffset a| [[getStartOffset]]</p>","location":"continuous-execution/ContinuousReader/#offset-deserializeoffsetstring-json"},{"title":"[source, java]","text":"","location":"continuous-execution/ContinuousReader/#source-java_2"},{"title":"Offset getStartOffset()","text":"<p>NOTE: Used exclusively in tests.</p> <p>| mergeOffsets a| [[mergeOffsets]]</p>","location":"continuous-execution/ContinuousReader/#offset-getstartoffset"},{"title":"[source, java]","text":"","location":"continuous-execution/ContinuousReader/#source-java_3"},{"title":"Offset mergeOffsets(PartitionOffset[] offsets)","text":"<p>Used exclusively when <code>ContinuousExecution</code> is requested to addOffset</p> <p>| needsReconfiguration a| [[needsReconfiguration]]</p>","location":"continuous-execution/ContinuousReader/#offset-mergeoffsetspartitionoffset-offsets"},{"title":"[source, java]","text":"","location":"continuous-execution/ContinuousReader/#source-java_4"},{"title":"boolean needsReconfiguration()","text":"<p>Indicates that the reader needs reconfiguration (e.g. to generate new input partitions)</p> <p>Used exclusively when <code>ContinuousExecution</code> is requested to run a streaming query in continuous mode</p> <p>| setStartOffset a| [[setStartOffset]]</p>","location":"continuous-execution/ContinuousReader/#boolean-needsreconfiguration"},{"title":"[source, java]","text":"","location":"continuous-execution/ContinuousReader/#source-java_5"},{"title":"void setStartOffset(Optional start) <p>Used exclusively when <code>ContinuousExecution</code> is requested to run the streaming query in continuous mode.</p> <p>|===</p> <p>[[implementations]] .ContinuousReaders [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | ContinuousReader | Description</p> <p>| ContinuousMemoryStream | [[ContinuousMemoryStream]]</p> <p>| KafkaContinuousReader | [[KafkaContinuousReader]]</p> <p>| RateStreamContinuousReader | [[RateStreamContinuousReader]]</p> <p>| TextSocketContinuousReader | [[TextSocketContinuousReader]]</p> <p>|===</p>","text":"","location":"continuous-execution/ContinuousReader/#void-setstartoffsetoptional-start"},{"title":"Data Sources","text":"<p>Spark Structured Streaming comes with the following streaming sources and sinks:</p> <ul> <li>File</li> <li>Kafka</li> <li>Text Socket</li> <li>Rate</li> <li>Console</li> <li>Foreach</li> <li>ForeachBatchSink</li> <li>Memory</li> </ul>","location":"datasources/"},{"title":"ForeachBatchSink","text":"<p><code>ForeachBatchSink</code> is a streaming sink that represents DataStreamWriter.foreachBatch streaming operator at runtime.</p>  Type Constructor <p><code>ForeachBatchSink[T]</code> is a Scala type constructor with the type parameter <code>T</code>.</p>  <p><code>ForeachBatchSink</code> was added in Spark 2.4.0 as part of SPARK-24565 Add API for in Structured Streaming for exposing output rows of each microbatch as a DataFrame.</p> <pre><code>import org.apache.spark.sql.Dataset\nval q = spark.readStream\n  .format(\"rate\")\n  .load\n  .writeStream\n  .foreachBatch { (output: Dataset[_], batchId: Long) =&gt; // &lt;-- creates a ForeachBatchSink\n    println(s\"Batch ID: $batchId\")\n    output.show\n  }\n  .start\n// q.stop\n\nscala&gt; println(q.lastProgress.sink.description)\nForeachBatchSink\n</code></pre>","location":"datasources/ForeachBatchSink/"},{"title":"Creating Instance","text":"<p><code>ForeachBatchSink</code> takes the following when created:</p> <ul> <li> Batch Writer Function (<code>(Dataset[T], Long) =&gt; Unit</code>) <li> Encoder of type <code>T</code> (<code>ExpressionEncoder[T]</code>)  <p><code>ForeachBatchSink</code> is created when <code>DataStreamWriter</code> is requested to start execution of the streaming query (with the foreachBatch source) for DataStreamWriter.foreachBatch streaming operator.</p>","location":"datasources/ForeachBatchSink/#creating-instance"},{"title":"Adding Batch <pre><code>addBatch(\n  batchId: Long,\n  data: DataFrame): Unit\n</code></pre> <p><code>addBatch</code> requests the encoder to <code>resolveAndBind</code> (using the output of the analyzed logical plan of the given <code>DataFrame</code>) that creates a \"resolved\" encoder. <code>addBatch</code> requests the resolved encoder to create an <code>Deserializer</code> (to convert a Spark SQL <code>Row</code> objects into objects of type <code>T</code>).</p> <p><code>addBatch</code> requests the <code>QueryExecution</code> (of the given <code>DataFrame</code>) for <code>RDD[InternalRow]</code> (executes the query plan) and applies <code>map</code> operator to convert rows to Scala objects.</p>  <p>Important</p> <p>At this point the \"old\" <code>DataFrame</code> is no longer a <code>DataFrame</code> but an <code>RDD[InternalRow]</code>. One of the \"side-effects\" is that whatever logical and physical optimizations may have been applied to the given <code>DataFrame</code> it is over now.</p>  <p><code>addBatch</code> creates a new <code>Dataset</code> (for the RDD) and executes batchWriter function (passing the <code>Dataset</code> and the <code>batchId</code>).</p> <p><code>addBatch</code> is a part of the Sink abstraction.</p>","text":"","location":"datasources/ForeachBatchSink/#adding-batch"},{"title":"Text Representation <p><code>ForeachBatchSink</code> uses ForeachBatchSink name.</p>","text":"","location":"datasources/ForeachBatchSink/#text-representation"},{"title":"Console Data Source","text":"","location":"datasources/console/"},{"title":"ConsoleSinkProvider","text":"<p><code>ConsoleSinkProvider</code> is a <code>SimpleTableProvider</code> (Spark SQL) for <code>console</code> data source.</p> <p><code>ConsoleSinkProvider</code> is a <code>DataSourceRegister</code> (Spark SQL) and registers itself as the console data source format.</p> <p><code>ConsoleSinkProvider</code> is a <code>CreatableRelationProvider</code> (Spark SQL).</p>","location":"datasources/console/ConsoleSinkProvider/"},{"title":"Demo","text":"<pre><code>import org.apache.spark.sql.streaming.Trigger\nval q = spark\n  .readStream\n  .format(\"rate\")\n  .load\n  .writeStream\n  .format(\"console\") // &lt;-- requests ConsoleSinkProvider for a sink\n  .trigger(Trigger.Once)\n  .start\n</code></pre> <pre><code>scala&gt; println(q.lastProgress.sink)\n{\n  \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSinkProvider@2392cfb1\"\n}\n</code></pre>","location":"datasources/console/ConsoleSinkProvider/#demo"},{"title":"ConsoleWriter","text":"<p><code>ConsoleWriter</code> is a writer for console data source format.</p>","location":"datasources/console/ConsoleWriter/"},{"title":"File Data Source","text":"<p>File Data Source comes with the following main abstractions:</p> <ul> <li>FileStreamSource</li> <li>FileStreamSink</li> </ul> <p>FileStreamSink uses FileStreamSinkLog for tracking valid files per micro-batch (as part of ManifestFileCommitProtocol).</p>","location":"datasources/file/"},{"title":"CompactibleFileStreamLog","text":"<p><code>CompactibleFileStreamLog</code>\u00a0is an extension of the HDFSMetadataLog abstraction for metadata logs that can compact logs at regular intervals.</p>","location":"datasources/file/CompactibleFileStreamLog/"},{"title":"Creating Instance","text":"<p><code>CompactibleFileStreamLog</code> takes the following to be created:</p> <ul> <li> Version of the Metadata Log <li> <code>SparkSession</code> <li> Path of the Metadata Log   Abstract Class  <p><code>CompactibleFileStreamLog</code>\u00a0is an abstract class and cannot be created directly. It is created indirectly for the concrete CompactibleFileStreamLogs.</p>","location":"datasources/file/CompactibleFileStreamLog/#creating-instance"},{"title":"Contract","text":"","location":"datasources/file/CompactibleFileStreamLog/#contract"},{"title":"Filtering Out Obsolete Logs <pre><code>compactLogs(\n  logs: Seq[T]): Seq[T]\n</code></pre> <p>Used when storing metadata and for all files (except deleted)</p>  <p>Important</p> <p><code>compactLogs</code> does nothing important in the available implementations. Consider this method a noop.</p>","text":"","location":"datasources/file/CompactibleFileStreamLog/#filtering-out-obsolete-logs"},{"title":"Default Compact Interval <pre><code>defaultCompactInterval: Int\n</code></pre> <p>Used for the compact interval</p>","text":"","location":"datasources/file/CompactibleFileStreamLog/#default-compact-interval"},{"title":"File Cleanup Delay <pre><code>fileCleanupDelayMs: Long\n</code></pre> <p>Used for delete expired log entries</p>","text":"","location":"datasources/file/CompactibleFileStreamLog/#file-cleanup-delay"},{"title":"isDeletingExpiredLog <pre><code>isDeletingExpiredLog: Boolean\n</code></pre> <p>Used to store metadata</p>","text":"","location":"datasources/file/CompactibleFileStreamLog/#isdeletingexpiredlog"},{"title":"Implementations","text":"<ul> <li>FileStreamSinkLog</li> <li>FileStreamSourceLog</li> </ul>","location":"datasources/file/CompactibleFileStreamLog/#implementations"},{"title":"Compaction <pre><code>compact(\n  batchId: Long,\n  logs: Array[T]): Boolean\n</code></pre> <p><code>compact</code> finds valid metadata files for compaction (for the given compaction <code>batchId</code> and compact interval) and makes sure that they are all available. <code>compact</code> tracks elapsed time (<code>loadElapsedMs</code>).</p> <p><code>compact</code> filters out obsolete logs among the valid metadata files and the input <code>logs</code> (which actually does nothing important given the note in compactLogs).</p> <p><code>compact</code> stores the metadata (the filtered metadata files and the input <code>logs</code>) for the input <code>batchId</code>. <code>compact</code> tracks elapsed time (<code>writeElapsedMs</code>).</p> <p><code>compact</code> prints out the following DEBUG message (only when the total elapsed time of <code>loadElapsedMs</code> and <code>writeElapsedMs</code> are below the unconfigurable <code>2000</code> ms):</p> <pre><code>Compacting took [elapsedMs] ms (load: [loadElapsedMs] ms, write: [writeElapsedMs] ms) for compact batch [batchId]\n</code></pre> <p>In case the total epased time is above the unconfigurable <code>2000</code> ms, <code>compact</code> prints out the following WARN messages:</p> <pre><code>Compacting took [elapsedMs] ms (load: [loadElapsedMs] ms, write: [writeElapsedMs] ms) for compact batch [batchId]\nLoaded [allLogs] entries (estimated [allLogs] bytes in memory), and wrote [compactedLogs] entries for compact batch [batchId]\n</code></pre> <p><code>compact</code> throws an <code>IllegalStateException</code> when one of the metadata files to compact is not valid (not accessible on a file system or of incorrect format):</p> <pre><code>[batchIdToPath] doesn't exist when compacting batch [batchId] (compactInterval: [compactInterval])\n</code></pre> <p><code>compact</code> is used while storing metadata for streaming batch.</p>","text":"","location":"datasources/file/CompactibleFileStreamLog/#compaction"},{"title":"spark.sql.streaming.fileSink.log.cleanupDelay <p><code>CompactibleFileStreamLog</code> uses spark.sql.streaming.fileSink.log.cleanupDelay configuration property to delete expired log entries.</p>","text":"","location":"datasources/file/CompactibleFileStreamLog/#sparksqlstreamingfilesinklogcleanupdelay"},{"title":"compact File Suffix <p><code>CompactibleFileStreamLog</code> uses .compact file suffix for batchIdToPath, getBatchIdFromFileName, and the compactInterval.</p>","text":"","location":"datasources/file/CompactibleFileStreamLog/#compact-file-suffix"},{"title":"Storing Metadata for Streaming Batch <pre><code>add(\n  batchId: Long,\n  logs: Array[T]): Boolean\n</code></pre> <p><code>add</code> checks whether the given <code>batchId</code> is compaction batch or not (alongside compact interval).</p> <p><code>add</code>...FIXME</p> <p><code>add</code> is part of the MetadataLog abstraction.</p>","text":"","location":"datasources/file/CompactibleFileStreamLog/#storing-metadata-for-streaming-batch"},{"title":"Deleting Expired Log Entries <pre><code>deleteExpiredLog(\n  currentBatchId: Long): Unit\n</code></pre> <p><code>deleteExpiredLog</code>...FIXME</p> <p><code>deleteExpiredLog</code> does nothing and simply returns when the current batch ID incremented (<code>currentBatchId + 1</code>) is below the compact interval plus the minBatchesToRetain.</p>","text":"","location":"datasources/file/CompactibleFileStreamLog/#deleting-expired-log-entries"},{"title":"Compact Interval <pre><code>compactInterval: Int\n</code></pre> <p><code>compactInterval</code> is the number of metadata log files between compactions.</p>  Lazy Value <p><code>compactInterval</code> is a Scala lazy value which means that the code to initialize it is executed once only (when accessed for the first time) and cached afterwards.</p>  <p><code>compactInterval</code> finds compacted IDs and determines the compact interval.</p> <p><code>compactInterval</code> requests the CheckpointFileManager for the files in the metadataPath that are batch (and possibly compacted). <code>compactInterval</code> takes the compacted files only (if available), converts them to batch IDs and sorts in descending order.</p> <p><code>compactInterval</code> starts with the default compact interval.</p> <ul> <li>If there are two compacted IDs, their difference is the compact interval</li> <li>If there is one compacted ID only, <code>compactInterval</code> \"derives\" the compact interval (FIXME)</li> </ul> <p><code>compactInterval</code> asserts that the compact interval is a positive value or throws an <code>AssertionError</code>.</p> <p><code>compactInterval</code> prints out the following INFO message to the logs (with the defaultCompactInterval):</p> <pre><code>Set the compact interval to [interval] [defaultCompactInterval: [defaultCompactInterval]]\n</code></pre>","text":"","location":"datasources/file/CompactibleFileStreamLog/#compact-interval"},{"title":"All Files (Except Deleted) <pre><code>allFiles(): Array[T]\n</code></pre> <p><code>allFiles</code>...FIXME</p> <p><code>allFiles</code> is used when:</p> <ul> <li><code>FileStreamSource</code> is created</li> <li><code>MetadataLogFileIndex</code> is created</li> </ul>","text":"","location":"datasources/file/CompactibleFileStreamLog/#all-files-except-deleted"},{"title":"Converting Batch Id to Hadoop Path <pre><code>batchIdToPath(\n  batchId: Long): Path\n</code></pre> <p><code>batchIdToPath</code>...FIXME</p> <p><code>batchIdToPath</code> is part of the HDFSMetadataLog abstraction.</p>","text":"","location":"datasources/file/CompactibleFileStreamLog/#converting-batch-id-to-hadoop-path"},{"title":"Converting Hadoop Path to Batch Id <pre><code>pathToBatchId(\n  path: Path): Long\n</code></pre> <p><code>pathToBatchId</code>...FIXME</p> <p><code>pathToBatchId</code> is part of the HDFSMetadataLog abstraction.</p>","text":"","location":"datasources/file/CompactibleFileStreamLog/#converting-hadoop-path-to-batch-id"},{"title":"isBatchFile <pre><code>isBatchFile(\n  path: Path): Boolean\n</code></pre> <p><code>isBatchFile</code> is <code>true</code> when successful to get the batchId for the given path. Otherwise is <code>false</code>.</p> <p><code>isBatchFile</code> is part of the HDFSMetadataLog abstraction.</p>","text":"","location":"datasources/file/CompactibleFileStreamLog/#isbatchfile"},{"title":"Serializing Metadata (Writing Metadata in Serialized Format) <pre><code>serialize(\n  logData: Array[T],\n  out: OutputStream): Unit\n</code></pre> <p><code>serialize</code> writes the version header (<code>v</code> and the &lt;&gt;) out to the given output stream (in <code>UTF_8</code>). <p><code>serialize</code> then writes the log data (serialized using Json4s (with Jackson binding) library). Entries are separated by new lines.</p> <p><code>serialize</code> is part of the HDFSMetadataLog abstraction.</p>","text":"","location":"datasources/file/CompactibleFileStreamLog/#serializing-metadata-writing-metadata-in-serialized-format"},{"title":"Deserializing Metadata <pre><code>deserialize(\n  in: InputStream): Array[T]\n</code></pre> <p><code>deserialize</code>...FIXME</p> <p><code>deserialize</code> is part of the HDFSMetadataLog abstraction.</p>","text":"","location":"datasources/file/CompactibleFileStreamLog/#deserializing-metadata"},{"title":"Utilities","text":"","location":"datasources/file/CompactibleFileStreamLog/#utilities"},{"title":"getBatchIdFromFileName <pre><code>getBatchIdFromFileName(\n  fileName: String): Long\n</code></pre> <p><code>getBatchIdFromFileName</code> simply removes the .compact suffix from the given <code>fileName</code> and converts the remaining part to a number.</p> <p><code>getBatchIdFromFileName</code> is used for pathToBatchId, isBatchFile, and delete expired log entries.</p>","text":"","location":"datasources/file/CompactibleFileStreamLog/#getbatchidfromfilename"},{"title":"getValidBatchesBeforeCompactionBatch <pre><code>getValidBatchesBeforeCompactionBatch(\n  compactionBatchId: Long,\n  compactInterval: Int): Seq[Long]\n</code></pre> <p><code>getValidBatchesBeforeCompactionBatch</code>...FIXME</p> <p><code>getBatchIdFromFileName</code> is used for compaction.</p>","text":"","location":"datasources/file/CompactibleFileStreamLog/#getvalidbatchesbeforecompactionbatch"},{"title":"FileStreamSink","text":"<p><code>FileStreamSink</code> is a streaming sink that writes data out to files (in a given file format and a directory).</p> <p><code>FileStreamSink</code> can only be used with Append output mode.</p>  <p>Tip</p> <p>Learn more in Demo: Deep Dive into FileStreamSink.</p>","location":"datasources/file/FileStreamSink/"},{"title":"Creating Instance","text":"<p><code>FileStreamSink</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> <li> Path <li> <code>FileFormat</code> <li> Names of the Partition Columns (if any) <li> Options (<code>Map[String, String]</code>)  <p><code>FileStreamSink</code> is created\u00a0when <code>DataSource</code> is requested to create a streaming sink for <code>FileFormat</code> data sources.</p>","location":"datasources/file/FileStreamSink/#creating-instance"},{"title":"Metadata Log Directory <p><code>FileStreamSink</code> uses _spark_metadata directory (under the path) as the Metadata Log Directory to store metadata indicating which files are valid and can be read (and skipping already committed batch).</p> <p>Metadata Log Directory is managed by FileStreamSinkLog.</p>","text":"","location":"datasources/file/FileStreamSink/#metadata-log-directory"},{"title":"Hadoop Path of Metadata Log <pre><code>logPath: Path\n</code></pre> <p><code>logPath</code> is the location of the Metadata Log (as a Hadoop Path).</p>","text":"","location":"datasources/file/FileStreamSink/#hadoop-path-of-metadata-log"},{"title":"FileStreamSinkLog <pre><code>fileLog: FileStreamSinkLog\n</code></pre> <p><code>fileLog</code> is a FileStreamSinkLog (for the version 1 and the metadata log path)</p> <p>Used for \"adding\" batch.</p>","text":"","location":"datasources/file/FileStreamSink/#filestreamsinklog"},{"title":"Text Representation <p><code>FileStreamSink</code> uses the path for the text representation (<code>toString</code>):</p> <pre><code>FileSink[path]\n</code></pre>","text":"","location":"datasources/file/FileStreamSink/#text-representation"},{"title":"\"Adding\" Batch of Data to Sink <pre><code>addBatch(\n  batchId: Long,\n  data: DataFrame): Unit\n</code></pre> <p><code>addBatch</code> requests the FileStreamSinkLog for the latest committed batch ID.</p> <p>With a newer <code>batchId</code>, <code>addBatch</code> creates a <code>FileCommitProtocol</code> based on spark.sql.streaming.commitProtocolClass configuration property.</p>  <p>The Internals of Apache Spark</p> <p>Learn more on FileCommitProtocol in The Internals of Apache Spark.</p>  <p>For a ManifestFileCommitProtocol, <code>addBatch</code> requests it to setupManifestOptions (with the FileStreamSinkLog and the given <code>batchId</code>).</p> <p>In the end, <code>addBatch</code> writes out the data using <code>FileFormatWriter.write</code> workflow (with the <code>FileCommitProtocol</code> and BasicWriteJobStatsTracker).</p>  <p>The Internals of Spark SQL</p> <p>Learn more on FileFormatWriter in The Internals of Spark SQL.</p>  <p><code>addBatch</code> prints out the following INFO message to the logs when the given <code>batchId</code> is below the latest committed batch ID:</p> <pre><code>Skipping already committed batch [batchId]\n</code></pre> <p><code>addBatch</code> is a part of the Sink abstraction.</p>","text":"","location":"datasources/file/FileStreamSink/#adding-batch-of-data-to-sink"},{"title":"Creating BasicWriteJobStatsTracker <pre><code>basicWriteJobStatsTracker: BasicWriteJobStatsTracker\n</code></pre> <p><code>basicWriteJobStatsTracker</code> creates a <code>BasicWriteJobStatsTracker</code> with the basic metrics:</p> <ul> <li>number of written files</li> <li>bytes of written output</li> <li>number of output rows</li> <li>number of dynamic partitions</li> </ul>  <p>Tip</p> <p>Learn more about BasicWriteJobStatsTracker in The Internals of Spark SQL online book.</p>  <p><code>basicWriteJobStatsTracker</code> is used when <code>FileStreamSink</code> is requested to addBatch.</p>","text":"","location":"datasources/file/FileStreamSink/#creating-basicwritejobstatstracker"},{"title":"hasMetadata Utility <pre><code>hasMetadata(\n  path: Seq[String],\n  hadoopConf: Configuration): Boolean\n</code></pre> <p><code>hasMetadata</code>...FIXME</p> <p><code>hasMetadata</code> is used (to short-circut listing files using MetadataLogFileIndex instead of using HDFS API) when:</p> <ul> <li><code>DataSource</code> (Spark SQL) is requested to resolve a <code>FileFormat</code> relation</li> <li><code>FileTable</code> (Spark SQL) is requested for a <code>PartitioningAwareFileIndex</code></li> <li><code>FileStreamSource</code> is requested to fetchAllFiles</li> </ul>","text":"","location":"datasources/file/FileStreamSink/#hasmetadata-utility"},{"title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.streaming.FileStreamSink</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.streaming.FileStreamSink=ALL\n</code></pre> <p>Refer to Logging.</p>","text":"","location":"datasources/file/FileStreamSink/#logging"},{"title":"FileStreamSinkLog","text":"<p><code>FileStreamSinkLog</code> is a CompactibleFileStreamLog (of SinkFileStatuses) for FileStreamSink and MetadataLogFileIndex.</p> <p><code>FileStreamSinkLog</code> concatenates metadata logs to a single compact file after defined compact interval.</p>","location":"datasources/file/FileStreamSinkLog/"},{"title":"Creating Instance","text":"<p><code>FileStreamSinkLog</code> (like the parent CompactibleFileStreamLog) takes the following to be created:</p> <ul> <li> Version of the Metadata Log <li> <code>SparkSession</code> <li> Path of the Metadata Log","location":"datasources/file/FileStreamSinkLog/#creating-instance"},{"title":"Configuration Properties","text":"","location":"datasources/file/FileStreamSinkLog/#configuration-properties"},{"title":"spark.sql.streaming.fileSink.log.cleanupDelay <p><code>FileStreamSinkLog</code> uses spark.sql.streaming.fileSink.log.cleanupDelay configuration property for fileCleanupDelayMs.</p>","text":"","location":"datasources/file/FileStreamSinkLog/#sparksqlstreamingfilesinklogcleanupdelay"},{"title":"spark.sql.streaming.fileSink.log.compactInterval <p><code>FileStreamSinkLog</code> uses spark.sql.streaming.fileSink.log.compactInterval configuration property for defaultCompactInterval.</p>","text":"","location":"datasources/file/FileStreamSinkLog/#sparksqlstreamingfilesinklogcompactinterval"},{"title":"spark.sql.streaming.fileSink.log.deletion <p><code>FileStreamSinkLog</code> uses spark.sql.streaming.fileSink.log.deletion configuration property for isDeletingExpiredLog.</p>","text":"","location":"datasources/file/FileStreamSinkLog/#sparksqlstreamingfilesinklogdeletion"},{"title":"Compacting Logs <pre><code>compactLogs(\n  logs: Seq[SinkFileStatus]): Seq[SinkFileStatus]\n</code></pre> <p><code>compactLogs</code> finds delete actions in the given collection of SinkFileStatuses.</p> <p>If there are no deletes, <code>compactLogs</code> gives the <code>SinkFileStatus</code>es back (unmodified).</p> <p>Otherwise, <code>compactLogs</code> removes the deleted paths from the <code>SinkFileStatus</code>es.</p> <p><code>compactLogs</code> is part of the CompactibleFileStreamLog abstraction.</p>","text":"","location":"datasources/file/FileStreamSinkLog/#compacting-logs"},{"title":"Version <p><code>FileStreamSinkLog</code> uses 1 for the version.</p>","text":"","location":"datasources/file/FileStreamSinkLog/#version"},{"title":"Actions","text":"","location":"datasources/file/FileStreamSinkLog/#actions"},{"title":"Add <p><code>FileStreamSinkLog</code> uses add action to create new metadata logs.</p>","text":"","location":"datasources/file/FileStreamSinkLog/#add"},{"title":"Delete <p><code>FileStreamSinkLog</code> uses delete action to mark status files to be excluded from compaction.</p>  <p>Important</p> <p>Delete action is not used in Spark Structured Streaming and will be removed in 3.1.0.</p>","text":"","location":"datasources/file/FileStreamSinkLog/#delete"},{"title":"FileStreamSource","text":"<p><code>FileStreamSource</code> is a streaming source that reads files (in a given file format) from a directory.</p> <p><code>FileStreamSource</code> is used by DataSource.createSource for <code>FileFormat</code>.</p>  <p>Tip</p> <p>Learn more in Demo: Using File Streaming Source.</p>","location":"datasources/file/FileStreamSource/"},{"title":"Creating Instance","text":"<p><code>FileStreamSource</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> <li> Path <li> Class Name of <code>FileFormat</code> <li> Schema <li> Names of the Partition Columns (if any) <li> Metadata Path <li> Options (<code>Map[String, String]</code>)  <p><code>FileStreamSource</code> is created\u00a0when <code>DataSource</code> is requested to create a streaming source for <code>FileFormat</code> data sources.</p> <p>While being created, <code>FileStreamSource</code> prints out the following INFO message to the logs (with the maxFilesPerBatch and maxFileAgeMs options):</p> <pre><code>maxFilesPerBatch = [maxFilesPerBatch], maxFileAgeMs = [maxFileAgeMs]\n</code></pre> <p><code>FileStreamSource</code> requests the FileStreamSourceLog for all files that are added to seenFiles internal registry. <code>FileStreamSource</code> requests the seenFiles internal registry to <code>purge</code> (remove aged entries).</p>","location":"datasources/file/FileStreamSource/#creating-instance"},{"title":"Options <p>Options are case-insensitive (so <code>cleanSource</code> and <code>CLEANSOURCE</code> are equivalent).</p>","text":"","location":"datasources/file/FileStreamSource/#options"},{"title":"cleanSource <p>How to clean up completed files.</p> <p>Available modes:</p> <ul> <li><code>archive</code></li> <li><code>delete</code></li> <li><code>off</code></li> </ul>","text":"","location":"datasources/file/FileStreamSource/#cleansource"},{"title":"fileNameOnly <p>Whether to check for new files on on the filename only (<code>true</code>) or the full path (<code>false</code>)</p> <p>Default: <code>false</code></p> <p>When enabled, <code>FileStreamSource</code> prints out the following WARN message to the logs:</p> <pre><code>'fileNameOnly' is enabled. Make sure your file names are unique (e.g. using UUID), otherwise, files with the same name but under different paths will be considered the same and causes data lost.\n</code></pre>","text":"","location":"datasources/file/FileStreamSource/#filenameonly"},{"title":"latestFirst <p>Whether to scan latest files first (<code>true</code>) or not (<code>false</code>)</p> <p>Default: <code>false</code></p> <p>When enabled, <code>FileStreamSource</code> prints out the following WARN message to the logs:</p> <pre><code>'latestFirst' is true. New files will be processed first, which may affect the watermark value. In addition, 'maxFileAge' will be ignored.\n</code></pre>","text":"","location":"datasources/file/FileStreamSource/#latestfirst"},{"title":"maxFileAgeMs <p>Maximum age of a file that can be found in this directory, before being ignored</p> <p>Default: <code>7d</code></p> <p>Uses time suffices: <code>us</code>, <code>ms</code>, <code>s</code>, <code>m</code>, <code>min</code>, <code>h</code>, <code>d</code>. No suffix is assumed to be in ms.</p>","text":"","location":"datasources/file/FileStreamSource/#maxfileagems"},{"title":"maxFilesPerTrigger <p>Maximum number of files per trigger (batch)</p>","text":"","location":"datasources/file/FileStreamSource/#maxfilespertrigger"},{"title":"sourceArchiveDir <p>Archive directory to move completed files to (for cleanSource set to <code>archive</code>)</p>","text":"","location":"datasources/file/FileStreamSource/#sourcearchivedir"},{"title":"SupportsAdmissionControl <p><code>FileStreamSource</code> is a SupportsAdmissionControl and controls the rate of data ingested.</p>","text":"","location":"datasources/file/FileStreamSource/#supportsadmissioncontrol"},{"title":"FileStreamSourceCleaner <p><code>FileStreamSource</code> may create a FileStreamSourceCleaner based on cleanSource option.</p>","text":"","location":"datasources/file/FileStreamSource/#filestreamsourcecleaner"},{"title":"FileStreamSourceLog <p><code>FileStreamSource</code> uses FileStreamSourceLog (for the given metadataPath).</p>","text":"","location":"datasources/file/FileStreamSource/#filestreamsourcelog"},{"title":"Latest Offset <p><code>FileStreamSource</code> tracks the latest offset in <code>metadataLogCurrentOffset</code> internal registry.</p>","text":"","location":"datasources/file/FileStreamSource/#latest-offset"},{"title":"Seen Files Registry <pre><code>seenFiles: SeenFilesMap\n</code></pre> <p><code>seenFiles</code> is...FIXME</p> <p><code>seenFiles</code> is used for...FIXME</p>","text":"","location":"datasources/file/FileStreamSource/#seen-files-registry"},{"title":"Committing <pre><code>commit(\n  end: Offset): Unit\n</code></pre> <p><code>commit</code> is...FIXME</p> <p><code>commit</code> is part of the Source abstraction.</p>","text":"","location":"datasources/file/FileStreamSource/#committing"},{"title":"getDefaultReadLimit <pre><code>getDefaultReadLimit: ReadLimit\n</code></pre> <p><code>getDefaultReadLimit</code> is...FIXME</p> <p><code>getDefaultReadLimit</code> is part of the SupportsAdmissionControl abstraction.</p>","text":"","location":"datasources/file/FileStreamSource/#getdefaultreadlimit"},{"title":"getOffset <pre><code>getOffset: Option[Offset]\n</code></pre> <p><code>getOffset</code> simply throws an <code>UnsupportedOperationException</code>:</p> <pre><code>latestOffset(Offset, ReadLimit) should be called instead of this method\n</code></pre> <p><code>getOffset</code> is part of the Source abstraction.</p>","text":"","location":"datasources/file/FileStreamSource/#getoffset"},{"title":"Generating DataFrame for Streaming Batch <pre><code>getBatch(\n  start: Option[Offset],\n  end: Offset): DataFrame\n</code></pre> <p><code>getBatch</code>...FIXME</p> <p><code>FileStreamSource.getBatch</code> asks &lt;&gt; for the batch. <p>You should see the following INFO and DEBUG messages in the logs:</p> <pre><code>Processing ${files.length} files from ${startId + 1}:$endId\nStreaming ${files.mkString(\", \")}\n</code></pre> <p>The method to create a result batch is given at instantiation time (as <code>dataFrameBuilder</code> constructor parameter).</p> <p><code>getBatch</code> is part of the Source abstraction.</p>","text":"","location":"datasources/file/FileStreamSource/#generating-dataframe-for-streaming-batch"},{"title":"fetchMaxOffset <pre><code>fetchMaxOffset(limit: ReadLimit): FileStreamSourceOffset\n</code></pre> <p><code>fetchMaxOffset</code>...FIXME</p> <p><code>fetchMaxOffset</code> is used for latestOffset.</p>","text":"","location":"datasources/file/FileStreamSource/#fetchmaxoffset"},{"title":"fetchAllFiles <pre><code>fetchAllFiles(): Seq[(String, Long)]\n</code></pre> <p><code>fetchAllFiles</code>...FIXME</p> <p><code>fetchAllFiles</code> is used for fetchMaxOffset.</p>","text":"","location":"datasources/file/FileStreamSource/#fetchallfiles"},{"title":"latestOffset <pre><code>latestOffset(\n  startOffset: streaming.Offset,\n  limit: ReadLimit): streaming.Offset\n</code></pre> <p><code>latestOffset</code>...FIXME</p> <p><code>latestOffset</code> is part of the SparkDataStream abstraction.</p>","text":"","location":"datasources/file/FileStreamSource/#latestoffset"},{"title":"Stopping Streaming Source <pre><code>stop(): Unit\n</code></pre> <p><code>stop</code>...FIXME</p> <p><code>stop</code> is part of the SupportsAdmissionControl abstraction.</p>","text":"","location":"datasources/file/FileStreamSource/#stopping-streaming-source"},{"title":"allFilesUsingInMemoryFileIndex <pre><code>allFilesUsingInMemoryFileIndex(): Seq[FileStatus]\n</code></pre> <p><code>allFilesUsingInMemoryFileIndex</code> is...FIXME</p> <p><code>allFilesUsingInMemoryFileIndex</code> is used for fetchAllFiles.</p>","text":"","location":"datasources/file/FileStreamSource/#allfilesusinginmemoryfileindex"},{"title":"allFilesUsingMetadataLogFileIndex <pre><code>allFilesUsingMetadataLogFileIndex(): Seq[FileStatus]\n</code></pre> <p><code>allFilesUsingMetadataLogFileIndex</code> is...FIXME</p> <p><code>allFilesUsingMetadataLogFileIndex</code> is used for fetchAllFiles</p>","text":"","location":"datasources/file/FileStreamSource/#allfilesusingmetadatalogfileindex"},{"title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.streaming.FileStreamSource</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.streaming.FileStreamSource=ALL\n</code></pre> <p>Refer to Logging.</p>","text":"","location":"datasources/file/FileStreamSource/#logging"},{"title":"FileStreamSourceCleaner","text":"<p><code>FileStreamSourceCleaner</code> is...FIXME</p>","location":"datasources/file/FileStreamSourceCleaner/"},{"title":"FileStreamSourceLog","text":"<p><code>FileStreamSourceLog</code> is a concrete CompactibleFileStreamLog (of <code>FileEntry</code> metadata) of FileStreamSource.</p> <p><code>FileStreamSourceLog</code> uses a fixed-size &lt;&gt; of metadata of compaction batches. <p>[[defaultCompactInterval]] <code>FileStreamSourceLog</code> uses &lt;&gt; configuration property (default: <code>10</code>) for the default compaction interval. <p>[[fileCleanupDelayMs]] <code>FileStreamSourceLog</code> uses &lt;&gt; configuration property (default: <code>10</code> minutes) for the fileCleanupDelayMs. <p>[[isDeletingExpiredLog]] <code>FileStreamSourceLog</code> uses &lt;&gt; configuration property (default: <code>true</code>) for the isDeletingExpiredLog.","location":"datasources/file/FileStreamSourceLog/"},{"title":"Creating Instance","text":"<p><code>FileStreamSourceLog</code> (like the parent CompactibleFileStreamLog) takes the following to be created:</p> <ul> <li>[[metadataLogVersion]] Metadata version</li> <li>[[sparkSession]] <code>SparkSession</code></li> <li>[[path]] Path of the metadata log directory</li> </ul> <p>=== [[add]] Storing (Adding) Metadata of Streaming Batch -- <code>add</code> Method</p>","location":"datasources/file/FileStreamSourceLog/#creating-instance"},{"title":"[source, scala]","text":"<p>add(   batchId: Long,   logs: Array[FileEntry]): Boolean</p>  <p><code>add</code> requests the parent <code>CompactibleFileStreamLog</code> to store metadata (possibly compacting logs if the batch is compaction).</p> <p>If so (and this is a compation batch), <code>add</code> adds the batch and the logs to &lt;&gt; internal registry (and possibly removing the eldest entry if the size is above the &lt;&gt;). <p><code>add</code> is part of the MetadataLog abstraction.</p> <p>=== [[get]][[get-range]] <code>get</code> Method</p>","location":"datasources/file/FileStreamSourceLog/#source-scala"},{"title":"[source, scala]","text":"<p>get(   startId: Option[Long],   endId: Option[Long]): Array[(Long, Array[FileEntry])]</p>  <p><code>get</code>...FIXME</p> <p><code>get</code> is part of the MetadataLog abstraction.</p>","location":"datasources/file/FileStreamSourceLog/#source-scala_1"},{"title":"Internal Properties","text":"<p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| cacheSize a| [[cacheSize]] Size of the &lt;&gt; that is exactly the compact interval <p>Used when the &lt;&gt; is requested to add a new entry in &lt;&gt; and &lt;&gt; a compaction batch <p>| fileEntryCache a| [[fileEntryCache]] Metadata of a streaming batch (<code>FileEntry</code>) per batch ID (<code>LinkedHashMap[Long, Array[FileEntry]]</code>) of size configured using the &lt;&gt; <ul> <li>New entry added for a compaction batch when &lt;&gt;  <p>Used when &lt;&gt; (for a compaction batch) <p>|===</p>","location":"datasources/file/FileStreamSourceLog/#internal-properties"},{"title":"ManifestFileCommitProtocol","text":"<p><code>ManifestFileCommitProtocol</code> is a <code>FileCommitProtocol</code> for tracking valid files (per micro-batch) in FileStreamSinkLog.</p>  <p>The Internals of Apache Spark</p> <p>Learn more on FileCommitProtocol in The Internals of Apache Spark.</p>","location":"datasources/file/ManifestFileCommitProtocol/"},{"title":"Creating Instance","text":"<p><code>ManifestFileCommitProtocol</code> takes the following to be created:</p> <ul> <li> Job ID (unused) <li> Path to write the output to  <p><code>ManifestFileCommitProtocol</code> is created\u00a0when <code>FileStreamSink</code> is requested to add a batch (which is every micro-batch).</p>","location":"datasources/file/ManifestFileCommitProtocol/#creating-instance"},{"title":"FileStreamSinkLog <p><code>ManifestFileCommitProtocol</code> is given a FileStreamSinkLog when setting up the manifest options for a micro-batch (right after having been created).</p> <p><code>FileStreamSinkLog</code> is used to add the SinkFileStatuses (in a micro-batch) when <code>ManifestFileCommitProtocol</code> is requested to commit a write job.</p>","text":"","location":"datasources/file/ManifestFileCommitProtocol/#filestreamsinklog"},{"title":"Setting Up Manifest Options <pre><code>setupManifestOptions(\n  fileLog: FileStreamSinkLog,\n  batchId: Long): Unit\n</code></pre> <p><code>setupManifestOptions</code> assigns the FileStreamSinkLog and batchId.</p> <p><code>setupManifestOptions</code> is used when <code>FileStreamSink</code> is requested to add a batch (right after having been created).</p>","text":"","location":"datasources/file/ManifestFileCommitProtocol/#setting-up-manifest-options"},{"title":"Setting Up Job <pre><code>setupJob(\n  jobContext: JobContext): Unit\n</code></pre> <p><code>setupJob</code> initializes pendingCommitFiles to be an empty collection of Hadoop Paths.</p> <p><code>setupJob</code> is part of the <code>FileCommitProtocol</code> (Spark SQL) abstraction.</p>","text":"","location":"datasources/file/ManifestFileCommitProtocol/#setting-up-job"},{"title":"Setting Up Task <pre><code>setupTask(\n  taskContext: TaskAttemptContext): Unit\n</code></pre> <p><code>setupTask</code> initializes addedFiles to be an empty collection of file locations (?)</p> <p><code>setupTask</code> is part of the <code>FileCommitProtocol</code> (Spark SQL) abstraction.</p>","text":"","location":"datasources/file/ManifestFileCommitProtocol/#setting-up-task"},{"title":"newTaskTempFile <pre><code>newTaskTempFile(\n  taskContext: TaskAttemptContext,\n  dir: Option[String],\n  ext: String): String\n</code></pre> <p><code>newTaskTempFile</code> creates a temporary file <code>part-[split]-[uuid][ext]</code> in the optional <code>dir</code> location or the path and adds it to addedFiles internal registry.</p> <p><code>newTaskTempFile</code> is part of the <code>FileCommitProtocol</code> (Spark SQL) abstraction.</p>","text":"","location":"datasources/file/ManifestFileCommitProtocol/#newtasktempfile"},{"title":"Task Committed <pre><code>onTaskCommit(\n  taskCommit: TaskCommitMessage): Unit\n</code></pre> <p><code>onTaskCommit</code> adds the SinkFileStatuss from the given <code>taskCommits</code> to pendingCommitFiles internal registry.</p> <p><code>onTaskCommit</code> is part of the <code>FileCommitProtocol</code> (Spark SQL) abstraction.</p>","text":"","location":"datasources/file/ManifestFileCommitProtocol/#task-committed"},{"title":"Committing Task <pre><code>commitTask(\n  taskContext: TaskAttemptContext): TaskCommitMessage\n</code></pre> <p><code>commitTask</code> creates a <code>TaskCommitMessage</code> with SinkFileStatuses for every added file.</p> <p><code>commitTask</code> is part of the <code>FileCommitProtocol</code> (Spark SQL) abstraction.</p>","text":"","location":"datasources/file/ManifestFileCommitProtocol/#committing-task"},{"title":"Aborting Task <pre><code>abortTask(\n  taskContext: TaskAttemptContext): Unit\n</code></pre> <p><code>abortTask</code> deletes added files.</p> <p><code>abortTask</code> is part of the <code>FileCommitProtocol</code> (Spark SQL) abstraction.</p>","text":"","location":"datasources/file/ManifestFileCommitProtocol/#aborting-task"},{"title":"Committing Job <pre><code>commitJob(\n  jobContext: JobContext,\n  taskCommits: Seq[TaskCommitMessage]): Unit\n</code></pre> <p><code>commitJob</code> takes SinkFileStatuss from the given <code>taskCommits</code>.</p> <p>In the end, <code>commitJob</code> requests the FileStreamSinkLog to add the <code>SinkFileStatus</code>s as the batchId. If successful (<code>true</code>), <code>commitJob</code> prints out the following INFO message to the logs:</p> <pre><code>Committed batch [batchId]\n</code></pre> <p>Otherwise, when failed (<code>false</code>), <code>commitJob</code> throws an <code>IllegalStateException</code>:</p> <pre><code>Race while writing batch [batchId]\n</code></pre> <p><code>commitJob</code> is part of the <code>FileCommitProtocol</code> (Spark SQL) abstraction.</p>","text":"","location":"datasources/file/ManifestFileCommitProtocol/#committing-job"},{"title":"Aborting Job <pre><code>abortJob(\n  jobContext: JobContext): Unit\n</code></pre> <p><code>abortJob</code> simply tries to remove all pendingCommitFiles if there are any and clear it up.</p> <p><code>abortJob</code> is part of the <code>FileCommitProtocol</code> (Spark SQL) abstraction.</p>","text":"","location":"datasources/file/ManifestFileCommitProtocol/#aborting-job"},{"title":"MetadataLogFileIndex","text":"<p><code>MetadataLogFileIndex</code> is a <code>PartitioningAwareFileIndex</code> of metadata log files (generated by FileStreamSink).</p>  <p>Tip</p> <p>Learn more about PartitioningAwareFileIndex in The Internals of Spark SQL online book.</p>","location":"datasources/file/MetadataLogFileIndex/"},{"title":"Creating Instance","text":"<p><code>MetadataLogFileIndex</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> <li> Hadoop Path <li> Parameters (<code>Map[String, String]</code>) <li> User-Defined Schema (<code>Option[StructType]</code>)  <p><code>MetadataLogFileIndex</code> is created\u00a0when:</p> <ul> <li><code>DataSource</code> is requested to resolveRelation (for <code>FileFormat</code> streaming data sources)</li> <li><code>FileTable</code> is requested for a <code>PartitioningAwareFileIndex</code> (for <code>FileFormat</code> streaming data sources)</li> <li><code>FileStreamSource</code> is requested to allFilesUsingMetadataLogFileIndex</li> </ul> <p>While being created, <code>MetadataLogFileIndex</code> prints out the following INFO message to the logs (with the metadataDirectory):</p> <pre><code>Reading streaming file log from [metadataDirectory]\n</code></pre>","location":"datasources/file/MetadataLogFileIndex/#creating-instance"},{"title":"Metadata Directory <pre><code>metadataDirectory: Path\n</code></pre> <p><code>metadataDirectory</code> is a Hadoop Path of Metadata Directory.</p> <p><code>metadataDirectory</code> is a _spark_metadata directory in the given path.</p> <p><code>metadataDirectory</code>\u00a0is used to create a FileStreamSinkLog.</p>","text":"","location":"datasources/file/MetadataLogFileIndex/#metadata-directory"},{"title":"FileStreamSinkLog <pre><code>metadataLog: FileStreamSinkLog\n</code></pre> <p><code>metadataLog</code> is a FileStreamSinkLog with the Metadata Directory.</p> <p><code>metadataLog</code>\u00a0is used for metadata log files.</p>","text":"","location":"datasources/file/MetadataLogFileIndex/#filestreamsinklog"},{"title":"Metadata Log Files <pre><code>allFilesFromLog: Array[FileStatus]\n</code></pre> <p><code>allFilesFromLog</code> requests the FileStreamSinkLog for all files that are in turn requested for their representation as a Hadoop FileStatus.</p> <p><code>allFilesFromLog</code>\u00a0is used for leafFiles and leafDirToChildrenFiles.</p>","text":"","location":"datasources/file/MetadataLogFileIndex/#metadata-log-files"},{"title":"Leaf Files <pre><code>leafFiles: mutable.LinkedHashMap[Path, FileStatus]\n</code></pre> <p><code>leafFiles</code>...FIXME</p> <p><code>leafFiles</code>\u00a0is part of the <code>PartitioningAwareFileIndex</code> abstraction (Spark SQL).</p>","text":"","location":"datasources/file/MetadataLogFileIndex/#leaf-files"},{"title":"leafDirToChildrenFiles <pre><code>leafDirToChildrenFiles: Map[Path, Array[FileStatus]]\n</code></pre> <p><code>leafDirToChildrenFiles</code>...FIXME</p> <p><code>leafDirToChildrenFiles</code>\u00a0is part of the <code>PartitioningAwareFileIndex</code> abstraction (Spark SQL).</p>","text":"","location":"datasources/file/MetadataLogFileIndex/#leafdirtochildrenfiles"},{"title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.streaming.MetadataLogFileIndex</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.streaming.MetadataLogFileIndex=ALL\n</code></pre> <p>Refer to Logging.</p>","text":"","location":"datasources/file/MetadataLogFileIndex/#logging"},{"title":"SinkFileStatus","text":"<p>[[creating-instance]] <code>SinkFileStatus</code> represents the status of files of FileStreamSink (and the type of the metadata of FileStreamSinkLog):</p> <ul> <li>[[path]] Path</li> <li>[[size]] Size</li> <li>[[isDir]] <code>isDir</code> flag</li> <li>[[modificationTime]] Modification time</li> <li>[[blockReplication]] Block replication</li> <li>[[blockSize]] Block size</li> <li>[[action]] Action (either add or delete)</li> </ul> <p>=== [[toFileStatus]] <code>toFileStatus</code> Method</p>","location":"datasources/file/SinkFileStatus/"},{"title":"[source, scala]","text":"","location":"datasources/file/SinkFileStatus/#source-scala"},{"title":"toFileStatus: FileStatus","text":"<p><code>toFileStatus</code> simply creates a new Hadoop FileStatus.</p> <p>NOTE: <code>toFileStatus</code> is used exclusively when <code>MetadataLogFileIndex</code> is created.</p> <p>=== [[apply]] Creating SinkFileStatus Instance</p>","location":"datasources/file/SinkFileStatus/#tofilestatus-filestatus"},{"title":"[source, scala]","text":"","location":"datasources/file/SinkFileStatus/#source-scala_1"},{"title":"apply(f: FileStatus): SinkFileStatus","text":"<p><code>apply</code> simply creates a new &lt;&gt; (with add action). <p><code>apply</code> is used when <code>ManifestFileCommitProtocol</code> is requested to commitTask.</p>","location":"datasources/file/SinkFileStatus/#applyf-filestatus-sinkfilestatus"},{"title":"Foreach Data Source","text":"","location":"datasources/foreach/"},{"title":"ForeachSink","text":"<p><code>ForeachSink</code> is a typed streaming sink that passes rows (of the type <code>T</code>) to ForeachWriter (one record at a time per partition).</p>  <p>Note</p> <p><code>ForeachSink</code> is assigned a <code>ForeachWriter</code> when <code>DataStreamWriter</code> is started.</p>  <p><code>ForeachSink</code> is used exclusively in foreach operator.</p> <pre><code>val records = spark.\n  readStream\n  format(\"text\").\n  load(\"server-logs/*.out\").\n  as[String]\n\nimport org.apache.spark.sql.ForeachWriter\nval writer = new ForeachWriter[String] {\n  override def open(partitionId: Long, version: Long) = true\n  override def process(value: String) = println(value)\n  override def close(errorOrNull: Throwable) = {}\n}\n\nrecords.writeStream\n  .queryName(\"server-logs processor\")\n  .foreach(writer)\n  .start\n</code></pre> <p>Internally, <code>addBatch</code> (the only method from the &lt;&gt;) takes records from the input spark-sql-dataframe.md[DataFrame] (as <code>data</code>), transforms them to expected type <code>T</code> (of this <code>ForeachSink</code>) and (now as a spark-sql-dataset.md[Dataset]) spark-sql-dataset.md#foreachPartition[processes each partition].","location":"datasources/foreach/ForeachSink/"},{"title":"[source, scala]","text":"","location":"datasources/foreach/ForeachSink/#source-scala"},{"title":"addBatch(batchId: Long, data: DataFrame): Unit","text":"<p><code>addBatch</code> then opens the constructor's datasources/foreach/ForeachWriter.md[ForeachWriter] (for the spark-taskscheduler-taskcontext.md#getPartitionId[current partition] and the input batch) and passes the records to process (one at a time per partition).</p> <p>CAUTION: FIXME Why does Spark track whether the writer failed or not? Why couldn't it <code>finally</code> and do <code>close</code>?</p> <p>CAUTION: FIXME Can we have a constant for <code>\"foreach\"</code> for <code>source</code> in <code>DataStreamWriter</code>?</p>","location":"datasources/foreach/ForeachSink/#addbatchbatchid-long-data-dataframe-unit"},{"title":"ForeachWriter","text":"<p><code>ForeachWriter</code> is the &lt;&gt; for a foreach writer that is a streaming format that controls streaming writes.  <p>Note</p> <p><code>ForeachWriter</code> is set using DataStreamWriter.foreach operator.</p>  <pre><code>val foreachWriter = new ForeachWriter[String] { ... }\nstreamingQuery.\n  writeStream.\n  foreach(foreachWriter).\n  start\n</code></pre> <p>=== [[contract]] ForeachWriter Contract</p>","location":"datasources/foreach/ForeachWriter/"},{"title":"[source, scala]","text":"<p>package org.apache.spark.sql</p> <p>abstract class ForeachWriter[T] {   def open(partitionId: Long, version: Long): Boolean   def process(value: T): Unit   def close(errorOrNull: Throwable): Unit }</p>  <p>.ForeachWriter Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| [[open]] <code>open</code> | Used when...</p> <p>| [[process]] <code>process</code> | Used when...</p> <p>| [[close]] <code>close</code> | Used when... |===</p>","location":"datasources/foreach/ForeachWriter/#source-scala"},{"title":"ForeachWriterProvider","text":"<p>== [[ForeachWriterProvider]] ForeachWriterProvider</p> <p><code>ForeachWriterProvider</code> is...FIXME</p>","location":"datasources/foreach/ForeachWriterProvider/"},{"title":"Kafka Data Source","text":"<p>Kafka Data Source is the streaming data source for Apache Kafka in Spark Structured Streaming.</p> <p>Kafka Data Source provides a streaming source and a streaming sink for micro-batch and continuous stream processing.</p>","location":"datasources/kafka/"},{"title":"spark-sql-kafka-0-10 External Module <p>Kafka Data Source is part of the spark-sql-kafka-0-10 external module that is distributed with the official distribution of Apache Spark, but it is not on the CLASSPATH by default.</p> <p>You should define <code>spark-sql-kafka-0-10</code> module as part of the build definition in your Spark project, e.g. as a <code>libraryDependency</code> in <code>build.sbt</code> for sbt:</p> <pre><code>libraryDependencies += \"org.apache.spark\" %% \"spark-sql-kafka-0-10\" % \"3.3.0\"\n</code></pre> <p>For Spark environments like <code>spark-submit</code> (and \"derivatives\" like <code>spark-shell</code>), you should use <code>--packages</code> command-line option:</p> <pre><code>./bin/spark-shell \\\n  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\n</code></pre>  <p>Note</p> <p>Replace the version of <code>spark-sql-kafka-0-10</code> module (e.g. <code>3.3.0</code> above) with one of the available versions found at The Central Repository's Search that matches your version of Apache Spark.</p>","text":"","location":"datasources/kafka/#spark-sql-kafka-0-10-external-module"},{"title":"Streaming Source <p>Kafka data source can load streaming data (reading records) from one or more Kafka topics.</p> <pre><code>val records = spark\n  .readStream\n  .format(\"kafka\")\n  .option(\"subscribePattern\", \"\"\"topic-\\d{2}\"\"\") // topics with two digits at the end\n  .option(\"kafka.bootstrap.servers\", \":9092\")\n  .load\n</code></pre> <p>Kafka data source supports many options for reading.</p> <p>Kafka data source for reading is available through KafkaSourceProvider that is a MicroBatchStream and ContinuousReadSupport for micro-batch and continuous stream processing, respectively.</p>","text":"","location":"datasources/kafka/#streaming-source"},{"title":"Predefined (Fixed) Schema <p>Kafka Data Source uses a predefined (fixed) schema.</p>    Name Type     key BinaryType   value BinaryType   topic StringType   partition IntegerType   offset LongType   timestamp TimestampType   timestampType IntegerType    <pre><code>scala&gt; records.printSchema\nroot\n |-- key: binary (nullable = true)\n |-- value: binary (nullable = true)\n |-- topic: string (nullable = true)\n |-- partition: integer (nullable = true)\n |-- offset: long (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n |-- timestampType: integer (nullable = true)\n</code></pre>","text":"","location":"datasources/kafka/#predefined-fixed-schema"},{"title":"Column.cast Operator","text":"<p>Use <code>Column.cast</code> operator to cast <code>BinaryType</code> to a <code>StringType</code> (for <code>key</code> and <code>value</code> columns).</p> <pre><code>scala&gt; :type records\norg.apache.spark.sql.DataFrame\n\nval values = records\n  .select($\"value\" cast \"string\") // deserializing values\nscala&gt; values.printSchema\nroot\n|-- value: string (nullable = true)\n</code></pre>","location":"datasources/kafka/#columncast-operator"},{"title":"Streaming Sink <p>Kafka data source can write streaming data (the result of executing a streaming query) to one or more Kafka topics.</p> <pre><code>val sq = records\n  .writeStream\n  .format(\"kafka\")\n  .option(\"kafka.bootstrap.servers\", \":9092\")\n  .option(\"topic\", \"kafka2console-output\")\n  .option(\"checkpointLocation\", \"checkpointLocation-kafka2console\")\n  .start\n</code></pre> <p>Internally, the kafka data source format for writing is available through KafkaSourceProvider.</p>","text":"","location":"datasources/kafka/#streaming-sink"},{"title":"Micro-Batch Stream Processing <p>Kafka Data Source supports Micro-Batch Stream Processing using KafkaMicroBatchReader.</p> <pre><code>import org.apache.spark.sql.streaming.Trigger\nimport scala.concurrent.duration._\nval sq = spark\n  .readStream\n  .format(\"kafka\")\n  .option(\"subscribepattern\", \"kafka2console.*\")\n  .option(\"kafka.bootstrap.servers\", \":9092\")\n  .load\n  .withColumn(\"value\", $\"value\" cast \"string\") // deserializing values\n  .writeStream\n  .format(\"console\")\n  .option(\"truncate\", false) // format-specific option\n  .option(\"checkpointLocation\", \"checkpointLocation-kafka2console\") // generic query option\n  .trigger(Trigger.ProcessingTime(30.seconds))\n  .queryName(\"kafka2console-microbatch\")\n  .start\n\n// In the end, stop the streaming query\nsq.stop\n</code></pre> <p>Kafka Data Source can assign a single task per Kafka partition (using KafkaOffsetRangeCalculator in Micro-Batch Stream Processing).</p> <p>Kafka Data Source can reuse a Kafka consumer (using KafkaMicroBatchReader in Micro-Batch Stream Processing).</p>","text":"","location":"datasources/kafka/#micro-batch-stream-processing"},{"title":"Continuous Stream Processing <p>Kafka Data Source supports Continuous Stream Processing using KafkaContinuousReader.</p> <pre><code>import org.apache.spark.sql.streaming.Trigger\nimport scala.concurrent.duration._\nval sq = spark\n  .readStream\n  .format(\"kafka\")\n  .option(\"subscribepattern\", \"kafka2console.*\")\n  .option(\"kafka.bootstrap.servers\", \":9092\")\n  .load\n  .withColumn(\"value\", $\"value\" cast \"string\") // convert bytes to string for display purposes\n  .writeStream\n  .format(\"console\")\n  .option(\"truncate\", false) // format-specific option\n  .option(\"checkpointLocation\", \"checkpointLocation-kafka2console\") // generic query option\n  .queryName(\"kafka2console-continuous\")\n  .trigger(Trigger.Continuous(10.seconds))\n  .start\n\n// In the end, stop the streaming query\nsq.stop\n</code></pre>","text":"","location":"datasources/kafka/#continuous-stream-processing"},{"title":"Configuration Options <p>Kafka data source supports options.</p>","text":"","location":"datasources/kafka/#configuration-options"},{"title":"Trigger.AvailableNow <p>Kafka data source supports Trigger.AvailableNow mode (as KafkaMicroBatchStream is a SupportsTriggerAvailableNow).</p>","text":"","location":"datasources/kafka/#triggeravailablenow"},{"title":"Logical Query Plan for Reading <p>When <code>DataStreamReader</code> is requested to load a dataset with kafka data source format, it creates a DataFrame with a StreamingRelationV2 leaf logical operator.</p> <pre><code>scala&gt; records.explain(extended = true)\n== Parsed Logical Plan ==\nStreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@1a366d0, kafka, Map(maxOffsetsPerTrigger -&gt; 1, startingOffsets -&gt; latest, subscribepattern -&gt; topic\\d, kafka.bootstrap.servers -&gt; :9092), [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@39b3de87,kafka,List(),None,List(),None,Map(maxOffsetsPerTrigger -&gt; 1, startingOffsets -&gt; latest, subscribepattern -&gt; topic\\d, kafka.bootstrap.servers -&gt; :9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n...\n</code></pre>","text":"","location":"datasources/kafka/#logical-query-plan-for-reading"},{"title":"Logical Query Plan for Writing <p>When <code>DataStreamWriter</code> is requested to start a streaming query with kafka data source format for writing, it requests the <code>StreamingQueryManager</code> to create a streaming query that in turn creates (a StreamingQueryWrapper with) a ContinuousExecution or a MicroBatchExecution for continuous and micro-batch stream processing, respectively.</p> <pre><code>scala&gt; sq.explain(extended = true)\n== Parsed Logical Plan ==\nWriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@42363db7\n+- Project [key#28 AS key#7, value#29 AS value#8, topic#30 AS topic#9, partition#31 AS partition#10, offset#32L AS offset#11L, timestamp#33 AS timestamp#12, timestampType#34 AS timestampType#13]\n   +- Streaming RelationV2 kafka[key#28, value#29, topic#30, partition#31, offset#32L, timestamp#33, timestampType#34] (Options: [subscribepattern=kafka2console.*,kafka.bootstrap.servers=:9092])\n</code></pre>","text":"","location":"datasources/kafka/#logical-query-plan-for-writing"},{"title":"Demo: Streaming Aggregation with Kafka Data Source <p>Check out Demo: Streaming Aggregation with Kafka Data Source.</p> <p>Use the following to publish events to Kafka.</p> <pre><code>// 1st streaming batch\n$ cat /tmp/1\n1,1,1\n15,2,1\n\n$ kafkacat -P -b localhost:9092 -t topic1 -l /tmp/1\n\n// Alternatively (and slower due to JVM bootup)\n$ cat /tmp/1 | ./bin/kafka-console-producer.sh --topic topic1 --broker-list localhost:9092\n</code></pre>","text":"","location":"datasources/kafka/#demo-streaming-aggregation-with-kafka-data-source"},{"title":"CachedKafkaConsumer","text":"<p>== [[CachedKafkaConsumer]] CachedKafkaConsumer</p> <p>CAUTION: FIXME</p> <p>=== [[poll]] <code>poll</code> Internal Method</p> <p>CAUTION: FIXME</p> <p>=== [[fetchData]] <code>fetchData</code> Internal Method</p> <p>CAUTION: FIXME</p>","location":"datasources/kafka/CachedKafkaConsumer/"},{"title":"ConsumerStrategy","text":"<p>== [[ConsumerStrategy]] ConsumerStrategy Contract for KafkaConsumer Providers</p> <p><code>ConsumerStrategy</code> is the &lt;&gt; for components that can &lt;&gt; using the given Kafka parameters. <p>[[contract]] [[createConsumer]] [source, scala]</p>","location":"datasources/kafka/ConsumerStrategy/"},{"title":"createConsumer(kafkaParams: java.util.Map[String, Object]): Consumer[Array[Byte], Array[Byte]]","text":"<p>[[available-consumerstrategies]] .Available ConsumerStrategies [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | ConsumerStrategy | createConsumer</p> <p>| [[AssignStrategy]] <code>AssignStrategy</code> | Uses ++http://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#assign(java.util.Collection)++[KafkaConsumer.assign(Collection partitions)] <p>| [[SubscribeStrategy]] <code>SubscribeStrategy</code> | Uses ++http://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#subscribe(java.util.Collection)++[KafkaConsumer.subscribe(Collection topics)] <p>| [[SubscribePatternStrategy]] <code>SubscribePatternStrategy</code> a| Uses ++http://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#subscribe(java.util.regex.Pattern,%20org.apache.kafka.clients.consumer.ConsumerRebalanceListener)++[KafkaConsumer.subscribe(Pattern pattern, ConsumerRebalanceListener listener)] with <code>NoOpConsumerRebalanceListener</code>.</p> <p>TIP: Refer to http://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html[java.util.regex.Pattern] for the format of supported topic subscription regex patterns. |===</p>","location":"datasources/kafka/ConsumerStrategy/#createconsumerkafkaparams-javautilmapstring-object-consumerarraybyte-arraybyte"},{"title":"KafkaBatch","text":"<p><code>KafkaBatch</code> is...FIXME</p>","location":"datasources/kafka/KafkaBatch/"},{"title":"KafkaContinuousInputPartition","text":"<p>== [[KafkaContinuousInputPartition]] KafkaContinuousInputPartition</p> <p><code>KafkaContinuousInputPartition</code> is...FIXME</p>","location":"datasources/kafka/KafkaContinuousInputPartition/"},{"title":"KafkaContinuousReader","text":"<p><code>KafkaContinuousReader</code> is a ContinuousReader for Kafka Data Source in Continuous Stream Processing.</p> <p><code>KafkaContinuousReader</code> is &lt;&gt; exclusively when <code>KafkaSourceProvider</code> is requested to create a ContinuousReader. <p>[[pollTimeoutMs]] [[kafkaConsumer.pollTimeoutMs]] <code>KafkaContinuousReader</code> uses kafkaConsumer.pollTimeoutMs configuration parameter (default: <code>512</code>) for KafkaContinuousInputPartitions when requested to &lt;&gt;. <p>[[logging]] [TIP] ==== Enable <code>INFO</code> or <code>WARN</code> logging levels for <code>org.apache.spark.sql.kafka010.KafkaContinuousReader</code> to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.kafka010.KafkaContinuousReader=INFO\n</code></pre>","location":"datasources/kafka/KafkaContinuousReader/"},{"title":"Refer to spark-sql-streaming-spark-logging.md[Logging].","text":"","location":"datasources/kafka/KafkaContinuousReader/#refer-to-spark-sql-streaming-spark-loggingmdlogging"},{"title":"Creating Instance","text":"<p><code>KafkaContinuousReader</code> takes the following to be created:</p> <ul> <li>[[offsetReader]] KafkaOffsetReader</li> <li>[[kafkaParams]] Kafka parameters (as <code>java.util.Map[String, Object]</code>)</li> <li>[[sourceOptions]] Source options (as <code>Map[String, String]</code>)</li> <li>[[metadataPath]] Metadata path</li> <li>[[initialOffsets]] Initial offsets</li> <li>[[failOnDataLoss]] <code>failOnDataLoss</code> flag</li> </ul> <p>=== [[planInputPartitions]] Plan Input Partitions -- <code>planInputPartitions</code> Method</p>","location":"datasources/kafka/KafkaContinuousReader/#creating-instance"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaContinuousReader/#source-scala"},{"title":"planInputPartitions(): java.util.List[InputPartition[InternalRow]]","text":"<p>NOTE: <code>planInputPartitions</code> is part of the <code>DataSourceReader</code> contract in Spark SQL for the number of <code>InputPartitions</code> to use as RDD partitions (when <code>DataSourceV2ScanExec</code> physical operator is requested for the partitions of the input RDD).</p> <p><code>planInputPartitions</code>...FIXME</p> <p>=== [[setStartOffset]] <code>setStartOffset</code> Method</p>","location":"datasources/kafka/KafkaContinuousReader/#planinputpartitions-javautillistinputpartitioninternalrow"},{"title":"[source, java]","text":"<p>setStartOffset(   start: Optional[Offset]): Unit</p>  <p><code>setStartOffset</code> is part of the ContinuousReader abstraction.</p> <p><code>setStartOffset</code>...FIXME</p> <p>=== [[deserializeOffset]] <code>deserializeOffset</code> Method</p>","location":"datasources/kafka/KafkaContinuousReader/#source-java"},{"title":"[source, java]","text":"<p>deserializeOffset(   json: String): Offset</p>  <p><code>deserializeOffset</code> is part of the ContinuousReader abstraction.</p> <p><code>deserializeOffset</code>...FIXME</p> <p>=== [[mergeOffsets]] <code>mergeOffsets</code> Method</p>","location":"datasources/kafka/KafkaContinuousReader/#source-java_1"},{"title":"[source, java]","text":"<p>mergeOffsets(   offsets: Array[PartitionOffset]): Offset</p>  <p><code>mergeOffsets</code> is part of the ContinuousReader abstraction.</p> <p><code>mergeOffsets</code>...FIXME</p>","location":"datasources/kafka/KafkaContinuousReader/#source-java_2"},{"title":"KafkaDataConsumer","text":"<p>== [[KafkaDataConsumer]] KafkaDataConsumer</p> <p><code>KafkaDataConsumer</code> is the &lt;&gt; of &lt;&gt; that use &lt;&gt; that can be &lt;&gt;. <p>[[contract]] .KafkaDataConsumer Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| internalConsumer a| [[internalConsumer]]</p>","location":"datasources/kafka/KafkaDataConsumer/"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaDataConsumer/#source-scala"},{"title":"internalConsumer: InternalKafkaConsumer","text":"<p>Used when...FIXME</p> <p>| release a| [[release]]</p>","location":"datasources/kafka/KafkaDataConsumer/#internalconsumer-internalkafkaconsumer"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaDataConsumer/#source-scala_1"},{"title":"release(): Unit","text":"<p>Used when...FIXME</p> <p>|===</p> <p>[[implementations]] .KafkaDataConsumers [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | KafkaDataConsumer | Description</p> <p>| CachedKafkaDataConsumer | [[CachedKafkaDataConsumer]]</p> <p>| NonCachedKafkaDataConsumer | [[NonCachedKafkaDataConsumer]]</p> <p>|===</p> <p>=== [[acquire]] Acquiring Cached KafkaDataConsumer for Partition -- <code>acquire</code> Object Method</p>","location":"datasources/kafka/KafkaDataConsumer/#release-unit"},{"title":"[source, scala]","text":"<p>acquire(   topicPartition: TopicPartition,   kafkaParams: ju.Map[String, Object],   useCache: Boolean ): KafkaDataConsumer</p>  <p><code>acquire</code>...FIXME</p> <p>NOTE: <code>acquire</code> is used when...FIXME</p> <p>=== [[get]] Getting Kafka Record -- <code>get</code> Method</p>","location":"datasources/kafka/KafkaDataConsumer/#source-scala_2"},{"title":"[source, scala]","text":"<p>get(   offset: Long,   untilOffset: Long,   pollTimeoutMs: Long,   failOnDataLoss: Boolean ): ConsumerRecord[Array[Byte], Array[Byte]]</p>  <p><code>get</code>...FIXME</p> <p>NOTE: <code>get</code> is used when...FIXME</p>","location":"datasources/kafka/KafkaDataConsumer/#source-scala_3"},{"title":"KafkaMicroBatchInputPartition","text":"<p><code>KafkaMicroBatchInputPartition</code> is an <code>InputPartition</code> (of <code>InternalRows</code>) that is used (&lt;&gt;) exclusively when <code>KafkaMicroBatchReader</code> is requested for input partitions (when <code>DataSourceV2ScanExec</code> physical operator is requested for the partitions of the input RDD). <p>[[creating-instance]] <code>KafkaMicroBatchInputPartition</code> takes the following to be created:</p> <ul> <li>[[offsetRange]] KafkaOffsetRange</li> <li>[[executorKafkaParams]] Kafka parameters used for Kafka clients on executors (<code>Map[String, Object]</code>)</li> <li>[[pollTimeoutMs]] Poll timeout (in ms)</li> <li>[[failOnDataLoss]] <code>failOnDataLoss</code> flag</li> <li>[[reuseKafkaConsumer]] <code>reuseKafkaConsumer</code> flag</li> </ul> <p>[[createPartitionReader]] <code>KafkaMicroBatchInputPartition</code> creates a KafkaMicroBatchInputPartitionReader when requested for a <code>InputPartitionReader[InternalRow]</code> (as a part of the <code>InputPartition</code> contract).</p> <p>[[preferredLocations]] <code>KafkaMicroBatchInputPartition</code> simply requests the given &lt;&gt; for the optional <code>preferredLoc</code> when requested for <code>preferredLocations</code> (as a part of the <code>InputPartition</code> contract).","location":"datasources/kafka/KafkaMicroBatchInputPartition/"},{"title":"KafkaMicroBatchInputPartitionReader","text":"<p><code>KafkaMicroBatchInputPartitionReader</code> is an <code>InputPartitionReader</code> (of <code>InternalRows</code>) that is &lt;&gt; exclusively when <code>KafkaMicroBatchInputPartition</code> is requested for one (as a part of the <code>InputPartition</code> contract).","location":"datasources/kafka/KafkaMicroBatchInputPartitionReader/"},{"title":"Creating Instance","text":"<p><code>KafkaMicroBatchInputPartitionReader</code> takes the following to be created:</p> <ul> <li>[[offsetRange]] KafkaOffsetRange</li> <li>[[executorKafkaParams]] Kafka parameters used for Kafka clients on executors (<code>Map[String, Object]</code>)</li> <li>[[pollTimeoutMs]] Poll timeout (in ms)</li> <li>[[failOnDataLoss]] <code>failOnDataLoss</code> flag</li> <li>[[reuseKafkaConsumer]] <code>reuseKafkaConsumer</code> flag</li> </ul> <p>NOTE: All the input arguments to create a <code>KafkaMicroBatchInputPartitionReader</code> are exactly the input arguments used to create a KafkaMicroBatchInputPartition.</p> <p><code>KafkaMicroBatchInputPartitionReader</code> initializes the &lt;&gt;. <p>=== [[next]] <code>next</code> Method</p>","location":"datasources/kafka/KafkaMicroBatchInputPartitionReader/#creating-instance"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaMicroBatchInputPartitionReader/#source-scala"},{"title":"next(): Boolean","text":"<p>NOTE: <code>next</code> is part of the <code>InputPartitionReader</code> contract to proceed to next record if available (<code>true</code>).</p> <p><code>next</code> checks whether the &lt;&gt; should &lt;&gt; or &lt;&gt; (i.e. &lt;&gt; is smaller than the untilOffset of the &lt;&gt;). <p>==== [[next-poll]] <code>next</code> Method -- KafkaDataConsumer Polls Records</p> <p>If so, <code>next</code> requests the &lt;&gt; to get (poll) records in the range of &lt;&gt; and the untilOffset (of the &lt;&gt;) with the given &lt;&gt; and &lt;&gt;. <p>With a new record, <code>next</code> requests the &lt;&gt; to convert (<code>toUnsafeRow</code>) the record to be the &lt;&gt;. <code>next</code> sets the &lt;&gt; as the offset of the record incremented. <code>next</code> returns <code>true</code>. <p>With no new record, <code>next</code> simply returns <code>false</code>.</p> <p>==== [[next-no-poll]] <code>next</code> Method -- No Polling</p> <p>If the &lt;&gt; is equal or larger than the untilOffset (of the &lt;&gt;), <code>next</code> simply returns <code>false</code>. <p>=== [[close]] Closing (Releasing KafkaDataConsumer) -- <code>close</code> Method</p>","location":"datasources/kafka/KafkaMicroBatchInputPartitionReader/#next-boolean"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaMicroBatchInputPartitionReader/#source-scala_1"},{"title":"close(): Unit","text":"<p>NOTE: <code>close</code> is part of the Java Closeable contract to release resources.</p> <p><code>close</code> simply requests the &lt;&gt; to <code>release</code>. <p>=== [[resolveRange]] <code>resolveRange</code> Internal Method</p>","location":"datasources/kafka/KafkaMicroBatchInputPartitionReader/#close-unit"},{"title":"[source, scala]","text":"<p>resolveRange(   range: KafkaOffsetRange): KafkaOffsetRange</p>  <p><code>resolveRange</code>...FIXME</p> <p>NOTE: <code>resolveRange</code> is used exclusively when <code>KafkaMicroBatchInputPartitionReader</code> is &lt;&gt; (and initializes the &lt;&gt; internal property). <p>=== [[internal-properties]] Internal Properties</p> <p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| consumer a| [[consumer]] KafkaDataConsumer for the partition (per &lt;&gt;) <p>Used in &lt;&gt;, &lt;&gt;, and &lt;&gt; <p>| converter a| [[converter]] <code>KafkaRecordToUnsafeRowConverter</code></p> <p>| nextOffset a| [[nextOffset]] Next offset</p> <p>| nextRow a| [[nextRow]] Next <code>UnsafeRow</code></p> <p>| rangeToRead a| [[rangeToRead]] <code>KafkaOffsetRange</code></p> <p>|===</p>","location":"datasources/kafka/KafkaMicroBatchInputPartitionReader/#source-scala_2"},{"title":"KafkaMicroBatchReader","text":"<p>MicroBatchReader is gone in 3.0.0</p> <p>No longer used in Spark Structured Streaming and the page will soon be removed.</p>  <p><code>KafkaMicroBatchReader</code> is the <code>MicroBatchReader</code> for kafka data source for Micro-Batch Stream Processing.</p> <p><code>KafkaMicroBatchReader</code> is created when <code>KafkaSourceProvider</code> is requested to create a MicroBatchReader.</p> <p>[[pollTimeoutMs]] <code>KafkaMicroBatchReader</code> uses the DataSourceOptions to access the kafkaConsumer.pollTimeoutMs option (default: <code>spark.network.timeout</code> or <code>120s</code>).</p> <p>[[maxOffsetsPerTrigger]] <code>KafkaMicroBatchReader</code> uses the DataSourceOptions to access the maxOffsetsPerTrigger option (default: <code>(undefined)</code>).</p> <p><code>KafkaMicroBatchReader</code> uses the Kafka properties for executors to create KafkaMicroBatchInputPartitions when requested to planInputPartitions.</p>","location":"datasources/kafka/KafkaMicroBatchReader/"},{"title":"Creating Instance","text":"<p><code>KafkaMicroBatchReader</code> takes the following to be created:</p> <ul> <li>[[kafkaOffsetReader]] KafkaOffsetReader</li> <li>[[executorKafkaParams]] Kafka properties for executors (<code>Map[String, Object]</code>)</li> <li>[[options]] <code>DataSourceOptions</code></li> <li>[[metadataPath]] Metadata Path</li> <li>[[startingOffsets]] Desired starting KafkaOffsetRangeLimit</li> <li>[[failOnDataLoss]] failOnDataLoss option</li> </ul> <p><code>KafkaMicroBatchReader</code> initializes the &lt;&gt;. <p>=== [[readSchema]] <code>readSchema</code> Method</p>","location":"datasources/kafka/KafkaMicroBatchReader/#creating-instance"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaMicroBatchReader/#source-scala"},{"title":"readSchema(): StructType","text":"<p>NOTE: <code>readSchema</code> is part of the <code>DataSourceReader</code> contract to...FIXME.</p> <p><code>readSchema</code> simply returns the predefined fixed schema.</p> <p>=== [[planInputPartitions]] Plan Input Partitions -- <code>planInputPartitions</code> Method</p>","location":"datasources/kafka/KafkaMicroBatchReader/#readschema-structtype"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaMicroBatchReader/#source-scala_1"},{"title":"planInputPartitions(): java.util.List[InputPartition[InternalRow]]","text":"<p>NOTE: <code>planInputPartitions</code> is part of the <code>DataSourceReader</code> contract in Spark SQL for the number of <code>InputPartitions</code> to use as RDD partitions (when <code>DataSourceV2ScanExec</code> physical operator is requested for the partitions of the input RDD).</p> <p><code>planInputPartitions</code> first finds the new partitions (<code>TopicPartitions</code> that are in the &lt;&gt; but not in the &lt;&gt;) and requests the &lt;&gt; to fetch their earliest offsets. <p><code>planInputPartitions</code> prints out the following INFO message to the logs:</p> <pre><code>Partitions added: [newPartitionInitialOffsets]\n</code></pre> <p><code>planInputPartitions</code> then prints out the following DEBUG message to the logs:</p> <pre><code>TopicPartitions: [comma-separated list of TopicPartitions]\n</code></pre> <p><code>planInputPartitions</code> requests the &lt;&gt; for &lt;&gt; (given the &lt;&gt; and the newly-calculated <code>newPartitionInitialOffsets</code> as the <code>fromOffsets</code>, the &lt;&gt; as the <code>untilOffsets</code>, and the &lt;&gt;). <p>In the end, <code>planInputPartitions</code> creates a KafkaMicroBatchInputPartition for every offset range (with the &lt;&gt;, the &lt;&gt;, the &lt;&gt; flag and whether to reuse a Kafka consumer among Spark tasks). <p><code>planInputPartitions</code> &lt;&gt; when...FIXME <p>=== [[getSortedExecutorList]] Available Executors in Spark Cluster (Sorted By Host and Executor ID in Descending Order) -- <code>getSortedExecutorList</code> Internal Method</p>","location":"datasources/kafka/KafkaMicroBatchReader/#planinputpartitions-javautillistinputpartitioninternalrow"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaMicroBatchReader/#source-scala_2"},{"title":"getSortedExecutorList(): Array[String]","text":"<p><code>getSortedExecutorList</code> requests the <code>BlockManager</code> to request the <code>BlockManagerMaster</code> to get the peers (the other nodes in a Spark cluster), creates a <code>ExecutorCacheTaskLocation</code> for every pair of host and executor ID, and in the end, sort it in descending order.</p> <p>NOTE: <code>getSortedExecutorList</code> is used exclusively when <code>KafkaMicroBatchReader</code> is requested to &lt;&gt; (and calculates offset ranges). <p>=== [[getOrCreateInitialPartitionOffsets]] <code>getOrCreateInitialPartitionOffsets</code> Internal Method</p>","location":"datasources/kafka/KafkaMicroBatchReader/#getsortedexecutorlist-arraystring"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaMicroBatchReader/#source-scala_3"},{"title":"getOrCreateInitialPartitionOffsets(): PartitionOffsetMap","text":"<p><code>getOrCreateInitialPartitionOffsets</code>...FIXME</p> <p>NOTE: <code>getOrCreateInitialPartitionOffsets</code> is used exclusively for the &lt;&gt; internal registry. <p>=== [[internal-properties]] Internal Properties</p> <p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| endPartitionOffsets a| [[endPartitionOffsets]] Ending offsets for the assigned partitions (<code>Map[TopicPartition, Long]</code>)</p> <p>Used when...FIXME</p> <p>| initialPartitionOffsets a| [[initialPartitionOffsets]]</p>","location":"datasources/kafka/KafkaMicroBatchReader/#getorcreateinitialpartitionoffsets-partitionoffsetmap"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaMicroBatchReader/#source-scala_4"},{"title":"initialPartitionOffsets: Map[TopicPartition, Long]","text":"<p>| rangeCalculator a| [[rangeCalculator]] KafkaOffsetRangeCalculator (for the given &lt;&gt;) <p>Used exclusively when <code>KafkaMicroBatchReader</code> is requested to &lt;&gt; (to calculate offset ranges) <p>| startPartitionOffsets a| [[startPartitionOffsets]] Starting offsets for the assigned partitions (<code>Map[TopicPartition, Long]</code>)</p> <p>Used when...FIXME</p> <p>|===</p>","location":"datasources/kafka/KafkaMicroBatchReader/#initialpartitionoffsets-maptopicpartition-long"},{"title":"Logging","text":"<p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.kafka010.KafkaMicroBatchReader</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.kafka010.KafkaMicroBatchReader=ALL\n</code></pre> <p>Refer to Logging.</p>","location":"datasources/kafka/KafkaMicroBatchReader/#logging"},{"title":"KafkaMicroBatchStream","text":"<p><code>KafkaMicroBatchStream</code> is a MicroBatchStream that SupportsTriggerAvailableNow.</p> <p><code>KafkaMicroBatchStream</code> is a ReportsSourceMetrics.</p>","location":"datasources/kafka/KafkaMicroBatchStream/"},{"title":"Creating Instance","text":"<p><code>KafkaMicroBatchStream</code> takes the following to be created:</p> <ul> <li> KafkaOffsetReader <li>Kafka Params For Executors</li> <li> Options <li> Metadata path <li> Starting KafkaOffsetRangeLimit <li> failOnDataLoss option  <p><code>KafkaMicroBatchStream</code> is created when:</p> <ul> <li><code>KafkaScan</code> is requested for a MicroBatchStream</li> </ul>","location":"datasources/kafka/KafkaMicroBatchStream/#creating-instance"},{"title":"Kafka Params For Executors <p><code>KafkaMicroBatchStream</code> is given Kafka params to use on executors when created.</p> <p>The Kafka params are the kafkaParamsForExecutors based on the options of the KafkaScan (this <code>KafkaMicroBatchStream</code> is created for) that have been converted (<code>kafka</code>-prefix removed).</p>","text":"","location":"datasources/kafka/KafkaMicroBatchStream/#kafka-params-for-executors"},{"title":"KafkaOffsetRangeCalculator","text":"<p><code>KafkaOffsetRangeCalculator</code> is &lt;&gt; for KafkaMicroBatchReader to &lt;&gt; (when <code>KafkaMicroBatchReader</code> is requested to planInputPartitions). <p>[[minPartitions]][[creating-instance]] <code>KafkaOffsetRangeCalculator</code> takes an optional minimum number of partitions per executor (<code>minPartitions</code>) to be created (that can either be undefined or greater than <code>0</code>).</p> <p>[[apply]] When created with a <code>DataSourceOptions</code>, <code>KafkaOffsetRangeCalculator</code> uses minPartitions option for the &lt;&gt;. <p>=== [[getRanges]] Offset Ranges -- <code>getRanges</code> Method</p>","location":"datasources/kafka/KafkaOffsetRangeCalculator/"},{"title":"[source, scala]","text":"<p>getRanges(   fromOffsets: PartitionOffsetMap,   untilOffsets: PartitionOffsetMap,   executorLocations: Seq[String] = Seq.empty): Seq[KafkaOffsetRange]</p>  <p><code>getRanges</code> finds the common <code>TopicPartitions</code> that are the keys that are used in the <code>fromOffsets</code> and <code>untilOffsets</code> collections (intersection).</p> <p>For every common <code>TopicPartition</code>, <code>getRanges</code> creates a &lt;&gt; with the from and until offsets from the <code>fromOffsets</code> and <code>untilOffsets</code> collections (and the &lt;&gt; undefined). <code>getRanges</code> filters out the <code>TopicPartitions</code> that &lt;&gt; (i.e. the difference between until and from offsets is not greater than <code>0</code>). <p>At this point, <code>getRanges</code> knows the <code>TopicPartitions</code> with records to consume.</p> <p><code>getRanges</code> branches off based on the defined &lt;&gt; and the number of <code>KafkaOffsetRanges</code> (<code>TopicPartitions</code> with records to consume). <p>For the &lt;&gt; undefined or smaller than the number of <code>KafkaOffsetRanges</code> (<code>TopicPartitions</code> to consume records from), <code>getRanges</code> updates every <code>KafkaOffsetRange</code> with the &lt;&gt; based on the <code>TopicPartition</code> and the <code>executorLocations</code>). <p>Otherwise (with the &lt;&gt; defined and greater than the number of <code>KafkaOffsetRanges</code>), <code>getRanges</code> splits <code>KafkaOffsetRanges</code> into smaller ones. <p>NOTE: <code>getRanges</code> is used exclusively when <code>KafkaMicroBatchReader</code> is requested to planInputPartitions.</p> <p>=== [[KafkaOffsetRange]] KafkaOffsetRange -- TopicPartition with From and Until Offsets and Optional Preferred Location</p> <p><code>KafkaOffsetRange</code> is a case class with the following attributes:</p> <ul> <li>[[topicPartition]] <code>TopicPartition</code></li> <li>[[fromOffset]] <code>fromOffset</code> offset</li> <li>[[untilOffset]] <code>untilOffset</code> offset</li> <li>[[preferredLoc]] Optional preferred location</li> </ul> <p>[[size]] <code>KafkaOffsetRange</code> knows the size, i.e. the number of records between the &lt;&gt; and &lt;&gt; offsets. <p>=== [[getLocation]] Selecting Preferred Executor for TopicPartition -- <code>getLocation</code> Internal Method</p>","location":"datasources/kafka/KafkaOffsetRangeCalculator/#source-scala"},{"title":"[source, scala]","text":"<p>getLocation(   tp: TopicPartition,   executorLocations: Seq[String]): Option[String]</p>  <p><code>getLocation</code>...FIXME</p> <p>NOTE: <code>getLocation</code> is used exclusively when <code>KafkaOffsetRangeCalculator</code> is requested to &lt;&gt;.","location":"datasources/kafka/KafkaOffsetRangeCalculator/#source-scala_1"},{"title":"KafkaOffsetRangeLimit \u2014 Desired Offset Range Limits","text":"<p><code>KafkaOffsetRangeLimit</code> represents the desired offset range limits for starting, ending, and specific offsets in Kafka Data Source.</p> <p>[[implementations]] .KafkaOffsetRangeLimits [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | KafkaOffsetRangeLimit | Description</p> <p>| EarliestOffsetRangeLimit | [[EarliestOffsetRangeLimit]] Intent to bind to the earliest offset</p> <p>| LatestOffsetRangeLimit | [[LatestOffsetRangeLimit]] Intent to bind to the latest offset</p> <p>| SpecificOffsetRangeLimit a| [[SpecificOffsetRangeLimit]] Intent to bind to specific offsets with the following special offset \"magic\" numbers:</p> <ul> <li>[[LATEST]] <code>-1</code> or <code>KafkaOffsetRangeLimit.LATEST</code> - the latest offset</li> <li>[[EARLIEST]] <code>-2</code> or <code>KafkaOffsetRangeLimit.EARLIEST</code> - the earliest offset</li> </ul> <p>|===</p> <p>NOTE: <code>KafkaOffsetRangeLimit</code> is a Scala sealed trait which means that all the &lt;&gt; are in the same compilation unit (a single file). <p><code>KafkaOffsetRangeLimit</code> is often used in a text-based representation and is converted to from latest, earliest or a JSON-formatted text using KafkaSourceProvider.getKafkaOffsetRangeLimit utility.</p> <p>NOTE: A JSON-formatted text is of the following format <code>{\"topicName\":{\"partition\":offset},...}</code>, e.g. <code>{\"topicA\":{\"0\":23,\"1\":-1},\"topicB\":{\"0\":-2}}</code>.</p> <p><code>KafkaOffsetRangeLimit</code> is used when:</p> <ul> <li> <p>KafkaContinuousReader is created (with the initial offsets)</p> </li> <li> <p>KafkaMicroBatchReader is created (with the starting offsets)</p> </li> <li> <p>KafkaRelation is created (with the starting and ending offsets)</p> </li> <li> <p>KafkaSource is created (with the starting offsets)</p> </li> <li> <p><code>KafkaSourceProvider</code> is requested to convert configuration options to KafkaOffsetRangeLimits</p> </li> </ul>","location":"datasources/kafka/KafkaOffsetRangeLimit/"},{"title":"KafkaOffsetReader","text":"<p><code>KafkaOffsetReader</code> relies on the ConsumerStrategy to &lt;&gt;. <p><code>KafkaOffsetReader</code> &lt;&gt; with group.id (<code>ConsumerConfig.GROUP_ID_CONFIG</code>) configuration explicitly set to &lt;&gt; (i.e. the given &lt;&gt; followed by &lt;&gt;). <p><code>KafkaOffsetReader</code> is &lt;&gt; when: <ul> <li> <p><code>KafkaRelation</code> is requested to build a distributed data scan with column pruning</p> </li> <li> <p><code>KafkaSourceProvider</code> is requested to create a KafkaSource, createMicroBatchReader, and createContinuousReader</p> </li> </ul> <p>[[options]] .KafkaOffsetReader's Options [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| fetchOffset.numRetries a| [[fetchOffset.numRetries]]</p> <p>Default: <code>3</code></p> <p>| fetchOffset.retryIntervalMs a| [[fetchOffset.retryIntervalMs]] How long to wait before retries</p> <p>Default: <code>1000</code></p> <p>|===</p> <p>[[kafkaSchema]] <code>KafkaOffsetReader</code> defines the predefined fixed schema.</p> <p>[[logging]] [TIP] ==== Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.kafka010.KafkaOffsetReader</code> to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.kafka010.KafkaOffsetReader=ALL\n</code></pre>","location":"datasources/kafka/KafkaOffsetReader/"},{"title":"Refer to &lt;&gt;. <p>=== [[creating-instance]] Creating KafkaOffsetReader Instance</p> <p><code>KafkaOffsetReader</code> takes the following to be created:</p> <ul> <li>[[consumerStrategy]] ConsumerStrategy</li> <li>[[driverKafkaParams]] Kafka parameters (as name-value pairs that are used exclusively to &lt;&gt; <li>[[readerOptions]] Options (as name-value pairs)</li> <li>[[driverGroupIdPrefix]] Prefix of the group ID</li>  <p><code>KafkaOffsetReader</code> initializes the &lt;&gt;. <p>=== [[nextGroupId]] <code>nextGroupId</code> Internal Method</p>","text":"","location":"datasources/kafka/KafkaOffsetReader/#refer-to"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaOffsetReader/#source-scala"},{"title":"nextGroupId(): String","text":"<p><code>nextGroupId</code> sets the &lt;&gt; to be the &lt;&gt;, <code>-</code> followed by the &lt;&gt; (i.e. <code>[driverGroupIdPrefix]-[nextId]</code>). <p>In the end, <code>nextGroupId</code> increments the &lt;&gt; and returns the &lt;&gt;. <p>NOTE: <code>nextGroupId</code> is used exclusively when <code>KafkaOffsetReader</code> is requested for a &lt;&gt;. <p>=== [[resetConsumer]] <code>resetConsumer</code> Internal Method</p>","location":"datasources/kafka/KafkaOffsetReader/#nextgroupid-string"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaOffsetReader/#source-scala_1"},{"title":"resetConsumer(): Unit","text":"<p><code>resetConsumer</code>...FIXME</p> <p>NOTE: <code>resetConsumer</code> is used when...FIXME</p> <p>=== [[fetchTopicPartitions]] <code>fetchTopicPartitions</code> Method</p>","location":"datasources/kafka/KafkaOffsetReader/#resetconsumer-unit"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaOffsetReader/#source-scala_2"},{"title":"fetchTopicPartitions(): Set[TopicPartition]","text":"<p>CAUTION: FIXME</p> <p><code>fetchTopicPartitions</code> is used when <code>KafkaRelation</code> is requested for getPartitionOffsets.</p> <p>=== [[fetchEarliestOffsets]] Fetching Earliest Offsets -- <code>fetchEarliestOffsets</code> Method</p>","location":"datasources/kafka/KafkaOffsetReader/#fetchtopicpartitions-settopicpartition"},{"title":"[source, scala]","text":"<p>fetchEarliestOffsets(): Map[TopicPartition, Long] fetchEarliestOffsets(newPartitions: Seq[TopicPartition]): Map[TopicPartition, Long]</p>  <p>CAUTION: FIXME</p> <p>NOTE: <code>fetchEarliestOffsets</code> is used when <code>KafkaSource</code> rateLimit and generates a DataFrame for a batch (when new partitions have been assigned).</p> <p>=== [[fetchLatestOffsets]] Fetching Latest Offsets -- <code>fetchLatestOffsets</code> Method</p>","location":"datasources/kafka/KafkaOffsetReader/#source-scala_3"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaOffsetReader/#source-scala_4"},{"title":"fetchLatestOffsets(): Map[TopicPartition, Long]","text":"<p>CAUTION: FIXME</p> <p>NOTE: <code>fetchLatestOffsets</code> is used when <code>KafkaSource</code> gets offsets or <code>initialPartitionOffsets</code> is initialized.</p> <p>=== [[withRetriesWithoutInterrupt]] <code>withRetriesWithoutInterrupt</code> Internal Method</p>","location":"datasources/kafka/KafkaOffsetReader/#fetchlatestoffsets-maptopicpartition-long"},{"title":"[source, scala]","text":"<p>withRetriesWithoutInterrupt(   body: =&gt; Map[TopicPartition, Long]): Map[TopicPartition, Long]</p>  <p><code>withRetriesWithoutInterrupt</code>...FIXME</p> <p>NOTE: <code>withRetriesWithoutInterrupt</code> is used when...FIXME</p> <p>=== [[fetchSpecificOffsets]] Fetching Offsets for Selected TopicPartitions -- <code>fetchSpecificOffsets</code> Method</p>","location":"datasources/kafka/KafkaOffsetReader/#source-scala_5"},{"title":"[source, scala]","text":"<p>fetchSpecificOffsets(   partitionOffsets: Map[TopicPartition, Long],   reportDataLoss: String =&gt; Unit): KafkaSourceOffset</p>  <p>.KafkaOffsetReader's fetchSpecificOffsets image::images/KafkaOffsetReader-fetchSpecificOffsets.png[align=\"center\"]</p> <p><code>fetchSpecificOffsets</code> requests the &lt;&gt; to <code>poll(0)</code>. <p><code>fetchSpecificOffsets</code> requests the &lt;&gt; for assigned partitions (using <code>Consumer.assignment()</code>). <p><code>fetchSpecificOffsets</code> requests the &lt;&gt; to <code>pause(partitions)</code>. <p>You should see the following DEBUG message in the logs:</p> <pre><code>DEBUG KafkaOffsetReader: Partitions assigned to consumer: [partitions]. Seeking to [partitionOffsets]\n</code></pre> <p>For every partition offset in the input <code>partitionOffsets</code>, <code>fetchSpecificOffsets</code> requests the &lt;&gt; to: <ul> <li><code>seekToEnd</code> for the latest (aka <code>-1</code>)</li> <li><code>seekToBeginning</code> for the earliest (aka <code>-2</code>)</li> <li><code>seek</code> for other offsets</li> </ul> <p>In the end, <code>fetchSpecificOffsets</code> creates a collection of Kafka's <code>TopicPartition</code> and <code>position</code> (using the &lt;&gt;). <p><code>fetchSpecificOffsets</code> is used when <code>KafkaSource</code> fetches and verifies initial partition offsets.</p> <p>=== [[createConsumer]] Creating Kafka Consumer -- <code>createConsumer</code> Internal Method</p>","location":"datasources/kafka/KafkaOffsetReader/#source-scala_6"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaOffsetReader/#source-scala_7"},{"title":"createConsumer(): Consumer[Array[Byte], Array[Byte]]","text":"<p><code>createConsumer</code> requests &lt;&gt; to create a Kafka Consumer with &lt;&gt; and &lt;&gt;. <p>NOTE: <code>createConsumer</code> is used when <code>KafkaOffsetReader</code> is &lt;&gt; (and initializes &lt;&gt;) and &lt;&gt; <p>=== [[consumer]] Creating Kafka Consumer (Unless Already Available) -- <code>consumer</code> Method</p>","location":"datasources/kafka/KafkaOffsetReader/#createconsumer-consumerarraybyte-arraybyte"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaOffsetReader/#source-scala_8"},{"title":"consumer: Consumer[Array[Byte], Array[Byte]]","text":"<p><code>consumer</code> gives the cached &lt;&lt;_consumer, Kafka Consumer&gt;&gt; or creates one itself.</p> <p>NOTE: Since <code>consumer</code> method is used (to access the internal &lt;&lt;_consumer, Kafka Consumer&gt;&gt;) in the <code>fetch</code> methods that gives the property of creating a new Kafka Consumer whenever the internal &lt;&lt;_consumer, Kafka Consumer&gt;&gt; reference become <code>null</code>, i.e. as in &lt;&gt;. <p><code>consumer</code>...FIXME</p> <p>NOTE: <code>consumer</code> is used when <code>KafkaOffsetReader</code> is requested to &lt;&gt;, &lt;&gt;, &lt;&gt;, and &lt;&gt;. <p>=== [[close]] Closing -- <code>close</code> Method</p>","location":"datasources/kafka/KafkaOffsetReader/#consumer-consumerarraybyte-arraybyte"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaOffsetReader/#source-scala_9"},{"title":"close(): Unit","text":"<p><code>close</code> &lt;&gt; (if the &lt;&lt;_consumer, Kafka Consumer&gt;&gt; is available). <p><code>close</code> requests the &lt;&gt; to shut down. <p><code>close</code> is used when:</p> <ul> <li> <p>KafkaContinuousReader, KafkaMicroBatchReader, and KafkaSource are requested to stop a streaming reader or source</p> </li> <li> <p><code>KafkaRelation</code> is requested to build a distributed data scan with column pruning</p> </li> </ul> <p>=== [[runUninterruptibly]] <code>runUninterruptibly</code> Internal Method</p>","location":"datasources/kafka/KafkaOffsetReader/#close-unit"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaOffsetReader/#source-scala_10"},{"title":"runUninterruptiblyT: T","text":"<p><code>runUninterruptibly</code>...FIXME</p> <p>NOTE: <code>runUninterruptibly</code> is used when...FIXME</p> <p>=== [[stopConsumer]] <code>stopConsumer</code> Internal Method</p>","location":"datasources/kafka/KafkaOffsetReader/#rununinterruptiblyt-t"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaOffsetReader/#source-scala_11"},{"title":"stopConsumer(): Unit","text":"<p><code>stopConsumer</code>...FIXME</p> <p>NOTE: <code>stopConsumer</code> is used when...FIXME</p> <p>=== [[toString]] Textual Representation -- <code>toString</code> Method</p>","location":"datasources/kafka/KafkaOffsetReader/#stopconsumer-unit"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaOffsetReader/#source-scala_12"},{"title":"toString: String","text":"<p>NOTE: <code>toString</code> is part of the ++https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object] contract for the string representation of the object.</p> <p><code>toString</code>...FIXME</p> <p>=== [[internal-properties]] Internal Properties</p> <p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| _consumer a| [[_consumer]] Kafka's https://kafka.apache.org/21/javadoc/org/apache/kafka/clients/consumer/Consumer.html[Consumer] (<code>Consumer[Array[Byte], Array[Byte]]</code>)</p> <p>&lt;&gt; when <code>KafkaOffsetReader</code> is &lt;&gt;. <p>Used when <code>KafkaOffsetReader</code>:</p> <ul> <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt;  <p>| execContext a| [[execContext]] https://www.scala-lang.org/api/2.12.8/scala/concurrent/ExecutionContextExecutorService.html[scala.concurrent.ExecutionContextExecutorService]</p> <p>| groupId a| [[groupId]]</p> <p>| kafkaReaderThread a| [[kafkaReaderThread]] https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ExecutorService.html[java.util.concurrent.ExecutorService]</p> <p>| maxOffsetFetchAttempts a| [[maxOffsetFetchAttempts]]</p> <p>| nextId a| [[nextId]]</p> <p>Initially <code>0</code></p> <p>| offsetFetchAttemptIntervalMs a| [[offsetFetchAttemptIntervalMs]]</p> <p>|===</p>","location":"datasources/kafka/KafkaOffsetReader/#tostring-string"},{"title":"KafkaRelation","text":"<p><code>KafkaRelation</code> is a <code>BaseRelation</code> (Spark SQL).</p> <p><code>KafkaRelation</code> is a <code>TableScan</code> (Spark SQL).</p> <p><code>KafkaRelation</code> is &lt;&gt; exclusively when <code>KafkaSourceProvider</code> is requested to create a BaseRelation. <p>[[options]] .KafkaRelation's Options [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| kafkaConsumer.pollTimeoutMs a| [[kafkaConsumer.pollTimeoutMs]][[pollTimeoutMs]]</p> <p>Default: <code>spark.network.timeout</code> configuration if set or <code>120s</code></p> <p>|===</p> <p>[[logging]] [TIP] ==== Enable <code>ALL</code> logging levels for <code>org.apache.spark.sql.kafka010.KafkaRelation</code> to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.kafka010.KafkaRelation=ALL\n</code></pre>","location":"datasources/kafka/KafkaRelation/"},{"title":"Refer to &lt;&gt;. <p>=== [[creating-instance]] Creating KafkaRelation Instance</p> <p><code>KafkaRelation</code> takes the following when created:</p> <ul> <li>[[sqlContext]] <code>SQLContext</code></li> <li>[[strategy]] ConsumerStrategy</li> <li>[[sourceOptions]] <code>Source</code> options (<code>Map[String, String]</code>)</li> <li>[[specifiedKafkaParams]] User-defined Kafka parameters (<code>Map[String, String]</code>)</li> <li>[[failOnDataLoss]] <code>failOnDataLoss</code> flag</li> <li>[[startingOffsets]] Starting offsets</li> <li>[[endingOffsets]] Ending offsets</li> </ul> <p>=== [[getPartitionOffsets]] <code>getPartitionOffsets</code> Internal Method</p>","text":"","location":"datasources/kafka/KafkaRelation/#refer-to"},{"title":"[source, scala]","text":"<p>getPartitionOffsets(   kafkaReader: KafkaOffsetReader,   kafkaOffsets: KafkaOffsetRangeLimit): Map[TopicPartition, Long]</p>  <p>CAUTION: FIXME</p> <p>NOTE: <code>getPartitionOffsets</code> is used exclusively when <code>KafkaRelation</code> &lt;&gt;. <p>=== [[buildScan]] Building Distributed Data Scan with Column Pruning -- <code>buildScan</code> Method</p>","location":"datasources/kafka/KafkaRelation/#source-scala"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaRelation/#source-scala_1"},{"title":"buildScan(): RDD[Row]","text":"<p>NOTE: <code>buildScan</code> is part of the <code>TableScan</code> (Spark SQL) contract to build a distributed data scan with column pruning.</p> <p><code>buildScan</code> generates a unique group ID of the format spark-kafka-relation-[randomUUID] (to make sure that a streaming query creates a new consumer group).</p> <p><code>buildScan</code> creates a KafkaOffsetReader with the following:</p> <ul> <li> <p>The given &lt;&gt; and the &lt;&gt;  <li> <p>Kafka parameters for the driver based on the given &lt;&gt;  <li> <p>spark-kafka-relation-[randomUUID]-driver for the <code>driverGroupIdPrefix</code></p> </li>  <p><code>buildScan</code> uses the <code>KafkaOffsetReader</code> to &lt;&gt; for the starting and ending offsets (based on the given &lt;&gt; and the &lt;&gt;, respectively). <code>buildScan</code> requests the <code>KafkaOffsetReader</code> to close afterwards. <p><code>buildScan</code> creates offset ranges (that are a collection of <code>KafkaSourceRDDOffsetRanges</code> with a Kafka <code>TopicPartition</code>, beginning and ending offsets and undefined preferred location).</p> <p><code>buildScan</code> prints out the following INFO message to the logs:</p> <pre><code>Generating RDD of offset ranges: [offsetRanges]\n</code></pre> <p><code>buildScan</code> creates a KafkaSourceRDD with the following:</p> <ul> <li> <p>Kafka parameters for executors based on the given &lt;&gt; and the unique group ID (<code>spark-kafka-relation-[randomUUID]</code>)  <li> <p>The offset ranges created</p> </li> <li> <p>&lt;&gt; configuration  <li> <p>The given &lt;&gt; flag  <li> <p><code>reuseKafkaConsumer</code> flag off (<code>false</code>)</p> </li>  <p><code>buildScan</code> requests the <code>KafkaSourceRDD</code> to <code>map</code> Kafka <code>ConsumerRecords</code> to <code>InternalRows</code>.</p> <p>In the end, <code>buildScan</code> requests the &lt;&gt; to create a <code>DataFrame</code> (with the name kafka and the predefined &lt;&gt;) that is immediately converted to a <code>RDD[InternalRow]</code>. <p><code>buildScan</code> throws a <code>IllegalStateException</code> when...FIXME</p> <pre><code>different topic partitions for starting offsets topics[[fromTopics]] and ending offsets topics[[untilTopics]]\n</code></pre> <p><code>buildScan</code> throws a <code>IllegalStateException</code> when...FIXME</p> <pre><code>[tp] doesn't have a from offset\n</code></pre>","location":"datasources/kafka/KafkaRelation/#buildscan-rddrow"},{"title":"KafkaScan","text":"<p>The Internals of Spark SQL</p> <p>Head over to The Internals of Spark SQL to learn more.</p>  <p><code>KafkaScan</code> is a logical <code>Scan</code> (Spark SQL).</p>","location":"datasources/kafka/KafkaScan/"},{"title":"Creating Instance","text":"<p><code>KafkaScan</code> takes the following to be created:</p> <ul> <li> Options  <p><code>KafkaScan</code> is created when:</p> <ul> <li><code>KafkaTable</code> (Spark SQL) is requested for a <code>ScanBuilder</code> (Spark SQL)</li> </ul>","location":"datasources/kafka/KafkaScan/#creating-instance"},{"title":"MicroBatchStream <pre><code>toMicroBatchStream(\n  checkpointLocation: String): MicroBatchStream\n</code></pre> <p><code>toMicroBatchStream</code> is part of the <code>Scan</code> (Spark SQL) abstraction.</p>  <p><code>toMicroBatchStream</code> validates the streaming part of the options.</p> <p><code>toMicroBatchStream</code> generate a unique group ID.</p> <p><code>toMicroBatchStream</code> removes kafka prefix from the keys in the options.</p> <p>In the end, <code>toMicroBatchStream</code> creates a KafkaMicroBatchStream with the following:</p> <ul> <li>A new KafkaOffsetReader with the strategy (from the options) and the kafkaParamsForDriver</li> <li>kafkaParamsForExecutors with the options with the <code>kafka.</code> prefix removed and an unique group ID</li> <li>The given <code>checkpointLocation</code></li> <li>Starting offsets using the options (with startingtimestamp, startingoffsetsbytimestamp and startingOffsets); LatestOffsetRangeLimit is the default</li> <li>failOnDataLoss option</li> </ul>","text":"","location":"datasources/kafka/KafkaScan/#microbatchstream"},{"title":"supportedCustomMetrics <pre><code>supportedCustomMetrics(): Array[CustomMetric]\n</code></pre> <p><code>supportedCustomMetrics</code> is part of the <code>Scan</code> (Spark SQL) abstraction.</p>  <p><code>supportedCustomMetrics</code> is the following metrics:</p> <ul> <li><code>dataLoss</code></li> <li><code>offsetOutOfRange</code></li> </ul>","text":"","location":"datasources/kafka/KafkaScan/#supportedcustommetrics"},{"title":"KafkaSink","text":"<p><code>KafkaSink</code> is a streaming sink that KafkaSourceProvider registers as the <code>kafka</code> format.</p> <pre><code>// start spark-shell or a Spark application with spark-sql-kafka-0-10 module\n// spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0-SNAPSHOT\nimport org.apache.spark.sql.SparkSession\nval spark: SparkSession = ...\nspark.\n  readStream.\n  format(\"text\").\n  load(\"server-logs/*.out\").\n  as[String].\n  writeStream.\n  queryName(\"server-logs processor\").\n  format(\"kafka\").  // &lt;-- uses KafkaSink\n  option(\"topic\", \"topic1\").\n  option(\"checkpointLocation\", \"/tmp/kafka-sink-checkpoint\"). // &lt;-- mandatory\n  start\n\n// in another terminal\n$ echo hello &gt; server-logs/hello.out\n\n// in the terminal with Spark\nFIXME\n</code></pre>","location":"datasources/kafka/KafkaSink/"},{"title":"Creating Instance","text":"<p><code>KafkaSink</code> takes the following when created:</p> <ul> <li>[[sqlContext]] <code>SQLContext</code></li> <li>[[executorKafkaParams]] Kafka parameters (used on executor) as a map of <code>(String, Object)</code> pairs</li> <li>[[topic]] Optional topic name</li> </ul> <p>=== [[addBatch]] <code>addBatch</code> Method</p>","location":"datasources/kafka/KafkaSink/#creating-instance"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaSink/#source-scala"},{"title":"addBatch(batchId: Long, data: DataFrame): Unit","text":"<p>Internally, <code>addBatch</code> requests <code>KafkaWriter</code> to write the input <code>data</code> to the &lt;&gt; (if defined) or a topic in &lt;&gt;. <p><code>addBatch</code> is a part of Sink abstraction.</p>","location":"datasources/kafka/KafkaSink/#addbatchbatchid-long-data-dataframe-unit"},{"title":"KafkaSource","text":"<p><code>KafkaSource</code> is a streaming source that loads data from Apache Kafka.</p>  <p>Note</p> <p>Kafka topics are checked for new records every trigger and so there is some noticeable delay between when the records have arrived to Kafka topics and when a Spark application processes them.</p>  <p><code>KafkaSource</code> uses the metadata log directory to persist offsets. The directory is the source ID under the <code>sources</code> directory in the checkpointRoot (of the StreamExecution).</p>  <p>Note</p> <p>The checkpointRoot directory is one of the following:</p> <ul> <li><code>checkpointLocation</code> option</li> <li>spark.sql.streaming.checkpointLocation configuration property</li> </ul>  <p><code>KafkaSource</code> &lt;&gt; for kafka format (that is registered by KafkaSourceProvider). <p></p> <p>[[schema]] <code>KafkaSource</code> uses a predefined (fixed) schema (that cannot be changed).</p> <p><code>KafkaSource</code> also supports batch Datasets.</p>","location":"datasources/kafka/KafkaSource/"},{"title":"Creating Instance","text":"<p><code>KafkaSource</code> takes the following to be created:</p> <ul> <li> <code>SQLContext</code> <li> KafkaOffsetReader <li> Parameters of executors (reading from Kafka) <li> Source Options <li> Path of Metadata Log (where <code>KafkaSource</code> persists KafkaSourceOffset offsets in JSON format) <li> Starting offsets (defined using startingOffsets option) <li> <code>failOnDataLoss</code> flag to create KafkaSourceRDDs every trigger and to report an IllegalStateException on data loss.","location":"datasources/kafka/KafkaSource/#creating-instance"},{"title":"Loading Kafka Records for Streaming Micro-Batch <pre><code>getBatch(\n  start: Option[Offset],\n  end: Offset): DataFrame\n</code></pre> <p><code>getBatch</code> is part of the Source abstraction.</p> <p><code>getBatch</code> creates a streaming <code>DataFrame</code> with a query plan with <code>LogicalRDD</code> logical operator to scan data from a KafkaSourceRDD.</p> <p>Internally, <code>getBatch</code> initializes &lt;&gt; (unless initialized already). <p>You should see the following INFO message in the logs:</p> <pre><code>GetBatch called with start = [start], end = [end]\n</code></pre> <p><code>getBatch</code> requests <code>KafkaSourceOffset</code> for end partition offsets for the input <code>end</code> offset (known as <code>untilPartitionOffsets</code>).</p> <p><code>getBatch</code> requests <code>KafkaSourceOffset</code> for start partition offsets for the input <code>start</code> offset (if defined) or uses &lt;&gt; (known as <code>fromPartitionOffsets</code>). <p><code>getBatch</code> finds the new partitions (as the difference between the topic partitions in <code>untilPartitionOffsets</code> and <code>fromPartitionOffsets</code>) and requests &lt;&gt; to fetch their earliest offsets. <p><code>getBatch</code> &lt;&gt; if the new partitions don't match to what &lt;&gt; fetched. <pre><code>Cannot find earliest offsets of [partitions]. Some data may have been missed\n</code></pre> <p>You should see the following INFO message in the logs:</p> <pre><code>Partitions added: [newPartitionOffsets]\n</code></pre> <p><code>getBatch</code> &lt;&gt; if the new partitions don't have their offsets <code>0</code>. <pre><code>Added partition [partition] starts from [offset] instead of 0. Some data may have been missed\n</code></pre> <p><code>getBatch</code> &lt;&gt; if the <code>fromPartitionOffsets</code> partitions differ from <code>untilPartitionOffsets</code> partitions. <pre><code>[partitions] are gone. Some data may have been missed\n</code></pre> <p>You should see the following DEBUG message in the logs:</p> <pre><code>TopicPartitions: [topicPartitions]\n</code></pre> <p><code>getBatch</code> &lt;&gt; (sorted by <code>executorId</code> and <code>host</code> of the registered block managers). <p>IMPORTANT: That is when <code>getBatch</code> goes very low-level to allow for cached <code>KafkaConsumers</code> in the executors to be re-used to read the same partition in every batch (aka location preference).</p> <p>You should see the following DEBUG message in the logs:</p> <pre><code>Sorted executors: [sortedExecutors]\n</code></pre> <p><code>getBatch</code> creates a <code>KafkaSourceRDDOffsetRange</code> per <code>TopicPartition</code>.</p> <p><code>getBatch</code> filters out <code>KafkaSourceRDDOffsetRanges</code> for which until offsets are smaller than from offsets. <code>getBatch</code> &lt;&gt; if they are found. <pre><code>Partition [topicPartition]'s offset was changed from [fromOffset] to [untilOffset], some data may have been missed\n</code></pre> <p><code>getBatch</code> creates a KafkaSourceRDD (with &lt;&gt;, &lt;&gt; and <code>reuseKafkaConsumer</code> flag enabled) and maps it to an RDD of <code>InternalRow</code>. <p>IMPORTANT: <code>getBatch</code> creates a <code>KafkaSourceRDD</code> with <code>reuseKafkaConsumer</code> flag enabled.</p> <p>You should see the following INFO message in the logs:</p> <pre><code>GetBatch generating RDD of offset range: [offsetRanges]\n</code></pre> <p><code>getBatch</code> sets &lt;&gt; if it was empty (which is when...FIXME) <p>In the end, <code>getBatch</code> creates a streaming <code>DataFrame</code> for the <code>KafkaSourceRDD</code> and the &lt;&gt;. <p>=== [[getOffset]] Fetching Offsets (From Metadata Log or Kafka Directly) -- <code>getOffset</code> Method</p>","text":"","location":"datasources/kafka/KafkaSource/#loading-kafka-records-for-streaming-micro-batch"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaSource/#source-scala"},{"title":"getOffset: Option[Offset] <p>NOTE: <code>getOffset</code> is a part of the ../../Source.md#getOffset[Source Contract].</p> <p>Internally, <code>getOffset</code> fetches the &lt;&gt; (from the metadata log or Kafka directly). <p>.KafkaSource Initializing initialPartitionOffsets While Fetching Initial Offsets image::images/KafkaSource-initialPartitionOffsets.png[align=\"center\"]</p> <p>NOTE: &lt;&gt; is a lazy value and is initialized the very first time <code>getOffset</code> is called (which is when <code>StreamExecution</code> MicroBatchExecution.md#constructNextBatch-hasNewData[constructs a streaming micro-batch]).","text":"","location":"datasources/kafka/KafkaSource/#getoffset-optionoffset"},{"title":"[source, scala] <p>scala&gt; spark.version res0: String = 2.3.0-SNAPSHOT</p> <p>// Case 1: Checkpoint directory undefined // initialPartitionOffsets read from Kafka directly val records = spark.   readStream.   format(\"kafka\").   option(\"subscribe\", \"topic1\").   option(\"kafka.bootstrap.servers\", \"localhost:9092\").   load // Start the streaming query // dump records to the console every 10 seconds import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val q = records.   writeStream.   format(\"console\").   option(\"truncate\", false).   trigger(Trigger.ProcessingTime(10.seconds)).   outputMode(OutputMode.Update).   start // Note the temporary checkpoint directory 17/08/07 11:09:29 INFO StreamExecution: Starting [id = 75dd261d-6b62-40fc-a368-9d95d3cb6f5f, runId = f18a5eb5-ccab-4d9d-8a81-befed41a72bd] with file:///private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/temporary-d0055630-24e4-4d9a-8f36-7a12a0f11bc0 to store the query checkpoint. ... INFO KafkaSource: Initial offsets: {\"topic1\":{\"0\":1}}</p> <p>// Stop the streaming query q.stop</p> <p>// Case 2: Checkpoint directory defined // initialPartitionOffsets read from Kafka directly // since the checkpoint directory is not available yet // it will be the next time the query is started val records = spark.   readStream.   format(\"kafka\").   option(\"subscribe\", \"topic1\").   option(\"kafka.bootstrap.servers\", \"localhost:9092\").   load.   select($\"value\" cast \"string\", $\"topic\", $\"partition\", $\"offset\") import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val q = records.   writeStream.   format(\"console\").   option(\"truncate\", false).   option(\"checkpointLocation\", \"/tmp/checkpoint\"). // \u2190 checkpoint directory   trigger(Trigger.ProcessingTime(10.seconds)).   outputMode(OutputMode.Update).   start // Note the checkpoint directory in use 17/08/07 11:21:25 INFO StreamExecution: Starting [id = b8f59854-61c1-4c2f-931d-62bbaf90ee3b, runId = 70d06a3b-f2b1-4fa8-a518-15df4cf59130] with file:///tmp/checkpoint to store the query checkpoint. ... INFO KafkaSource: Initial offsets: {\"topic1\":{\"0\":1}} ... INFO StreamExecution: Stored offsets for batch 0. Metadata OffsetSeqMetadata(0,1502098526848,Map(spark.sql.shuffle.partitions -&gt; 200, spark.sql.streaming.stateStore.providerClass -&gt; org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider))</p> <p>// Review the checkpoint location // $ ls -ltr /tmp/checkpoint/offsets // total 8 // -rw-r--r--  1 jacek  wheel  248  7 sie 11:21 0 // $ tail -2 /tmp/checkpoint/offsets/0 | jq</p> <p>// Produce messages to Kafka so the latest offset changes // And more importanly the offset gets stored to checkpoint location</p>","text":"","location":"datasources/kafka/KafkaSource/#source-scala_1"},{"title":"Batch: 1 <p>+---------------------------+------+---------+------+ |value                      |topic |partition|offset| +---------------------------+------+---------+------+ |testing checkpoint location|topic1|0        |2     | +---------------------------+------+---------+------+</p> <p>// and one more // Note the offset</p>","text":"","location":"datasources/kafka/KafkaSource/#batch-1"},{"title":"Batch: 2 <p>+------------+------+---------+------+ |value       |topic |partition|offset| +------------+------+---------+------+ |another test|topic1|0        |3     | +------------+------+---------+------+</p> <p>// See what was checkpointed // $ ls -ltr /tmp/checkpoint/offsets // total 24 // -rw-r--r--  1 jacek  wheel  248  7 sie 11:35 0 // -rw-r--r--  1 jacek  wheel  248  7 sie 11:37 1 // -rw-r--r--  1 jacek  wheel  248  7 sie 11:38 2 // $ tail -2 /tmp/checkpoint/offsets/2 | jq</p> <p>// Stop the streaming query q.stop</p> <p>// And start over to see what offset the query starts from // Checkpoint location should have the offsets val q = records.   writeStream.   format(\"console\").   option(\"truncate\", false).   option(\"checkpointLocation\", \"/tmp/checkpoint\"). // \u2190 checkpoint directory   trigger(Trigger.ProcessingTime(10.seconds)).   outputMode(OutputMode.Update).   start // Whoops...console format does not support recovery (!) // Reported as https://issues.apache.org/jira/browse/SPARK-21667 org.apache.spark.sql.AnalysisException: This query does not support recovering from checkpoint location. Delete /tmp/checkpoint/offsets to start over.;   at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:222)   at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:278)   at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:284)   ... 61 elided</p> <p>// Change the sink (= output format) to JSON val q = records.   writeStream.   format(\"json\").   option(\"path\", \"/tmp/json-sink\").   option(\"checkpointLocation\", \"/tmp/checkpoint\"). // \u2190 checkpoint directory   trigger(Trigger.ProcessingTime(10.seconds)).   start // Note the checkpoint directory in use 17/08/07 12:09:02 INFO StreamExecution: Starting [id = 02e00924-5f0d-4501-bcb8-80be8a8be385, runId = 5eba2576-dad6-4f95-9031-e72514475edc] with file:///tmp/checkpoint to store the query checkpoint. ... 17/08/07 12:09:02 INFO KafkaSource: GetBatch called with start = Some({\"topic1\":{\"0\":3}}), end = {\"topic1\":{\"0\":4}} 17/08/07 12:09:02 INFO KafkaSource: Partitions added: Map() 17/08/07 12:09:02 DEBUG KafkaSource: TopicPartitions: topic1-0 17/08/07 12:09:02 DEBUG KafkaSource: Sorted executors: 17/08/07 12:09:02 INFO KafkaSource: GetBatch generating RDD of offset range: KafkaSourceRDDOffsetRange(topic1-0,3,4,None) 17/08/07 12:09:03 DEBUG KafkaOffsetReader: Partitions assigned to consumer: [topic1-0]. Seeking to the end. 17/08/07 12:09:03 DEBUG KafkaOffsetReader: Got latest offsets for partition : Map(topic1-0 -&gt; 4) 17/08/07 12:09:03 DEBUG KafkaSource: GetOffset: ArrayBuffer((topic1-0,4)) 17/08/07 12:09:03 DEBUG StreamExecution: getOffset took 122 ms 17/08/07 12:09:03 DEBUG StreamExecution: Resuming at batch 3 with committed offsets {KafkaSource[Subscribe[topic1]]: {\"topic1\":{\"0\":4}}} and available offsets {KafkaSource[Subscribe[topic1]]: {\"topic1\":{\"0\":4}}} 17/08/07 12:09:03 DEBUG StreamExecution: Stream running from {KafkaSource[Subscribe[topic1]]: {\"topic1\":{\"0\":4}}} to {KafkaSource[Subscribe[topic1]]: {\"topic1\":{\"0\":4}}}</p>  <p><code>getOffset</code> requests &lt;&gt; to fetchLatestOffsets (known later as <code>latest</code>). <p>NOTE: (Possible performance degradation?) It is possible that <code>getOffset</code> will request the latest offsets from Kafka twice, i.e. while initializing &lt;&gt; (when no metadata log is available and KafkaSource's &lt;&gt; is <code>LatestOffsetRangeLimit</code>) and always as part of <code>getOffset</code> itself. <p><code>getOffset</code> then calculates &lt;&gt; based on the  &lt;&gt; option. <p>.getOffset's Offset Calculation per maxOffsetsPerTrigger [cols=\"1,1\",options=\"header\",width=\"100%\"] |=== | maxOffsetsPerTrigger | Offsets</p> <p>| Unspecified (i.e. <code>None</code>) | <code>latest</code></p> <p>| Defined (but &lt;&gt; is empty) | &lt;&gt; with <code>limit</code> limit, &lt;&gt; as <code>from</code>, <code>until</code> as <code>latest</code> <p>| Defined (and &lt;&gt; contains partitions and offsets) | &lt;&gt; with <code>limit</code> limit, &lt;&gt; as <code>from</code>, <code>until</code> as <code>latest</code> |=== <p>You should see the following DEBUG message in the logs:</p> <pre><code>GetOffset: [offsets]\n</code></pre> <p>In the end, <code>getOffset</code> creates a KafkaSourceOffset with <code>offsets</code> (as <code>Map[TopicPartition, Long]</code>).</p> <p>=== [[fetchAndVerify]] Fetching and Verifying Specific Offsets -- <code>fetchAndVerify</code> Internal Method</p>","text":"","location":"datasources/kafka/KafkaSource/#batch-2"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaSource/#source-scala_2"},{"title":"fetchAndVerify(specificOffsets: Map[TopicPartition, Long]): KafkaSourceOffset <p><code>fetchAndVerify</code> requests &lt;&gt; to fetchSpecificOffsets for the given <code>specificOffsets</code>. <p><code>fetchAndVerify</code> makes sure that the starting offsets in <code>specificOffsets</code> are the same as in Kafka and &lt;&gt; otherwise. <pre><code>startingOffsets for [tp] was [off] but consumer reset to [result(tp)]\n</code></pre> <p>In the end, <code>fetchAndVerify</code> creates a KafkaSourceOffset (with the result of &lt;&gt;). <p>NOTE: <code>fetchAndVerify</code> is used exclusively when <code>KafkaSource</code> initializes &lt;&gt;. <p>=== [[initialPartitionOffsets]] Initial Partition Offsets (of 0th Batch) -- <code>initialPartitionOffsets</code> Internal Lazy Property</p>","text":"","location":"datasources/kafka/KafkaSource/#fetchandverifyspecificoffsets-maptopicpartition-long-kafkasourceoffset"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaSource/#source-scala_3"},{"title":"initialPartitionOffsets: Map[TopicPartition, Long] <p><code>initialPartitionOffsets</code> is the initial partition offsets for the batch <code>0</code> that were already persisted in the &lt;&gt; or persisted on demand. <p>As the very first step, <code>initialPartitionOffsets</code> creates a custom HDFSMetadataLog (of KafkaSourceOffsets metadata) in the &lt;&gt;. <p><code>initialPartitionOffsets</code> requests the <code>HDFSMetadataLog</code> for the metadata of the <code>0</code>th batch (as <code>KafkaSourceOffset</code>).</p> <p>If the metadata is available, <code>initialPartitionOffsets</code> requests the metadata for the collection of TopicPartitions and their offsets.</p> <p>If the metadata could not be found, <code>initialPartitionOffsets</code> creates a new <code>KafkaSourceOffset</code> per &lt;&gt;: <ul> <li> <p>For <code>EarliestOffsetRangeLimit</code>, <code>initialPartitionOffsets</code> requests the &lt;&gt; to fetchEarliestOffsets  <li> <p>For <code>LatestOffsetRangeLimit</code>, <code>initialPartitionOffsets</code> requests the &lt;&gt; to fetchLatestOffsets  <li> <p>For <code>SpecificOffsetRangeLimit</code>, <code>initialPartitionOffsets</code> requests the &lt;&gt; to fetchSpecificOffsets (and report a data loss per the &lt;&gt; flag)   <p><code>initialPartitionOffsets</code> requests the custom <code>HDFSMetadataLog</code> to add the offsets to the metadata log (as the metadata of the <code>0</code>th batch).</p> <p><code>initialPartitionOffsets</code> prints out the following INFO message to the logs:</p> <pre><code>Initial offsets: [offsets]\n</code></pre>  <p>Note</p> <p><code>initialPartitionOffsets</code> is used when <code>KafkaSource</code> is requested for the following:</p> <ul> <li> <p>&lt;&gt;  <li> <p>&lt;&gt; (when the start offsets are not defined, i.e. before <code>StreamExecution</code> commits the first streaming batch and so nothing is in committedOffsets registry for a <code>KafkaSource</code> data source yet)    <p>==== [[initialPartitionOffsets-HDFSMetadataLog-serialize]] <code>HDFSMetadataLog.serialize</code></p>","text":"","location":"datasources/kafka/KafkaSource/#initialpartitionoffsets-maptopicpartition-long"},{"title":"[source, scala] <p>serialize(   metadata: KafkaSourceOffset,   out: OutputStream): Unit</p>  <p><code>serialize</code> requests the <code>OutputStream</code> to write a zero byte (to support Spark 2.1.0 as per SPARK-19517).</p> <p><code>serialize</code> creates a <code>BufferedWriter</code> over a <code>OutputStreamWriter</code> over the <code>OutputStream</code> (with <code>UTF_8</code> charset encoding).</p> <p><code>serialize</code> requests the <code>BufferedWriter</code> to write the v1 version indicator followed by a new line.</p> <p><code>serialize</code> then requests the <code>KafkaSourceOffset</code> for a JSON-serialized representation and the <code>BufferedWriter</code> to write it out.</p> <p>In the end, <code>serialize</code> requests the <code>BufferedWriter</code> to flush (the underlying stream).</p> <p><code>serialize</code> is part of the HDFSMetadataLog abstraction.</p> <p>=== [[rateLimit]] <code>rateLimit</code> Internal Method</p>","text":"","location":"datasources/kafka/KafkaSource/#source-scala_4"},{"title":"[source, scala] <p>rateLimit(   limit: Long,   from: Map[TopicPartition, Long],   until: Map[TopicPartition, Long]): Map[TopicPartition, Long]</p>  <p><code>rateLimit</code> requests &lt;&gt; to fetchEarliestOffsets. <p>CAUTION: FIXME</p> <p>NOTE: <code>rateLimit</code> is used exclusively when <code>KafkaSource</code> &lt;&gt; (when &lt;&gt; option is specified). <p>=== [[getSortedExecutorList]] <code>getSortedExecutorList</code> Method</p> <p>CAUTION: FIXME</p> <p>=== [[reportDataLoss]] <code>reportDataLoss</code> Internal Method</p> <p>CAUTION: FIXME</p>","text":"","location":"datasources/kafka/KafkaSource/#source-scala_5"},{"title":"[NOTE]","text":"<p><code>reportDataLoss</code> is used when <code>KafkaSource</code> does the following:</p> <ul> <li>&lt;&gt; <li>","location":"datasources/kafka/KafkaSource/#note"},{"title":"&lt;&gt;   <p>=== [[internal-properties]] Internal Properties</p> <p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| currentPartitionOffsets | [[currentPartitionOffsets]] Current partition offsets (as <code>Map[TopicPartition, Long]</code>)</p> <p>Initially <code>NONE</code> and set when <code>KafkaSource</code> is requested to &lt;&gt; or &lt;&gt;. <p>| pollTimeoutMs a| [[pollTimeoutMs]]</p> <p>| sc a| [[sc]] Spark Core's <code>SparkContext</code> (of the &lt;&gt;) <p>Used when:</p> <ul> <li> <p>&lt;&gt; (and creating a KafkaSourceRDD)  <li> <p>Initializing the pollTimeoutMs internal property</p> </li>  <p>|===</p>","text":"","location":"datasources/kafka/KafkaSource/#_1"},{"title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.kafka010.KafkaSource</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.kafka010.KafkaSource=ALL\n</code></pre> <p>Refer to Logging.</p>","text":"","location":"datasources/kafka/KafkaSource/#logging"},{"title":"KafkaSourceInitialOffsetWriter","text":"<p><code>KafkaSourceInitialOffsetWriter</code> is a Hadoop DFS-based metadata storage for KafkaSourceOffsets.</p> <p><code>KafkaSourceInitialOffsetWriter</code> is &lt;&gt; exclusively when <code>KafkaMicroBatchReader</code> is requested to getOrCreateInitialPartitionOffsets. <p>[[VERSION]] <code>KafkaSourceInitialOffsetWriter</code> uses <code>1</code> for the version.</p>","location":"datasources/kafka/KafkaSourceInitialOffsetWriter/"},{"title":"Creating Instance","text":"<p><code>KafkaSourceInitialOffsetWriter</code> takes the following to be created:</p> <ul> <li>[[sparkSession]] <code>SparkSession</code></li> <li>[[metadataPath]] Path of the metadata log directory</li> </ul> <p>=== [[deserialize]] Deserializing Metadata (Reading Metadata from Serialized Format) -- <code>deserialize</code> Method</p>","location":"datasources/kafka/KafkaSourceInitialOffsetWriter/#creating-instance"},{"title":"[source, scala]","text":"<p>deserialize(   in: InputStream): KafkaSourceOffset</p>  <p><code>deserialize</code>...FIXME</p> <p><code>deserialize</code> is part of the HDFSMetadataLog abstraction.</p>","location":"datasources/kafka/KafkaSourceInitialOffsetWriter/#source-scala"},{"title":"KafkaSourceOffset","text":"<p><code>KafkaSourceOffset</code> is an Offset for kafka data source.</p> <p><code>KafkaSourceOffset</code> is &lt;&gt; (directly or indirectly using &lt;&gt;) when: <ul> <li> <p><code>KafkaContinuousReader</code> is requested to setStartOffset, deserializeOffset, and mergeOffsets</p> </li> <li> <p><code>KafkaMicroBatchReader</code> is requested to getStartOffset, getEndOffset, deserializeOffset, and getOrCreateInitialPartitionOffsets</p> </li> <li> <p><code>KafkaOffsetReader</code> is requested to fetchSpecificOffsets</p> </li> <li> <p><code>KafkaSource</code> is requested for the initial partition offsets (of 0th batch) and getOffset</p> </li> <li> <p><code>KafkaSourceInitialOffsetWriter</code> is requested to deserialize a KafkaSourceOffset (from an InputStream)</p> </li> <li> <p><code>KafkaSourceOffset</code> is requested for &lt;&gt;   <p>[[creating-instance]][[partitionToOffsets]] <code>KafkaSourceOffset</code> takes a collection of Kafka <code>TopicPartitions</code> with offsets to be created.</p> <p>=== [[getPartitionOffsets]] Partition Offsets -- <code>getPartitionOffsets</code> Method</p>","location":"datasources/kafka/KafkaSourceOffset/"},{"title":"[source, scala]","text":"<p>getPartitionOffsets(   offset: Offset): Map[TopicPartition, Long]</p>  <p><code>getPartitionOffsets</code> takes &lt;&gt; from <code>offset</code>. <p>If <code>offset</code> is <code>KafkaSourceOffset</code>, <code>getPartitionOffsets</code> takes the partitions and offsets straight from it.</p> <p>If however <code>offset</code> is <code>SerializedOffset</code>, <code>getPartitionOffsets</code> deserializes the offsets from JSON.</p> <p><code>getPartitionOffsets</code> reports an <code>IllegalArgumentException</code> when <code>offset</code> is neither <code>KafkaSourceOffset</code> or <code>SerializedOffset</code>.</p> <pre><code>Invalid conversion from offset of [class] to KafkaSourceOffset\n</code></pre> <p><code>getPartitionOffsets</code> is used when:</p> <ul> <li> <p><code>KafkaContinuousReader</code> is requested to planInputPartitions</p> </li> <li> <p><code>KafkaSource</code> is requested to generate a streaming DataFrame with records from Kafka for a streaming micro-batch</p> </li> </ul> <p>=== [[json]] JSON-Encoded Offset -- <code>json</code> Method</p>","location":"datasources/kafka/KafkaSourceOffset/#source-scala"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaSourceOffset/#source-scala_1"},{"title":"json: String","text":"<p><code>json</code> is part of the Offset abstraction.</p> <p><code>json</code>...FIXME</p> <p>=== [[apply]] Creating KafkaSourceOffset Instance -- <code>apply</code> Utility Method</p>","location":"datasources/kafka/KafkaSourceOffset/#json-string"},{"title":"[source, scala]","text":"<p>apply(   offsetTuples: (String, Int, Long)*): KafkaSourceOffset // &lt;1&gt; apply(   offset: SerializedOffset): KafkaSourceOffset</p>  <p>&lt;1&gt; Used in tests only</p> <p><code>apply</code>...FIXME</p> <p><code>apply</code> is used when:</p> <ul> <li> <p><code>KafkaSourceInitialOffsetWriter</code> is requested to deserialize a KafkaSourceOffset (from an InputStream)</p> </li> <li> <p><code>KafkaSource</code> is requested for the initial partition offsets (of 0th batch)</p> </li> <li> <p><code>KafkaSourceOffset</code> is requested to getPartitionOffsets</p> </li> </ul>","location":"datasources/kafka/KafkaSourceOffset/#source-scala_2"},{"title":"KafkaSourceProvider","text":"<p><code>KafkaSourceProvider</code> is a <code>DataSourceRegister</code> that registers kafka data source alias.</p>  <p>The Internals of Spark SQL</p> <p>Read up on DataSourceRegister in The Internals of Spark SQL book.</p>  <p><code>KafkaSourceProvider</code> supports micro-batch stream processing (through MicroBatchStream) and uses a specialized KafkaMicroBatchReader.</p>","location":"datasources/kafka/KafkaSourceProvider/"},{"title":"Properties of Kafka Consumers on Executors","text":"ConsumerConfig's Key Value     KEY_DESERIALIZER_CLASS_CONFIG ByteArrayDeserializer   VALUE_DESERIALIZER_CLASS_CONFIG ByteArrayDeserializer   AUTO_OFFSET_RESET_CONFIG none   GROUP_ID_CONFIG uniqueGroupId-executor   ENABLE_AUTO_COMMIT_CONFIG false   RECEIVE_BUFFER_CONFIG 65536","location":"datasources/kafka/KafkaSourceProvider/#properties-of-kafka-consumers-on-executors"},{"title":"Required Options","text":"<p><code>KafkaSourceProvider</code> requires the following options (that you can set using <code>option</code> method of DataStreamReader or DataStreamWriter):</p> <ul> <li> <p>Exactly one of the following options: subscribe, subscribePattern or assign</p> </li> <li> <p>kafka.bootstrap.servers</p> </li> </ul>  <p>Tip</p> <p>Refer to Kafka Data Source's Options for the supported configuration options.</p>","location":"datasources/kafka/KafkaSourceProvider/#required-options"},{"title":"Creating KafkaTable <pre><code>getTable(\n  options: CaseInsensitiveStringMap): KafkaTable\n</code></pre> <p><code>getTable</code> creates a KafkaTable with the value of <code>includeheaders</code> option (default: <code>false</code>).</p> <p><code>getTable</code> is part of the <code>SimpleTableProvider</code> abstraction (Spark SQL).</p>","text":"","location":"datasources/kafka/KafkaSourceProvider/#creating-kafkatable"},{"title":"Creating Streaming Sink <pre><code>createSink(\n  sqlContext: SQLContext,\n  parameters: Map[String, String],\n  partitionColumns: Seq[String],\n  outputMode: OutputMode): Sink\n</code></pre> <p><code>createSink</code> creates a KafkaSink for <code>topic</code> option (if defined) and Kafka Producer parameters.</p> <p><code>createSink</code> is part of the StreamSinkProvider abstraction.</p>","text":"","location":"datasources/kafka/KafkaSourceProvider/#creating-streaming-sink"},{"title":"Creating Streaming Source <pre><code>createSource(\n  sqlContext: SQLContext,\n  metadataPath: String,\n  schema: Option[StructType],\n  providerName: String,\n  parameters: Map[String, String]): Source\n</code></pre> <p><code>createSource</code> validates stream options.</p> <p><code>createSource</code>...FIXME</p> <p><code>createSource</code> is part of the StreamSourceProvider abstraction.</p>","text":"","location":"datasources/kafka/KafkaSourceProvider/#creating-streaming-source"},{"title":"Validating Options For Batch And Streaming Queries <pre><code>validateGeneralOptions(\n  parameters: Map[String, String]): Unit\n</code></pre>  <p>Note</p> <p>Parameters are case-insensitive, i.e. <code>OptioN</code> and <code>option</code> are equal.</p>  <p><code>validateGeneralOptions</code> makes sure that exactly one topic subscription strategy is used in <code>parameters</code> and can be:</p> <ul> <li><code>subscribe</code></li> <li><code>subscribepattern</code></li> <li><code>assign</code></li> </ul> <p><code>validateGeneralOptions</code> reports an <code>IllegalArgumentException</code> when there is no subscription strategy in use or there are more than one strategies used.</p> <p><code>validateGeneralOptions</code> makes sure that the value of subscription strategies meet the requirements:</p> <ul> <li><code>assign</code> strategy starts with <code>{</code> (the opening curly brace)</li> <li><code>subscribe</code> strategy has at least one topic (in a comma-separated list of topics)</li> <li><code>subscribepattern</code> strategy has the pattern defined</li> </ul> <p><code>validateGeneralOptions</code> makes sure that <code>group.id</code> has not been specified and reports an <code>IllegalArgumentException</code> otherwise.</p> <pre><code>Kafka option 'group.id' is not supported as user-specified consumer groups are not used to track offsets.\n</code></pre> <p><code>validateGeneralOptions</code> makes sure that <code>auto.offset.reset</code> has not been specified and reports an <code>IllegalArgumentException</code> otherwise.</p> <pre><code>Kafka option 'auto.offset.reset' is not supported.\nInstead set the source option 'startingoffsets' to 'earliest' or 'latest' to specify where to start. Structured Streaming manages which offsets are consumed internally, rather than relying on the kafkaConsumer to do it. This will ensure that no data is missed when new topics/partitions are dynamically subscribed. Note that 'startingoffsets' only applies when a new Streaming query is started, and\nthat resuming will always pick up from where the query left off. See the docs for more details.\n</code></pre> <p><code>validateGeneralOptions</code> makes sure that the following options have not been specified and reports an <code>IllegalArgumentException</code> otherwise:</p> <ul> <li><code>kafka.key.deserializer</code></li> <li><code>kafka.value.deserializer</code></li> <li><code>kafka.enable.auto.commit</code></li> <li><code>kafka.interceptor.classes</code></li> </ul> <p>In the end, <code>validateGeneralOptions</code> makes sure that <code>kafka.bootstrap.servers</code> option was specified and reports an <code>IllegalArgumentException</code> otherwise.</p> <pre><code>Option 'kafka.bootstrap.servers' must be specified for configuring Kafka consumer\n</code></pre> <p><code>validateGeneralOptions</code> is used when <code>KafkaSourceProvider</code> validates options for streaming and batch queries.</p>","text":"","location":"datasources/kafka/KafkaSourceProvider/#validating-options-for-batch-and-streaming-queries"},{"title":"Creating ConsumerStrategy <pre><code>strategy(\n  caseInsensitiveParams: Map[String, String]): ConsumerStrategy\n</code></pre> <p><code>strategy</code> converts a key (in <code>caseInsensitiveParams</code>) to a ConsumerStrategy.</p>    Key ConsumerStrategy     <code>assign</code> AssignStrategy   <code>subscribe</code> SubscribeStrategy   <code>subscribepattern</code> SubscribePatternStrategy    <p><code>strategy</code> is used when...FIXME</p>","text":"","location":"datasources/kafka/KafkaSourceProvider/#creating-consumerstrategy"},{"title":"AssignStrategy <p>AssignStrategy with Kafka TopicPartitions</p> <p><code>strategy</code> uses <code>JsonUtils.partitions</code> method to parse a JSON with topic names and partitions, e.g.</p> <pre><code>{\"topicA\":[0,1],\"topicB\":[0,1]}\n</code></pre> <p>The topic names and partitions are mapped directly to Kafka's <code>TopicPartition</code> objects.</p>","text":"","location":"datasources/kafka/KafkaSourceProvider/#assignstrategy"},{"title":"SubscribeStrategy <p>SubscribeStrategy with topic names</p> <p><code>strategy</code> extracts topic names from a comma-separated string, e.g.</p> <pre><code>topic1,topic2,topic3\n</code></pre>","text":"","location":"datasources/kafka/KafkaSourceProvider/#subscribestrategy"},{"title":"SubscribePatternStrategy <p>SubscribePatternStrategy with topic subscription regex pattern (that uses a Java java.util.regex.Pattern for the pattern), e.g.</p> <pre><code>topic\\d\n</code></pre>","text":"","location":"datasources/kafka/KafkaSourceProvider/#subscribepatternstrategy"},{"title":"Name and Schema of Streaming Source <pre><code>sourceSchema(\n  sqlContext: SQLContext,\n  schema: Option[StructType],\n  providerName: String,\n  parameters: Map[String, String]): (String, StructType)\n</code></pre> <p><code>sourceSchema</code> gives the short name (i.e. <code>kafka</code>) and the fixed schema.</p> <p>Internally, <code>sourceSchema</code> validates Kafka options and makes sure that the optional input <code>schema</code> is indeed undefined.</p> <p>When the input <code>schema</code> is defined, <code>sourceSchema</code> reports a <code>IllegalArgumentException</code>.</p> <pre><code>Kafka source has a fixed schema and cannot be set with a custom one\n</code></pre> <p><code>sourceSchema</code> is part of the StreamSourceProvider abstraction.</p>","text":"","location":"datasources/kafka/KafkaSourceProvider/#name-and-schema-of-streaming-source"},{"title":"Validating Kafka Options for Streaming Queries <pre><code>validateStreamOptions(\n  caseInsensitiveParams: Map[String, String]): Unit\n</code></pre> <p><code>validateStreamOptions</code> makes sure that <code>endingoffsets</code> option is not used. Otherwise, <code>validateStreamOptions</code> reports a <code>IllegalArgumentException</code>.</p> <pre><code>ending offset not valid in streaming queries\n</code></pre> <p><code>validateStreamOptions</code> validates the general options.</p> <p><code>validateStreamOptions</code> is used when <code>KafkaSourceProvider</code> is requested for the schema for Kafka source and to create a KafkaSource.</p>","text":"","location":"datasources/kafka/KafkaSourceProvider/#validating-kafka-options-for-streaming-queries"},{"title":"Converting Configuration Options to KafkaOffsetRangeLimit <pre><code>getKafkaOffsetRangeLimit(\n  params: Map[String, String],\n  offsetOptionKey: String,\n  defaultOffsets: KafkaOffsetRangeLimit): KafkaOffsetRangeLimit\n</code></pre> <p><code>getKafkaOffsetRangeLimit</code> finds the given <code>offsetOptionKey</code> in the <code>params</code> and does the following conversion:</p> <ul> <li> <p>latest becomes LatestOffsetRangeLimit</p> </li> <li> <p>earliest becomes EarliestOffsetRangeLimit</p> </li> <li> <p>A JSON-formatted text becomes SpecificOffsetRangeLimit</p> </li> <li> <p>When the given <code>offsetOptionKey</code> is not found, <code>getKafkaOffsetRangeLimit</code> returns the given <code>defaultOffsets</code></p> </li> </ul> <p><code>getKafkaOffsetRangeLimit</code> is used when:</p> <ul> <li><code>KafkaSourceProvider</code> is requested to createSource, createMicroBatchReader, createContinuousReader, createRelation, and validateBatchOptions</li> </ul>","text":"","location":"datasources/kafka/KafkaSourceProvider/#converting-configuration-options-to-kafkaoffsetrangelimit"},{"title":"Creating Fake BaseRelation <pre><code>createRelation(\n  sqlContext: SQLContext,\n  parameters: Map[String, String]): BaseRelation\n</code></pre> <p><code>createRelation</code>...FIXME</p> <p><code>createRelation</code> is part of the <code>RelationProvider</code> abstraction (Spark SQL).</p>","text":"","location":"datasources/kafka/KafkaSourceProvider/#creating-fake-baserelation"},{"title":"Validating Configuration Options for Batch Processing <pre><code>validateBatchOptions(\n  caseInsensitiveParams: Map[String, String]): Unit\n</code></pre> <p><code>validateBatchOptions</code>...FIXME</p> <p><code>validateBatchOptions</code> is used when <code>KafkaSourceProvider</code> is requested to createSource.</p>","text":"","location":"datasources/kafka/KafkaSourceProvider/#validating-configuration-options-for-batch-processing"},{"title":"Configuration Properties","text":"","location":"datasources/kafka/KafkaSourceProvider/#configuration-properties"},{"title":"failOnDataLoss <pre><code>failOnDataLoss(\n  caseInsensitiveParams: Map[String, String]): Boolean\n</code></pre> <p><code>failOnDataLoss</code> looks up the <code>failOnDataLoss</code> configuration property (in the <code>caseInsensitiveParams</code>) or defaults to <code>true</code>.</p>","text":"","location":"datasources/kafka/KafkaSourceProvider/#failondataloss"},{"title":"Utilities","text":"","location":"datasources/kafka/KafkaSourceProvider/#utilities"},{"title":"kafkaParamsForDriver <pre><code>kafkaParamsForDriver(\n  specifiedKafkaParams: Map[String, String]): Map[String, Object]\n</code></pre> <p><code>kafkaParamsForDriver</code>...FIXME</p> <p><code>kafkaParamsForDriver</code> is used when:</p> <ul> <li><code>KafkaBatch</code> is requested to planInputPartitions</li> <li><code>KafkaRelation</code> is requested to buildScan</li> <li><code>KafkaSourceProvider</code> is requested for a streaming source</li> <li><code>KafkaScan</code> is requested for a MicroBatchStream and ContinuousStream</li> </ul>","text":"","location":"datasources/kafka/KafkaSourceProvider/#kafkaparamsfordriver"},{"title":"kafkaParamsForExecutors <pre><code>kafkaParamsForExecutors(\n  specifiedKafkaParams: Map[String, String],\n  uniqueGroupId: String): Map[String, Object]\n</code></pre> <p><code>kafkaParamsForExecutors</code> sets the Kafka properties for executors.</p> <p>While setting the properties, <code>kafkaParamsForExecutors</code> prints out the following DEBUG message to the logs:</p> <pre><code>executor: Set [key] to [value], earlier value: [value]\n</code></pre> <p><code>kafkaParamsForExecutors</code> is used when:</p> <ul> <li><code>KafkaSourceProvider</code> is requested to createSource (for a KafkaSource), createMicroBatchReader (for a KafkaMicroBatchReader), and createContinuousReader (for a KafkaContinuousReader)</li> <li><code>KafkaRelation</code> is requested to buildScan (for a <code>KafkaSourceRDD</code>)</li> </ul>","text":"","location":"datasources/kafka/KafkaSourceProvider/#kafkaparamsforexecutors"},{"title":"Kafka Producer Parameters <pre><code>kafkaParamsForProducer(\n  params: CaseInsensitiveMap[String]): ju.Map[String, Object]\n</code></pre> <p><code>kafkaParamsForProducer</code>...FIXME</p> <p><code>kafkaParamsForProducer</code> is used when:</p> <ul> <li><code>KafkaSourceProvider</code> is requested for a streaming sink or relation</li> <li><code>KafkaTable</code> is requested for a WriteBuilder</li> </ul>","text":"","location":"datasources/kafka/KafkaSourceProvider/#kafka-producer-parameters"},{"title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.kafka010.KafkaSourceProvider</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.kafka010.KafkaSourceProvider=ALL\n</code></pre> <p>Refer to Logging.</p>","text":"","location":"datasources/kafka/KafkaSourceProvider/#logging"},{"title":"KafkaSourceRDD","text":"<p><code>KafkaSourceRDD</code> is an <code>RDD</code> of Kafka's ConsumerRecords (<code>RDD[ConsumerRecord[Array[Byte], Array[Byte]]]</code>) and no parent RDDs.</p> <p><code>KafkaSourceRDD</code> is &lt;&gt; when: <ul> <li> <p><code>KafkaRelation</code> is requested to build a distributed data scan with column pruning</p> </li> <li> <p><code>KafkaSource</code> is requested to generate a streaming DataFrame with records from Kafka for a streaming micro-batch</p> </li> </ul>","location":"datasources/kafka/KafkaSourceRDD/"},{"title":"Creating Instance","text":"<p><code>KafkaSourceRDD</code> takes the following when created:</p> <ul> <li>[[sc]] <code>SparkContext</code></li> <li>[[executorKafkaParams]] Collection of key-value settings for executors reading records from Kafka topics</li> <li>[[offsetRanges]] Collection of <code>KafkaSourceRDDOffsetRange</code> offsets</li> <li>[[pollTimeoutMs]] Timeout (in milliseconds) to poll data from Kafka + Used when <code>KafkaSourceRDD</code> &lt;&gt; (for given offsets) and in turn requests the <code>CachedKafkaConsumer</code> to poll for records. <li>[[failOnDataLoss]] Flag to...FIXME</li> <li>[[reuseKafkaConsumer]] Flag to...FIXME</li>  <p>=== [[getPreferredLocations]] Placement Preferences of Partition (Preferred Locations) -- <code>getPreferredLocations</code> Method</p>","location":"datasources/kafka/KafkaSourceRDD/#creating-instance"},{"title":"[source, scala]","text":"<p>getPreferredLocations(   split: Partition): Seq[String]</p>  <p>NOTE: <code>getPreferredLocations</code> is part of the <code>RDD</code> contract to specify placement preferences.</p> <p><code>getPreferredLocations</code> converts the given <code>Partition</code> to a <code>KafkaSourceRDDPartition</code> and...FIXME</p> <p>=== [[compute]] Computing Partition -- <code>compute</code> Method</p>","location":"datasources/kafka/KafkaSourceRDD/#source-scala"},{"title":"[source, scala]","text":"<p>compute(   thePart: Partition,   context: TaskContext ): Iterator[ConsumerRecord[Array[Byte], Array[Byte]]]</p>  <p>NOTE: <code>compute</code> is part of the <code>RDD</code> contract to compute a given partition.</p> <p><code>compute</code> uses <code>KafkaDataConsumer</code> utility to acquire a cached KafkaDataConsumer (for a partition).</p> <p><code>compute</code> &lt;&gt; (based on the <code>offsetRange</code> of the given partition that is assumed a <code>KafkaSourceRDDPartition</code>). <p><code>compute</code> returns a <code>NextIterator</code> so that <code>getNext</code> uses the <code>KafkaDataConsumer</code> to get a record.</p> <p>When the beginning and ending offsets (of the offset range) are equal, <code>compute</code> prints out the following INFO message to the logs, requests the <code>KafkaDataConsumer</code> to release and returns an empty iterator.</p> <pre><code>Beginning offset [fromOffset] is the same as ending offset skipping [topic] [partition]\n</code></pre> <p><code>compute</code> throws an <code>AssertionError</code> when the beginning offset (<code>fromOffset</code>) is after the ending offset (<code>untilOffset</code>):</p>","location":"datasources/kafka/KafkaSourceRDD/#source-scala_1"},{"title":"[options=\"wrap\"]","text":"","location":"datasources/kafka/KafkaSourceRDD/#optionswrap"},{"title":"Beginning offset [fromOffset] is after the ending offset [untilOffset] for topic [topic] partition [partition]. You either provided an invalid fromOffset, or the Kafka topic has been damaged","text":"<p>=== [[getPartitions]] <code>getPartitions</code> Method</p>","location":"datasources/kafka/KafkaSourceRDD/#beginning-offset-fromoffset-is-after-the-ending-offset-untiloffset-for-topic-topic-partition-partition-you-either-provided-an-invalid-fromoffset-or-the-kafka-topic-has-been-damaged"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaSourceRDD/#source-scala_2"},{"title":"getPartitions: Array[Partition]","text":"<p>NOTE: <code>getPartitions</code> is part of the <code>RDD</code> contract to...FIXME.</p> <p><code>getPartitions</code>...FIXME</p> <p>=== [[persist]] Persisting RDD -- <code>persist</code> Method</p>","location":"datasources/kafka/KafkaSourceRDD/#getpartitions-arraypartition"},{"title":"[source, scala]","text":"","location":"datasources/kafka/KafkaSourceRDD/#source-scala_3"},{"title":"persist: Array[Partition]","text":"<p>NOTE: <code>persist</code> is part of the <code>RDD</code> contract to persist an RDD.</p> <p><code>persist</code>...FIXME</p> <p>=== [[resolveRange]] <code>resolveRange</code> Internal Method</p>","location":"datasources/kafka/KafkaSourceRDD/#persist-arraypartition"},{"title":"[source, scala]","text":"<p>resolveRange(   consumer: KafkaDataConsumer,   range: KafkaSourceRDDOffsetRange ): KafkaSourceRDDOffsetRange</p>  <p><code>resolveRange</code>...FIXME</p> <p>NOTE: <code>resolveRange</code> is used when...FIXME</p>","location":"datasources/kafka/KafkaSourceRDD/#source-scala_4"},{"title":"KafkaTable","text":"<p><code>KafkaTable</code> is...FIXME</p>","location":"datasources/kafka/KafkaTable/"},{"title":"Kafka Options","text":"<p>Options with <code>kafka.</code> prefix (e.g. kafka.bootstrap.servers) are considered configuration properties for the Kafka consumers used on the driver and executors.</p>","location":"datasources/kafka/options/"},{"title":"assign <p>Topic subscription strategy that accepts a JSON with topic names and partitions, e.g.</p> <pre><code>{\"topicA\":[0,1],\"topicB\":[0,1]}\n</code></pre> <p>Exactly one topic subscription strategy is allowed (that <code>KafkaSourceProvider</code> validates before creating <code>KafkaSource</code>).</p>","text":"","location":"datasources/kafka/options/#assign"},{"title":"failOnDataLoss <p>Default: <code>true</code></p> <p>Used when:</p> <ul> <li><code>KafkaSourceProvider</code> is requested for failOnDataLoss configuration property</li> </ul>","text":"","location":"datasources/kafka/options/#failondataloss"},{"title":"includeHeaders <p>Default: <code>false</code></p>","text":"","location":"datasources/kafka/options/#includeheaders"},{"title":"kafka.bootstrap.servers <p>(required) <code>bootstrap.servers</code> configuration property of the Kafka consumers used on the driver and executors</p> <p>Default: <code>(empty)</code></p>","text":"","location":"datasources/kafka/options/#kafkabootstrapservers"},{"title":"kafkaConsumer.pollTimeoutMs <p>The time (in milliseconds) spent waiting in <code>Consumer.poll</code> if data is not available in the buffer.</p> <p>Default: <code>spark.network.timeout</code> or <code>120s</code></p>","text":"","location":"datasources/kafka/options/#kafkaconsumerpolltimeoutms"},{"title":"maxOffsetsPerTrigger <p>Number of records to fetch per trigger (to limit the number of records to fetch).</p> <p>Default: <code>(undefined)</code></p> <p>Unless defined, <code>KafkaSource</code> requests KafkaOffsetReader for the latest offsets.</p>","text":"","location":"datasources/kafka/options/#maxoffsetspertrigger"},{"title":"minPartitions <p>Minimum number of partitions per executor (given Kafka partitions)</p> <p>Default: <code>(undefined)</code></p> <p>Must be undefined (default) or greater than <code>0</code></p> <p>When undefined (default) or smaller than the number of <code>TopicPartitions</code> with records to consume from, KafkaMicroBatchReader uses KafkaOffsetRangeCalculator to find the preferred executor for every <code>TopicPartition</code> (and the available executors).</p>","text":"","location":"datasources/kafka/options/#minpartitions"},{"title":"startingOffsets <p>Starting offsets</p> <p>Default: <code>latest</code></p> <p>Possible values:</p> <ul> <li> <p><code>latest</code></p> </li> <li> <p><code>earliest</code></p> </li> <li> <p>JSON with topics, partitions and their starting offsets, e.g.</p> <pre><code>{\"topicA\":{\"part\":offset,\"p1\":-1},\"topicB\":{\"0\":-2}}\n</code></pre> </li> </ul>  <p>Tip</p> <p>Use Scala's tripple quotes for the JSON for topics, partitions and offsets.</p> <pre><code>option(\n  \"startingOffsets\",\n  \"\"\"{\"topic1\":{\"0\":5,\"4\":-1},\"topic2\":{\"0\":-2}}\"\"\")\n</code></pre>","text":"","location":"datasources/kafka/options/#startingoffsets"},{"title":"subscribe <p>Topic subscription strategy that accepts topic names as a comma-separated string, e.g.</p> <pre><code>topic1,topic2,topic3\n</code></pre> <p>Exactly one topic subscription strategy is allowed (that <code>KafkaSourceProvider</code> validates before creating <code>KafkaSource</code>).</p>","text":"","location":"datasources/kafka/options/#subscribe"},{"title":"subscribepattern <p>Topic subscription strategy that uses Java's java.util.regex.Pattern for the topic subscription regex pattern of topics to subscribe to, e.g.</p> <pre><code>topic\\d\n</code></pre>  <p>Tip</p> <p>Use Scala's tripple quotes for the regular expression for topic subscription regex pattern.</p> <pre><code>option(\"subscribepattern\", \"\"\"topic\\d\"\"\")\n</code></pre>  <p>Exactly one topic subscription strategy is allowed (that <code>KafkaSourceProvider</code> validates before creating <code>KafkaSource</code>).</p>","text":"","location":"datasources/kafka/options/#subscribepattern"},{"title":"topic <p>Optional topic name to use for writing a streaming query</p> <p>Default: <code>(empty)</code></p> <p>Unless defined, Kafka data source uses the topic names as defined in the <code>topic</code> field in the incoming data.</p>","text":"","location":"datasources/kafka/options/#topic"},{"title":"Memory Data Source","text":"<p>Memory Data Source is made up of the following two base implementations to support the older DataSource API V1 and the modern DataSource API V2:</p> <ul> <li> <p>MemoryStreamBase</p> </li> <li> <p>MemorySinkBase</p> </li> </ul> <p>Memory data source supports Micro-Batch and Continuous stream processing modes.</p> <p>[cols=\"30,35,35\",options=\"header\",width=\"100%\"] |=== | Stream Processing | Source | Sink</p> <p>| Micro-Batch | MemoryStream | MemorySink</p> <p>| Continuous | ContinuousMemoryStream | MemorySinkV2</p> <p>|===</p>","location":"datasources/memory/"},{"title":"[CAUTION]","text":"<p>Memory Data Source is not for production use due to design contraints, e.g. infinite in-memory collection of lines read and no fault recovery.</p>","location":"datasources/memory/#caution"},{"title":"<code>MemoryStream</code> is designed primarily for unit tests, tutorials and debugging.","text":"<p>=== [[memory-sink]] Memory Sink</p> <p>Memory sink requires that a streaming query has a name (defined using DataStreamWriter.queryName or <code>queryName</code> option).</p> <p>Memory sink may optionally define checkpoint location using <code>checkpointLocation</code> option that is used to recover from for Complete output mode only.</p>","location":"datasources/memory/#memorystream-is-designed-primarily-for-unit-tests-tutorials-and-debugging"},{"title":"Memory Sink and CreateViewCommand","text":"<p>When a streaming query with <code>memory</code> sink is started, DataStreamWriter uses <code>Dataset.createOrReplaceTempView</code> operator to create or replace a local temporary view with the name of the query (which is required).</p> <p></p>","location":"datasources/memory/#memory-sink-and-createviewcommand"},{"title":"Examples","text":"<p>.Memory Source in Micro-Batch Stream Processing [source, scala]</p>  <p>val spark: SparkSession = ???</p> <p>implicit val ctx = spark.sqlContext</p> <p>import org.apache.spark.sql.execution.streaming.MemoryStream // It uses two implicits: Encoder[Int] and SQLContext val intsIn = MemoryStream[Int]</p> <p>val ints = intsIn.toDF   .withColumn(\"t\", current_timestamp())   .withWatermark(\"t\", \"5 minutes\")   .groupBy(window($\"t\", \"5 minutes\") as \"window\")   .agg(count(\"*\") as \"total\")</p> <p>import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val totalsOver5mins = ints.   writeStream.   format(\"memory\").   queryName(\"totalsOver5mins\").   outputMode(OutputMode.Append).   trigger(Trigger.ProcessingTime(10.seconds)).   start</p> <p>val zeroOffset = intsIn.addData(0, 1, 2) totalsOver5mins.processAllAvailable() spark.table(\"totalsOver5mins\").show</p> <p>scala&gt; intsOut.show +-----+ |value| +-----+ |    0| |    1| |    2| +-----+</p>","location":"datasources/memory/#examples"},{"title":"memoryQuery.stop()","text":"<p>.Memory Sink in Micro-Batch Stream Processing [source, scala]</p>  <p>val queryName = \"memoryDemo\" val sq = spark   .readStream   .format(\"rate\")   .load   .writeStream   .format(\"memory\")   .queryName(queryName)   .start</p> <p>// The name of the streaming query is an in-memory table val showAll = sql(s\"select * from $queryName\") scala&gt; showAll.show(truncate = false) +-----------------------+-----+ |timestamp              |value| +-----------------------+-----+ |2019-10-10 15:19:16.431|42   | |2019-10-10 15:19:17.431|43   | +-----------------------+-----+</p> <p>import org.apache.spark.sql.streaming.StreamingQuery assert(sq.isInstanceOf[StreamingQuery])</p> <p>import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val se = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery</p> <p>import org.apache.spark.sql.execution.streaming.MemorySink val sink = se.sink.asInstanceOf[MemorySink]</p> <p>assert(sink.toString == \"MemorySink\")</p>","location":"datasources/memory/#memoryquerystop"},{"title":"sink.clear()","text":"","location":"datasources/memory/#sinkclear"},{"title":"ContinuousMemoryStream","text":"<p>== [[ContinuousMemoryStream]] ContinuousMemoryStream</p> <p><code>ContinuousMemoryStream</code> is...FIXME</p>","location":"datasources/memory/ContinuousMemoryStream/"},{"title":"MemoryPlan Logical Operator","text":"<p><code>MemoryPlan</code> is a leaf logical operator (i.e. <code>LogicalPlan</code>) that is used to query the data that has been written into a MemorySink. <code>MemoryPlan</code> is created when starting continuous writing (to a <code>MemorySink</code>).</p> <p>TIP: See the example in MemoryStream.</p> <pre><code>scala&gt; intsOut.explain(true)\n== Parsed Logical Plan ==\nSubqueryAlias memstream\n+- MemoryPlan org.apache.spark.sql.execution.streaming.MemorySink@481bf251, [value#21]\n\n== Analyzed Logical Plan ==\nvalue: int\nSubqueryAlias memstream\n+- MemoryPlan org.apache.spark.sql.execution.streaming.MemorySink@481bf251, [value#21]\n\n== Optimized Logical Plan ==\nMemoryPlan org.apache.spark.sql.execution.streaming.MemorySink@481bf251, [value#21]\n\n== Physical Plan ==\nLocalTableScan [value#21]\n</code></pre> <p>When executed, <code>MemoryPlan</code> is translated to <code>LocalTableScanExec</code> physical operator (similar to <code>LocalRelation</code> logical operator) in <code>BasicOperators</code> execution planning strategy.</p>","location":"datasources/memory/MemoryPlan/"},{"title":"MemorySink","text":"<p><code>MemorySink</code> is a streaming sink that &lt;&gt;. <p><code>MemorySink</code> is intended only for testing or demos.</p> <p><code>MemorySink</code> is used for <code>memory</code> format and requires a query name (by <code>queryName</code> method or <code>queryName</code> option).</p> <p>NOTE: <code>MemorySink</code> was introduced in the https://github.com/apache/spark/pull/12119[pull request for [SPARK-14288][SQL] Memory Sink for streaming].</p> <p>Use <code>toDebugString</code> to see the batches.</p> <p>Its aim is to allow users to test streaming applications in the Spark shell or other local tests.</p> <p>You can set <code>checkpointLocation</code> using <code>option</code> method or it will be set to spark.sql.streaming.checkpointLocation property.</p> <p>If <code>spark.sql.streaming.checkpointLocation</code> is set, the code uses <code>$location/$queryName</code> directory.</p> <p>Finally, when no <code>spark.sql.streaming.checkpointLocation</code> is set, a temporary directory <code>memory.stream</code> under <code>java.io.tmpdir</code> is used with <code>offsets</code> subdirectory inside.</p> <p>NOTE: The directory is cleaned up at shutdown using <code>ShutdownHookManager.registerShutdownDeleteDir</code>.</p> <p>It creates <code>MemorySink</code> instance based on the schema of the DataFrame it operates on.</p> <p>It creates a new DataFrame using <code>MemoryPlan</code> with <code>MemorySink</code> instance created earlier and registers it as a temporary table (using spark-sql-dataframe.md#registerTempTable[DataFrame.registerTempTable] method).</p> <p>NOTE: At this point you can query the table as if it were a regular non-streaming table using spark-sql-sqlcontext.md#sql[sql] method.</p> <p>A new StreamingQuery.md[StreamingQuery] is started (using StreamingQueryManager.startQuery) and returned.</p> <p>[[logging]] [TIP] ==== Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.streaming.MemorySink</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.streaming.MemorySink=ALL\n</code></pre>","location":"datasources/memory/MemorySink/"},{"title":"Refer to &lt;&gt;. <p>=== [[creating-instance]] Creating MemorySink Instance</p> <p><code>MemorySink</code> takes the following to be created:</p> <ul> <li>[[schema]] Output schema</li> <li>[[outputMode]] OutputMode</li> </ul> <p><code>MemorySink</code> initializes the &lt;&gt; internal property. <p>=== [[batches]] In-Memory Buffer of Streaming Batches -- <code>batches</code> Internal Property</p>","text":"","location":"datasources/memory/MemorySink/#refer-to"},{"title":"[source, scala]","text":"","location":"datasources/memory/MemorySink/#source-scala"},{"title":"batches: ArrayBuffer[AddedData]","text":"<p><code>batches</code> holds data from streaming batches that have been &lt;&gt; (written) to this sink. <p>For Append and Update output modes, <code>batches</code> holds rows from all batches.</p> <p>For Complete output mode, <code>batches</code> holds rows from the last batch only.</p> <p><code>batches</code> can be cleared (emptied) using &lt;&gt;. <p>=== [[addBatch]] Adding Batch of Data to Sink -- <code>addBatch</code> Method</p>","location":"datasources/memory/MemorySink/#batches-arraybufferaddeddata"},{"title":"[source, scala]","text":"<p>addBatch(   batchId: Long,   data: DataFrame): Unit</p>  <p><code>addBatch</code> branches off based on whether the given <code>batchId</code> has already been &lt;&gt; or &lt;&gt;. <p>A batch ID is considered committed when the given batch ID is greater than the &lt;&gt; (if available). <p><code>addBatch</code> is part of the Sink abstraction.</p> <p>==== [[addBatch-not-committed]] Batch Not Committed</p> <p>With the <code>batchId</code> not committed, <code>addBatch</code> prints out the following DEBUG message to the logs:</p> <pre><code>Committing batch [batchId] to [this]\n</code></pre> <p><code>addBatch</code> collects records from the given <code>data</code>.</p> <p>NOTE: <code>addBatch</code> uses <code>Dataset.collect</code> operator to collect records.</p> <p>For &lt;&gt; and &lt;&gt; output modes, <code>addBatch</code> adds the data (as a <code>AddedData</code>) to the &lt;&gt; internal registry. <p>For &lt;&gt; output mode, <code>addBatch</code> clears the &lt;&gt; internal registry first before adding the data (as a <code>AddedData</code>). <p>For any other output mode, <code>addBatch</code> reports an <code>IllegalArgumentException</code>:</p> <pre><code>Output mode [outputMode] is not supported by MemorySink\n</code></pre> <p>==== [[addBatch-committed]] Batch Committed</p> <p>With the <code>batchId</code> committed, <code>addBatch</code> simply prints out the following DEBUG message to the logs and returns.</p> <pre><code>Skipping already committed batch: [batchId]\n</code></pre> <p>=== [[clear]] Clearing Up Internal Batch Buffer -- <code>clear</code> Method</p>","location":"datasources/memory/MemorySink/#source-scala_1"},{"title":"[source, scala]","text":"","location":"datasources/memory/MemorySink/#source-scala_2"},{"title":"clear(): Unit","text":"<p><code>clear</code> simply removes (clears) all data from the &lt;&gt; internal registry. <p>NOTE: <code>clear</code> is used exclusively in tests.</p>","location":"datasources/memory/MemorySink/#clear-unit"},{"title":"MemorySinkBase","text":"<p>[[contract]] .MemorySinkBase Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| allData a| [[allData]]</p>","location":"datasources/memory/MemorySinkBase/"},{"title":"[source, scala]","text":"","location":"datasources/memory/MemorySinkBase/#source-scala"},{"title":"allData: Seq[Row]","text":"<p>| dataSinceBatch a| [[dataSinceBatch]]</p>","location":"datasources/memory/MemorySinkBase/#alldata-seqrow"},{"title":"[source, scala]","text":"<p>dataSinceBatch(   sinceBatchId: Long): Seq[Row]</p>  <p>| latestBatchData a| [[latestBatchData]]</p>","location":"datasources/memory/MemorySinkBase/#source-scala_1"},{"title":"[source, scala]","text":"","location":"datasources/memory/MemorySinkBase/#source-scala_2"},{"title":"latestBatchData: Seq[Row]","text":"<p>| latestBatchId a| [[latestBatchId]]</p>","location":"datasources/memory/MemorySinkBase/#latestbatchdata-seqrow"},{"title":"[source, scala]","text":"","location":"datasources/memory/MemorySinkBase/#source-scala_3"},{"title":"latestBatchId: Option[Long]","text":"<p>|===</p> <p>[[implementations]] .MemorySinkBases [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | MemorySinkBase | Description</p> <p>| MemorySink | [[MemorySink]] Streaming sink for Micro-Batch Stream Processing (based on Data Source API V1)</p> <p>| MemorySinkV2 | [[MemorySinkV2]] Streaming sink for Continuous Stream Processing (based on Data Source API V2)</p> <p>|===</p>","location":"datasources/memory/MemorySinkBase/#latestbatchid-optionlong"},{"title":"MemorySinkV2 \u2014 Writable Streaming Sink for Continuous Stream Processing","text":"<p><code>MemorySinkV2</code> is a <code>DataSourceV2</code> for memory data source format in Continuous Stream Processing.</p>  <p>Tip</p> <p>Read up on DataSourceV2 in The Internals of Spark SQL online book.</p>  <p><code>MemorySinkV2</code> is a custom MemorySinkBase.</p>","location":"datasources/memory/MemorySinkV2/"},{"title":"MemoryStream -- Streaming Reader for Micro-Batch Stream Processing","text":"<p><code>MemoryStream</code> is...FIXME</p> <p>[[logging]] [TIP] ==== Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.streaming.MemoryStream</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.streaming.MemoryStream=ALL\n</code></pre>","location":"datasources/memory/MemoryStream/"},{"title":"Refer to &lt;&gt;. <p>=== [[creating-instance]] Creating MemoryStream Instance</p> <p><code>MemoryStream</code> takes the following to be created:</p> <ul> <li>[[id]] ID</li> <li>[[sqlContext]] <code>SQLContext</code></li> </ul> <p><code>MemoryStream</code> initializes the &lt;&gt;. <p>=== [[apply]] Creating MemoryStream Instance -- <code>apply</code> Object Factory</p>","text":"","location":"datasources/memory/MemoryStream/#refer-to"},{"title":"[source, scala]","text":"<p>applyA : Encoder: MemoryStream[A]</p>  <p><code>apply</code> uses an <code>memoryStreamId</code> internal counter to &lt;&gt; with a unique &lt;&gt; and the implicit <code>SQLContext</code>. <p>=== [[addData]] Adding Data to Source -- <code>addData</code> Method</p>","location":"datasources/memory/MemoryStream/#source-scala"},{"title":"[source, scala]","text":"<p>addData(   data: TraversableOnce[A]): Offset</p>  <p><code>addData</code> adds the given <code>data</code> to the &lt;&gt; internal registry. <p>Internally, <code>addData</code> prints out the following DEBUG message to the logs:</p> <pre><code>Adding: [data]\n</code></pre> <p>In the end, <code>addData</code> increments the &lt;&gt; and adds the data to the &lt;&gt; internal registry. <p>=== [[getBatch]] Generating Next Streaming Batch -- <code>getBatch</code> Method</p> <p><code>getBatch</code> is a part of the Source abstraction.</p> <p>When executed, <code>getBatch</code> uses the internal &lt;&gt; collection to return requested offsets. <p>You should see the following DEBUG message in the logs:</p> <pre><code>DEBUG MemoryStream: MemoryBatch [[startOrdinal], [endOrdinal]]: [newBlocks]\n</code></pre> <p>=== [[logicalPlan]] Logical Plan -- <code>logicalPlan</code> Internal Property</p>","location":"datasources/memory/MemoryStream/#source-scala_1"},{"title":"[source, scala]","text":"","location":"datasources/memory/MemoryStream/#source-scala_2"},{"title":"logicalPlan: LogicalPlan","text":"<p><code>logicalPlan</code> is part of the MemoryStreamBase abstraction.</p> <p><code>logicalPlan</code> is simply a StreamingExecutionRelation (for this memory source and the attributes).</p> <p><code>MemoryStream</code> uses StreamingExecutionRelation logical plan to build Datasets or DataFrames when requested.</p> <pre><code>scala&gt; val ints = MemoryStream[Int]\nints: org.apache.spark.sql.execution.streaming.MemoryStream[Int] = MemoryStream[value#13]\n\nscala&gt; ints.toDS.queryExecution.logical.isStreaming\nres14: Boolean = true\n\nscala&gt; ints.toDS.queryExecution.logical\nres15: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan = MemoryStream[value#13]\n</code></pre> <p>=== [[schema]] Schema (schema method)</p> <p><code>MemoryStream</code> works with the data of the spark-sql-schema.md[schema] as described by the spark-sql-Encoder.md[Encoder] (of the <code>Dataset</code>).</p> <p>=== [[toString]] Textual Representation -- <code>toString</code> Method</p>","location":"datasources/memory/MemoryStream/#logicalplan-logicalplan"},{"title":"[source, scala]","text":"","location":"datasources/memory/MemoryStream/#source-scala_3"},{"title":"toString: String","text":"<p>NOTE: <code>toString</code> is part of the ++https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object] contract for the string representation of the object.</p> <p><code>toString</code> uses the &lt;&gt; to return the following textual representation: <pre><code>MemoryStream[[output]]\n</code></pre> <p>=== [[planInputPartitions]] Plan Input Partitions -- <code>planInputPartitions</code> Method</p>","location":"datasources/memory/MemoryStream/#tostring-string"},{"title":"[source, scala]","text":"","location":"datasources/memory/MemoryStream/#source-scala_4"},{"title":"planInputPartitions(): java.util.List[InputPartition[InternalRow]]","text":"<p>NOTE: <code>planInputPartitions</code> is part of the <code>DataSourceReader</code> contract in Spark SQL for the number of <code>InputPartitions</code> to use as RDD partitions (when <code>DataSourceV2ScanExec</code> physical operator is requested for the partitions of the input RDD).</p> <p><code>planInputPartitions</code>...FIXME</p> <p><code>planInputPartitions</code> prints out a DEBUG message to the logs with the &lt;&gt; (with the batches after the &lt;&gt;). <p><code>planInputPartitions</code>...FIXME</p> <p>=== [[generateDebugString]] <code>generateDebugString</code> Internal Method</p>","location":"datasources/memory/MemoryStream/#planinputpartitions-javautillistinputpartitioninternalrow"},{"title":"[source, scala]","text":"<p>generateDebugString(   rows: Seq[UnsafeRow],   startOrdinal: Int,   endOrdinal: Int): String</p>  <p><code>generateDebugString</code> resolves and binds the encoder for the data.</p> <p>In the end, <code>generateDebugString</code> returns the following string:</p> <pre><code>MemoryBatch [[startOrdinal], [endOrdinal]]: [rows]\n</code></pre> <p>NOTE: <code>generateDebugString</code> is used exclusively when <code>MemoryStream</code> is requested to &lt;&gt;.","location":"datasources/memory/MemoryStream/#source-scala_5"},{"title":"Internal Properties","text":"<p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| batches a| [[batches]] Batch data (<code>ListBuffer[Array[UnsafeRow]]</code>)</p> <p>| currentOffset a| [[currentOffset]] Current offset</p> <p>| lastOffsetCommitted a| [[lastOffsetCommitted]] Last committed offset</p> <p>| output a| [[output]] Output schema (<code>Seq[Attribute]</code>) of the logical query plan</p> <p>Used exclusively for &lt;&gt; <p>|===</p>","location":"datasources/memory/MemoryStream/#internal-properties"},{"title":"MemoryStreamBase -- Base Contract for Memory Sources","text":"<p><code>MemoryStreamBase</code> is the &lt;&gt; of...FIXME <p>[[contract]] .MemoryStreamBase Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| addData a| [[addData]]</p>","location":"datasources/memory/MemoryStreamBase/"},{"title":"[source, scala]","text":"<p>addData(   data: TraversableOnce[A]): Offset</p>  <p>| logicalPlan a| [[logicalPlan]]</p>","location":"datasources/memory/MemoryStreamBase/#source-scala"},{"title":"[source, scala]","text":"","location":"datasources/memory/MemoryStreamBase/#source-scala_1"},{"title":"logicalPlan: LogicalPlan","text":"<p>|===</p> <p>[[implementations]] .MemoryStreamBases [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | MemoryStreamBase | Description</p> <p>| ContinuousMemoryStream | [[ContinuousMemoryStream]]</p> <p>|===</p> <p>=== [[creating-instance]] Creating MemoryStreamBase Instance</p> <p><code>MemoryStreamBase</code> takes the following to be created:</p> <ul> <li>[[sqlContext]] <code>SQLContext</code></li> </ul> <p>NOTE: <code>MemoryStreamBase</code> is a Scala abstract class and cannot be &lt;&gt; directly. It is created indirectly for the &lt;&gt;. <p>=== [[toDS]] Creating Streaming Dataset -- <code>toDS</code> Method</p>","location":"datasources/memory/MemoryStreamBase/#logicalplan-logicalplan"},{"title":"[source, scala]","text":"","location":"datasources/memory/MemoryStreamBase/#source-scala_2"},{"title":"toDS(): Dataset[A]","text":"<p><code>toDS</code> simply creates a <code>Dataset</code> (for the &lt;&gt; and the &lt;&gt;) <p>=== [[toDF]] Creating Streaming DataFrame -- <code>toDF</code> Method</p>","location":"datasources/memory/MemoryStreamBase/#tods-dataseta"},{"title":"[source, scala]","text":"","location":"datasources/memory/MemoryStreamBase/#source-scala_3"},{"title":"toDF(): DataFrame","text":"<p><code>toDF</code> simply creates a <code>Dataset</code> of rows (for the &lt;&gt; and the &lt;&gt;) <p>=== [[internal-properties]] Internal Properties</p> <p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| attributes a| [[attributes]] Schema attributes of the &lt;&gt; (<code>Seq[AttributeReference]</code>) <p>Used when...FIXME</p> <p>| encoder a| [[encoder]] Spark SQL's <code>ExpressionEncoder</code> for the data</p> <p>Used when...FIXME</p> <p>|===</p>","location":"datasources/memory/MemoryStreamBase/#todf-dataframe"},{"title":"Rate Data Source","text":"","location":"datasources/rate/"},{"title":"RateStreamContinuousReader","text":"<p><code>RateStreamContinuousReader</code> is a ContinuousReader that...FIXME</p>","location":"datasources/rate/RateStreamContinuousReader/"},{"title":"RateStreamProvider","text":"<p><code>RateStreamProvider</code> is...FIXME</p>","location":"datasources/rate/RateStreamProvider/"},{"title":"Rate Per Micro-Batch Data Source","text":"<p>Rate Per Micro-Batch Data Source provides a consistent number of rows per microbatch.</p> <p>From this commit:</p>  <p>This proposes to introduce a new data source having short name as \"rate-micro-batch\", which produces similar input rows as \"rate\" (increment long values with timestamps), but ensures that each micro-batch has a \"predictable\" set of input rows.</p> <p>\"rate-micro-batch\" data source receives a config to specify the number of rows per micro-batch, which defines the set of input rows for further micro-batches. For example, if the number of rows per micro-batch is set to 1000, the first batch would have 1000 rows having value range as <code>0~999</code>, the second batch would have 1000 rows having value range as <code>1000~1999</code>, and so on. This characteristic brings different use cases compared to rate data source, as we can't predict the input rows for rate data source like this.</p> <p>For generated time (timestamp column), the data source applies the same mechanism to make the value of column be predictable. <code>startTimestamp</code> option defines the starting value of generated time, and <code>advanceMillisPerBatch</code> option defines how much time the generated time should advance per micro-batch. All input rows in the same micro-batch will have same timestamp.</p>  <p>Rate Per Micro-Batch data source is a new feature of Apache Spark 3.3.0 (SPARK-37062).</p>","location":"datasources/rate-micro-batch/"},{"title":"Internals","text":"<p>Rate Per Micro-Batch Data Source is registered by RatePerMicroBatchProvider to be available under rate-micro-batch alias.</p> <p><code>RatePerMicroBatchProvider</code> uses RatePerMicroBatchTable as the Table (Spark SQL).</p> <p>When requested for a MicroBatchStream, <code>RatePerMicroBatchTable</code> creates a RatePerMicroBatchStream with extra support for Trigger.AvailableNow mode.</p> <p>Rate Per Micro-Batch Data Source supports options (esp. rowsPerBatch and advanceMillisPerBatch for Trigger.AvailableNow mode).</p>","location":"datasources/rate-micro-batch/#internals"},{"title":"RatePerMicroBatchProvider","text":"<p><code>RatePerMicroBatchProvider</code> is a <code>SimpleTableProvider</code> (Spark SQL).</p>","location":"datasources/rate-micro-batch/RatePerMicroBatchProvider/"},{"title":"DataSourceRegister <p><code>RatePerMicroBatchProvider</code> is a <code>DataSourceRegister</code> (Spark SQL) that registers <code>rate-micro-batch</code> alias.</p>","text":"","location":"datasources/rate-micro-batch/RatePerMicroBatchProvider/#datasourceregister"},{"title":"Creating Table <pre><code>getTable(\n  options: CaseInsensitiveStringMap): Table\n</code></pre> <p><code>getTable</code> creates a RatePerMicroBatchTable with the options (given the <code>CaseInsensitiveStringMap</code>).</p>  <p><code>getTable</code> is part of the <code>SimpleTableProvider</code> (Spark SQL) abstraction.</p>","text":"","location":"datasources/rate-micro-batch/RatePerMicroBatchProvider/#creating-table"},{"title":"RatePerMicroBatchStream","text":"<p><code>RatePerMicroBatchStream</code> is a MicroBatchStream that SupportsTriggerAvailableNow.</p>","location":"datasources/rate-micro-batch/RatePerMicroBatchStream/"},{"title":"Creating Instance","text":"<p><code>RatePerMicroBatchStream</code> takes the following to be created:</p> <ul> <li> rowsPerBatch <li> numPartitions <li> startTimestamp <li> advanceMsPerBatch <li> Options  <p><code>RatePerMicroBatchStream</code> is created when:</p> <ul> <li><code>RatePerMicroBatchTable</code> is requested to create a ScanBuilder</li> </ul>","location":"datasources/rate-micro-batch/RatePerMicroBatchStream/#creating-instance"},{"title":"isTriggerAvailableNow Flag <p><code>RatePerMicroBatchStream</code> defines <code>isTriggerAvailableNow</code> flag to determine whether it is executed in Trigger.AvailableNow mode or not (based on prepareForTriggerAvailableNow).</p> <p>By default, <code>isTriggerAvailableNow</code> flag is off (<code>false</code>).</p> <p><code>isTriggerAvailableNow</code> is used when:</p> <ul> <li><code>RatePerMicroBatchStream</code> is requested for the latest offset</li> </ul>","text":"","location":"datasources/rate-micro-batch/RatePerMicroBatchStream/#istriggeravailablenow-flag"},{"title":"prepareForTriggerAvailableNow <pre><code>prepareForTriggerAvailableNow(): Unit\n</code></pre> <p><code>prepareForTriggerAvailableNow</code> is part of the SupportsTriggerAvailableNow abstraction.</p>  <p><code>prepareForTriggerAvailableNow</code> turns the isTriggerAvailableNow flag on (for Trigger.AvailableNow mode).</p>","text":"","location":"datasources/rate-micro-batch/RatePerMicroBatchStream/#preparefortriggeravailablenow"},{"title":"Latest Offset <pre><code>latestOffset(\n  startOffset: Offset,\n  limit: ReadLimit): Offset\n</code></pre> <p><code>latestOffset</code> is part of the SupportsAdmissionControl abstraction.</p>  <p><code>latestOffset</code> is different whether it is executed in Trigger.AvailableNow mode or not (based on isTriggerAvailableNow flag).</p> <p><code>latestOffset</code> calculates next offset just once for Trigger.AvailableNow and the offsetForTriggerAvailableNow registry has no value assigned yet.</p> <p>In other words, for isTriggerAvailableNow, <code>latestOffset</code> returns the value of the offsetForTriggerAvailableNow registry (that remains the same for every <code>latestOffset</code>).</p> <p>For all the other triggers (when isTriggerAvailableNow is disabled), <code>latestOffset</code> calculates next offset.</p>","text":"","location":"datasources/rate-micro-batch/RatePerMicroBatchStream/#latest-offset"},{"title":"Calculating Next Offset <pre><code>calculateNextOffset(\n  start: Offset): Offset\n</code></pre> <p><code>calculateNextOffset</code> extractOffsetAndTimestamp from the given start Offset.</p> <p><code>calculateNextOffset</code> creates a <code>RatePerMicroBatchStreamOffset</code> for the end offset (that is rowsPerBatch rows long):</p> <ul> <li>Increments the start offset by the rowsPerBatch for the end offset</li> <li>Increments the start offset timestamp by the advanceMsPerBatch for the end offset timestamp</li> </ul>","text":"","location":"datasources/rate-micro-batch/RatePerMicroBatchStream/#calculating-next-offset"},{"title":"RatePerMicroBatchTable","text":"<p><code>RatePerMicroBatchTable</code> is a <code>Table</code> (Spark SQL) that <code>SupportsRead</code> (Spark SQL).</p>","location":"datasources/rate-micro-batch/RatePerMicroBatchTable/"},{"title":"Creating Instance","text":"<p><code>RatePerMicroBatchTable</code> takes the following to be created:</p> <ul> <li> rowsPerBatch <li> numPartitions <li> startTimestamp <li> advanceMillisPerBatch  <p><code>RatePerMicroBatchTable</code> is created when:</p> <ul> <li><code>RatePerMicroBatchProvider</code> is requested for the table</li> </ul>","location":"datasources/rate-micro-batch/RatePerMicroBatchTable/#creating-instance"},{"title":"schema <pre><code>schema(): StructType\n</code></pre>    Name Data Type     timestamp TimestampType   value LongType    <p><code>schema</code> is part of the <code>Table</code> (Spark SQL) abstraction.</p>","text":"","location":"datasources/rate-micro-batch/RatePerMicroBatchTable/#schema"},{"title":"Creating ScanBuilder <pre><code>newScanBuilder(\n  options: CaseInsensitiveStringMap): ScanBuilder\n</code></pre> <p><code>newScanBuilder</code> is part of the <code>SupportsRead</code> (Spark SQL) abstraction.</p>  <p><code>newScanBuilder</code> creates a new <code>Scan</code> (Spark SQL) that creates a RatePerMicroBatchStream when requested for a <code>MicroBatchStream</code> (Spark SQL).</p>","text":"","location":"datasources/rate-micro-batch/RatePerMicroBatchTable/#creating-scanbuilder"},{"title":"Options","text":"","location":"datasources/rate-micro-batch/options/"},{"title":"advanceMillisPerBatch <p>default: <code>1000</code></p>","text":"","location":"datasources/rate-micro-batch/options/#advancemillisperbatch"},{"title":"numPartitions <p>default: <code>SparkSession.active.sparkContext.defaultParallelism</code></p>","text":"","location":"datasources/rate-micro-batch/options/#numpartitions"},{"title":"rowsPerBatch <p>default: <code>0</code></p>","text":"","location":"datasources/rate-micro-batch/options/#rowsperbatch"},{"title":"startTimestamp <p>default: <code>0</code></p>","text":"","location":"datasources/rate-micro-batch/options/#starttimestamp"},{"title":"Text Socket Data Source","text":"<p>Text Socket Data Source comes with the following main abstractions:</p> <ul> <li>TextSocketSourceProvider</li> <li>TextSocketSource</li> </ul>","location":"datasources/socket/"},{"title":"TextSocketSource","text":"<p><code>TextSocketSource</code> is a streaming source that reads text lines from a socket at the <code>host</code> and <code>port</code>.</p> <p><code>TextSocketSource</code> uses lines internal in-memory buffer to keep all of the lines that were read from a socket forever.</p>  <p>Caution</p> <p>This source is not for production use due to design contraints, e.g. infinite in-memory collection of lines read and no fault recovery.</p> <p>It is designed only for tutorials and debugging.</p>","location":"datasources/socket/TextSocketSource/"},{"title":"Creating Instance","text":"<p>When <code>TextSocketSource</code> is created (see TextSocketSourceProvider), it gets 4 parameters passed in:</p> <ol> <li><code>host</code></li> <li><code>port</code></li> <li>includeTimestamp flag</li> <li>spark-sql-sqlcontext.md[SQLContext]</li> </ol> <p>CAUTION: It appears that the source did not get \"renewed\" to use spark-sql-sparksession.md[SparkSession] instead.</p> <p>It opens a socket at given <code>host</code> and <code>port</code> parameters and reads a buffering character-input stream using the default charset and the default-sized input buffer (of <code>8192</code> bytes) line by line.</p> <p>CAUTION: FIXME Review Java's <code>Charset.defaultCharset()</code></p> <p>It starts a <code>readThread</code> daemon thread (called <code>TextSocketSource(host, port)</code>) to read lines from the socket. The lines are added to the internal &lt;&gt; buffer.","location":"datasources/socket/TextSocketSource/#creating-instance"},{"title":"lines Internal Buffer <pre><code>lines: ArrayBuffer[(String, Timestamp)]\n</code></pre> <p><code>lines</code> is the internal buffer of all the lines <code>TextSocketSource</code> read from the socket.</p> <p>=== [[getOffset]] Maximum Available Offset (getOffset method)</p> <p><code>TextSocketSource</code>'s offset can either be none or <code>LongOffset</code> of the number of lines in the internal &lt;&gt; buffer. <p><code>getOffset</code> is a part of the Source abstraction.</p> <p>=== [[schema]] Schema (schema method)</p> <p><code>TextSocketSource</code> supports two spark-sql-schema.md[schemas]:</p> <ol> <li>A single <code>value</code> field of String type.</li> <li><code>value</code> field of <code>StringType</code> type and <code>timestamp</code> field of spark-sql-DataType.md#TimestampType[TimestampType] type of format <code>yyyy-MM-dd HH:mm:ss</code>.</li> </ol>  <p>Tip</p> <p>Refer to sourceSchema for <code>TextSocketSourceProvider</code>.</p>","text":"","location":"datasources/socket/TextSocketSource/#lines-internal-buffer"},{"title":"Stopping TextSocketSource <p>When stopped, <code>TextSocketSource</code> closes the socket connection.</p>","text":"","location":"datasources/socket/TextSocketSource/#stopping-textsocketsource"},{"title":"Demo <pre><code>import org.apache.spark.sql.SparkSession\nval spark: SparkSession = SparkSession.builder.getOrCreate()\n\n// Connect to localhost:9999\n// You can use \"nc -lk 9999\" for demos\nval textSocket = spark.\n  readStream.\n  format(\"socket\").\n  option(\"host\", \"localhost\").\n  option(\"port\", 9999).\n  load\n\nimport org.apache.spark.sql.Dataset\nval lines: Dataset[String] = textSocket.as[String].map(_.toUpperCase)\n\nval query = lines.writeStream.format(\"console\").start\n\n// Start typing the lines in nc session\n// They will appear UPPERCASE in the terminal\n\n-------------------------------------------\nBatch: 0\n-------------------------------------------\n+---------+\n|    value|\n+---------+\n|UPPERCASE|\n+---------+\n\nscala&gt; query.explain\n== Physical Plan ==\n*SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true) AS value#21]\n+- *MapElements &lt;function1&gt;, obj#20: java.lang.String\n   +- *DeserializeToObject value#43.toString, obj#19: java.lang.String\n      +- LocalTableScan [value#43]\n\nscala&gt; query.stop\n</code></pre>","text":"","location":"datasources/socket/TextSocketSource/#demo"},{"title":"TextSocketSourceProvider","text":"<p><code>TextSocketSourceProvider</code> is a StreamSourceProvider for Text Socket Data Source.</p> <p><code>TextSocketSourceProvider</code> requires two options (that you can set using <code>option</code> method):</p> <ol> <li><code>host</code> which is the host name.</li> <li><code>port</code> which is the port number. It must be an integer.</li> </ol> <p><code>TextSocketSourceProvider</code> also supports &lt;&gt; option that is a boolean flag that you can use to include timestamps in the schema.","location":"datasources/socket/TextSocketSourceProvider/"},{"title":"DataSourceRegister","text":"<p><code>TextSocketSourceProvider</code> is a <code>DataSourceRegister</code> with the short name of socket.</p>","location":"datasources/socket/TextSocketSourceProvider/#datasourceregister"},{"title":"includeTimestamp Option","text":"","location":"datasources/socket/TextSocketSourceProvider/#includetimestamp-option"},{"title":"createSource <p><code>createSource</code> creates a TextSocketSource (with the <code>host</code> and <code>port</code>).</p>","text":"","location":"datasources/socket/TextSocketSourceProvider/#createsource"},{"title":"sourceSchema <p><code>sourceSchema</code> returns <code>textSocket</code> as the name of the source and the schema that can be one of the two available schemas:</p> <ol> <li> <p><code>SCHEMA_REGULAR</code> (default) which is a schema with a single <code>value</code> field of String type.</p> </li> <li> <p><code>SCHEMA_TIMESTAMP</code> when <code>&lt;&lt;includeTimestamp, includeTimestamp&gt;&gt;</code> flag option is set. It is not, i.e. <code>false</code>, by default. The schema are <code>value</code> field of <code>StringType</code> type and <code>timestamp</code> field of spark-sql-DataType.md#TimestampType[TimestampType] type of format <code>yyyy-MM-dd HH:mm:ss</code>.</p> </li> </ol> <p>TIP: Read about spark-sql-schema.md[schema].</p> <p>Internally, it starts by printing out the following WARN message to the logs:</p> <pre><code>The socket source should not be used for production applications! It does not support recovery and stores state indefinitely.\n</code></pre> <p>It then checks whether <code>host</code> and <code>port</code> parameters are defined and if not it throws a <code>AnalysisException</code>:</p> <pre><code>Set a host to read from with option(\"host\", ...).\n</code></pre>","text":"","location":"datasources/socket/TextSocketSourceProvider/#sourceschema"},{"title":"Demos","text":"<ul> <li>Arbitrary Stateful Streaming Aggregation with KeyValueGroupedDataset.flatMapGroupsWithState Operator</li> <li>current_timestamp Function For Processing Time in Streaming Queries</li> <li>Custom Streaming Sink (and Monitoring SQL Queries in web UI)</li> <li>Deep Dive into FileStreamSink</li> <li>Exploring Checkpointed State</li> <li>groupByKey Streaming Aggregation in Update Mode</li> <li>Internals of FlatMapGroupsWithStateExec Physical Operator</li> <li>Kafka Data Source</li> <li>Streaming Aggregation</li> <li>Streaming Query for Running Counts (Socket Source and Complete Output Mode)</li> <li>Streaming Watermark</li> <li>Streaming Watermark with Aggregation in Append Output Mode</li> <li>StateStoreSaveExec with Complete Output Mode</li> <li>StateStoreSaveExec with Update Output Mode</li> <li>Using StreamingQueryManager for Query Termination Management</li> <li>Using File Streaming Source</li> </ul>","location":"demo/"},{"title":"Demo: StateStoreSaveExec with Complete Output Mode","text":"<p>The following example code shows the behaviour of StateStoreSaveExec.md#doExecute-Complete[StateStoreSaveExec in Complete output mode].</p>","location":"demo/StateStoreSaveExec-Complete/"},{"title":"[source, scala]","text":"<p>// START: Only for easier debugging // The state is then only for one partition // which should make monitoring it easier import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, 1) scala&gt; spark.sessionState.conf.numShufflePartitions res1: Int = 1 // END: Only for easier debugging</p> <p>// Read datasets from a Kafka topic // ./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0-SNAPSHOT // Streaming aggregation using groupBy operator is required to have StateStoreSaveExec operator val valuesPerGroup = spark.   readStream.   format(\"kafka\").   option(\"subscribe\", \"topic1\").   option(\"kafka.bootstrap.servers\", \"localhost:9092\").   load.   withColumn(\"tokens\", split('value, \",\")).   withColumn(\"group\", 'tokens(0)).   withColumn(\"value\", 'tokens(1) cast \"int\").   select(\"group\", \"value\").   groupBy(\"group\").   agg(collect_list(\"value\") as \"values\").   orderBy(\"group\".asc)</p> <p>// valuesPerGroup is a streaming Dataset with just one source // so it knows nothing about output mode or watermark yet // That's why StatefulOperatorStateInfo is generic // and no batch-specific values are printed out // That will be available after the first streaming batch // Use sq.explain to know the runtime-specific values scala&gt; valuesPerGroup.explain == Physical Plan == *Sort [group#25 ASC NULLS FIRST], true, 0 +- Exchange rangepartitioning(group#25 ASC NULLS FIRST, 1)    +- ObjectHashAggregate(keys=[group#25], functions=[collect_list(value#36, 0, 0)])       +- Exchange hashpartitioning(group#25, 1)          +- StateStoreSave [group#25], StatefulOperatorStateInfo(,899f0fd1-b202-45cd-9ebd-09101ca90fa8,0,0), Append, 0             +- ObjectHashAggregate(keys=[group#25], functions=[merge_collect_list(value#36, 0, 0)])                +- Exchange hashpartitioning(group#25, 1)                   +- StateStoreRestore [group#25], StatefulOperatorStateInfo(,899f0fd1-b202-45cd-9ebd-09101ca90fa8,0,0)                      +- ObjectHashAggregate(keys=[group#25], functions=[merge_collect_list(value#36, 0, 0)])                         +- Exchange hashpartitioning(group#25, 1)                            +- ObjectHashAggregate(keys=[group#25], functions=[partial_collect_list(value#36, 0, 0)])                               +- *Project [split(cast(value#1 as string), ,)[0] AS group#25, cast(split(cast(value#1 as string), ,)[1] as int) AS value#36]                                  +- StreamingRelation kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6] <p>// Start the query and hence StateStoreSaveExec // Use Complete output mode import scala.concurrent.duration._ import org.apache.spark.sql.streaming.{OutputMode, Trigger} val sq = valuesPerGroup.   writeStream.   format(\"console\").   option(\"truncate\", false).   trigger(Trigger.ProcessingTime(10.seconds)).   outputMode(OutputMode.Complete).   start</p>","location":"demo/StateStoreSaveExec-Complete/#source-scala"},{"title":"Batch: 0","text":"<p>+-----+------+ |group|values| +-----+------+ +-----+------+</p> <p>// there's only 1 stateful operator and hence 0 for the index in stateOperators scala&gt; println(sq.lastProgress.stateOperators(0).prettyJson) {   \"numRowsTotal\" : 0,   \"numRowsUpdated\" : 0,   \"memoryUsedBytes\" : 60 }</p> <p>// publish 1 new key-value pair in a single streaming batch // 0,1</p>","location":"demo/StateStoreSaveExec-Complete/#batch-0"},{"title":"Batch: 1","text":"<p>+-----+------+ |group|values| +-----+------+ |0    |[1]   | +-----+------+</p> <p>// it's Complete output mode so numRowsTotal is the number of keys in the state store // no keys were available earlier (it's just started!) and so numRowsUpdated is 0 scala&gt; println(sq.lastProgress.stateOperators(0).prettyJson) {   \"numRowsTotal\" : 1,   \"numRowsUpdated\" : 0,   \"memoryUsedBytes\" : 324 }</p> <p>// publish new key and old key in a single streaming batch // new keys // 1,1 // updates to already-stored keys // 0,2</p>","location":"demo/StateStoreSaveExec-Complete/#batch-1"},{"title":"Batch: 2","text":"<p>+-----+------+ |group|values| +-----+------+ |0    |[2, 1]| |1    |[1]   | +-----+------+</p> <p>// it's Complete output mode so numRowsTotal is the number of keys in the state store // no keys were available earlier and so numRowsUpdated is...0?! // Think it's a BUG as it should've been 1 (for the row 0,2) // 8/30 Sent out a question to the Spark user mailing list scala&gt; println(sq.lastProgress.stateOperators(0).prettyJson) {   \"numRowsTotal\" : 2,   \"numRowsUpdated\" : 0,   \"memoryUsedBytes\" : 572 }</p> <p>// In the end... sq.stop</p>","location":"demo/StateStoreSaveExec-Complete/#batch-2"},{"title":"Demo: StateStoreSaveExec with Update Output Mode","text":"<p>CAUTION: FIXME Example of Update with StateStoreSaveExec (and optional watermark)</p>","location":"demo/StateStoreSaveExec-Update/"},{"title":"Demo: Using StreamingQueryManager for Query Termination Management","text":"<p>The demo shows how to use StreamingQueryManager (and specifically awaitAnyTermination and resetTerminated) for query termination management.</p> <pre><code>// Save the code as demo-StreamingQueryManager.scala\n// Start it using spark-shell\n// $ ./bin/spark-shell -i demo-StreamingQueryManager.scala\n\n// Register a StreamingQueryListener to receive notifications about state changes of streaming queries\nimport org.apache.spark.sql.streaming.StreamingQueryListener\nval myQueryListener = new StreamingQueryListener {\n  import org.apache.spark.sql.streaming.StreamingQueryListener._\n  def onQueryTerminated(event: QueryTerminatedEvent): Unit = {\n    println(s\"Query ${event.id} terminated\")\n  }\n\n  def onQueryStarted(event: QueryStartedEvent): Unit = {}\n  def onQueryProgress(event: QueryProgressEvent): Unit = {}\n}\nspark.streams.addListener(myQueryListener)\n\nimport org.apache.spark.sql.streaming._\nimport scala.concurrent.duration._\n\n// Start streaming queries\n\n// Start the first query\nval q4s = spark.readStream.\n  format(\"rate\").\n  load.\n  writeStream.\n  format(\"console\").\n  trigger(Trigger.ProcessingTime(4.seconds)).\n  option(\"truncate\", false).\n  start\n\n// Start another query that is slightly slower\nval q10s = spark.readStream.\n  format(\"rate\").\n  load.\n  writeStream.\n  format(\"console\").\n  trigger(Trigger.ProcessingTime(10.seconds)).\n  option(\"truncate\", false).\n  start\n\n// Both queries run concurrently\n// You should see different outputs in the console\n// q4s prints out 4 rows every batch and twice as often as q10s\n// q10s prints out 10 rows every batch\n\n/*\n-------------------------------------------\nBatch: 7\n-------------------------------------------\n+-----------------------+-----+\n|timestamp              |value|\n+-----------------------+-----+\n|2017-10-27 13:44:07.462|21   |\n|2017-10-27 13:44:08.462|22   |\n|2017-10-27 13:44:09.462|23   |\n|2017-10-27 13:44:10.462|24   |\n+-----------------------+-----+\n\n-------------------------------------------\nBatch: 8\n-------------------------------------------\n+-----------------------+-----+\n|timestamp              |value|\n+-----------------------+-----+\n|2017-10-27 13:44:11.462|25   |\n|2017-10-27 13:44:12.462|26   |\n|2017-10-27 13:44:13.462|27   |\n|2017-10-27 13:44:14.462|28   |\n+-----------------------+-----+\n\n-------------------------------------------\nBatch: 2\n-------------------------------------------\n+-----------------------+-----+\n|timestamp              |value|\n+-----------------------+-----+\n|2017-10-27 13:44:09.847|6    |\n|2017-10-27 13:44:10.847|7    |\n|2017-10-27 13:44:11.847|8    |\n|2017-10-27 13:44:12.847|9    |\n|2017-10-27 13:44:13.847|10   |\n|2017-10-27 13:44:14.847|11   |\n|2017-10-27 13:44:15.847|12   |\n|2017-10-27 13:44:16.847|13   |\n|2017-10-27 13:44:17.847|14   |\n|2017-10-27 13:44:18.847|15   |\n+-----------------------+-----+\n*/\n\n// Stop q4s on a separate thread\n// as we're about to block the current thread awaiting query termination\nimport java.util.concurrent.Executors\nimport java.util.concurrent.TimeUnit.SECONDS\ndef queryTerminator(query: StreamingQuery) = new Runnable {\n  def run = {\n    println(s\"Stopping streaming query: ${query.id}\")\n    query.stop\n  }\n}\nimport java.util.concurrent.TimeUnit.SECONDS\n// Stop the first query after 10 seconds\nExecutors.newSingleThreadScheduledExecutor.\n  scheduleWithFixedDelay(queryTerminator(q4s), 10, 60 * 5, SECONDS)\n// Stop the other query after 20 seconds\nExecutors.newSingleThreadScheduledExecutor.\n  scheduleWithFixedDelay(queryTerminator(q10s), 20, 60 * 5, SECONDS)\n\n// Use StreamingQueryManager to wait for any query termination (either q1 or q2)\n// the current thread will block indefinitely until either streaming query has finished\nspark.streams.awaitAnyTermination\n\n// You are here only after either streaming query has finished\n// Executing spark.streams.awaitAnyTermination again would return immediately\n\n// You should have received the QueryTerminatedEvent for the query termination\n\n// reset the last terminated streaming query\nspark.streams.resetTerminated\n\n// You know at least one query has terminated\n\n// Wait for the other query to terminate\nspark.streams.awaitAnyTermination\n\nassert(spark.streams.active.isEmpty)\n\nprintln(\"The demo went all fine. Exiting...\")\n\n// leave spark-shell\nSystem.exit(0)\n</code></pre>","location":"demo/StreamingQueryManager-awaitAnyTermination-resetTerminated/"},{"title":"Demo: Arbitrary Stateful Streaming Aggregation with KeyValueGroupedDataset.flatMapGroupsWithState Operator","text":"<p>The following demo shows an example of Arbitrary Stateful Streaming Aggregation with KeyValueGroupedDataset.flatMapGroupsWithState operator.</p> <pre><code>import java.sql.Timestamp\ntype DeviceId = Long\ncase class Signal(timestamp: Timestamp, deviceId: DeviceId, value: Long)\n\n// input stream\nimport org.apache.spark.sql.functions._\nval signals = spark\n  .readStream\n  .format(\"rate\")\n  .option(\"rowsPerSecond\", 1)\n  .load\n  .withColumn(\"deviceId\", rint(rand() * 10) cast \"int\") // 10 devices randomly assigned to values\n  .withColumn(\"value\", $\"value\" % 10)  // randomize the values (just for fun)\n  .as[Signal] // convert to our type (from \"unpleasant\" Row)\n\nimport org.apache.spark.sql.streaming.GroupState\ntype Key = Int\ntype Count = Long\ntype State = Map[Key, Count]\ncase class EventsCounted(deviceId: DeviceId, count: Long)\ndef countValuesPerDevice(\n    deviceId: Int,\n    signals: Iterator[Signal],\n    state: GroupState[State]): Iterator[EventsCounted] = {\n  val values = signals.toSeq\n  println(s\"Device: $deviceId\")\n  println(s\"Signals (${values.size}):\")\n  values.zipWithIndex.foreach { case (v, idx) =&gt; println(s\"$idx. $v\") }\n  println(s\"State: $state\")\n\n  // update the state with the count of elements for the key\n  val initialState: State = Map(deviceId -&gt; 0)\n  val oldState = state.getOption.getOrElse(initialState)\n  // the name to highlight that the state is for the key only\n  val newValue = oldState(deviceId) + values.size\n  val newState = Map(deviceId -&gt; newValue)\n  state.update(newState)\n\n  // you must not return as it's already consumed\n  // that leads to a very subtle error where no elements are in an iterator\n  // iterators are one-pass data structures\n  Iterator(EventsCounted(deviceId, newValue))\n}\n\n// stream processing using flatMapGroupsWithState operator\nval deviceId: Signal =&gt; DeviceId = { case Signal(_, deviceId, _) =&gt; deviceId }\nval signalsByDevice = signals.groupByKey(deviceId)\n\nimport org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode}\nval signalCounter = signalsByDevice.flatMapGroupsWithState(\n  outputMode = OutputMode.Append,\n  timeoutConf = GroupStateTimeout.NoTimeout)(countValuesPerDevice)\n\nimport org.apache.spark.sql.streaming.{OutputMode, Trigger}\nimport scala.concurrent.duration._\nval sq = signalCounter.\n  writeStream.\n  format(\"console\").\n  option(\"truncate\", false).\n  trigger(Trigger.ProcessingTime(10.seconds)).\n  outputMode(OutputMode.Append).\n  start\n</code></pre>","location":"demo/arbitrary-stateful-streaming-aggregation-flatMapGroupsWithState/"},{"title":"Demo: current_timestamp Function For Processing Time in Streaming Queries","text":"<p>The demo shows what happens when you use <code>current_timestamp</code> function in your structured queries.</p>","location":"demo/current_timestamp/"},{"title":"[NOTE]","text":"<p>The main motivation was to answer the question https://stackoverflow.com/q/46274593/1305344[How to achieve ingestion time?] in Spark Structured Streaming.</p>","location":"demo/current_timestamp/#note"},{"title":"You're very welcome to upvote the question and answers at your earliest convenience. Thanks!","text":"<p>Quoting the https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html[Apache Flink documentation]:</p>  <p>Event time is the time that each individual event occurred on its producing device. This time is typically embedded within the records before they enter Flink and that event timestamp can be extracted from the record.</p>  <p>That is exactly how event time is considered in withWatermark operator which you use to describe what column to use for event time. The column could be part of the input dataset or...generated.</p> <p>And that is the moment where my confusion starts.</p> <p>In order to generate the event time column for <code>withWatermark</code> operator you could use <code>current_timestamp</code> or <code>current_date</code> standard functions.</p>","location":"demo/current_timestamp/#youre-very-welcome-to-upvote-the-question-and-answers-at-your-earliest-convenience-thanks"},{"title":"[source, scala]","text":"<p>// rate format gives event time // but let's generate a brand new column with ours // for demo purposes val values = spark.   readStream.   format(\"rate\").   load.   withColumn(\"current_timestamp\", current_timestamp) scala&gt; values.printSchema root  |-- timestamp: timestamp (nullable = true)  |-- value: long (nullable = true)  |-- current_timestamp: timestamp (nullable = false)</p>  <p>Both are special for Spark Structured Streaming as <code>StreamExecution</code> MicroBatchExecution.md#runBatch-triggerLogicalPlan[replaces] their underlying Catalyst expressions, <code>CurrentTimestamp</code> and <code>CurrentDate</code> respectively, with <code>CurrentBatchTimestamp</code> expression and the time of the current batch.</p>","location":"demo/current_timestamp/#source-scala"},{"title":"[source, scala]","text":"<p>import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = values.   writeStream.   format(\"console\").   option(\"truncate\", false).   trigger(Trigger.ProcessingTime(10.seconds)).   start</p> <p>// note the value of current_timestamp // that corresponds to the batch time</p>","location":"demo/current_timestamp/#source-scala_1"},{"title":"Batch: 1","text":"<p>+-----------------------+-----+-------------------+ |timestamp              |value|current_timestamp  | +-----------------------+-----+-------------------+ |2017-09-18 10:53:31.523|0    |2017-09-18 10:53:40| |2017-09-18 10:53:32.523|1    |2017-09-18 10:53:40| |2017-09-18 10:53:33.523|2    |2017-09-18 10:53:40| |2017-09-18 10:53:34.523|3    |2017-09-18 10:53:40| |2017-09-18 10:53:35.523|4    |2017-09-18 10:53:40| |2017-09-18 10:53:36.523|5    |2017-09-18 10:53:40| |2017-09-18 10:53:37.523|6    |2017-09-18 10:53:40| |2017-09-18 10:53:38.523|7    |2017-09-18 10:53:40| +-----------------------+-----+-------------------+</p> <p>// Use web UI's SQL tab for the batch (Submitted column) // or sq.recentProgress scala&gt; println(sq.recentProgress(1).timestamp) 2017-09-18T08:53:40.000Z</p> <p>// Note current_batch_timestamp</p> <p>scala&gt; sq.explain(extended = true) == Parsed Logical Plan == 'Project [timestamp#2137, value#2138L, current_batch_timestamp(1505725650005, TimestampType, None) AS current_timestamp#50] +- LogicalRDD [timestamp#2137, value#2138L], true</p> <p>== Analyzed Logical Plan == timestamp: timestamp, value: bigint, current_timestamp: timestamp Project [timestamp#2137, value#2138L, current_batch_timestamp(1505725650005, TimestampType, Some(Europe/Berlin)) AS current_timestamp#50] +- LogicalRDD [timestamp#2137, value#2138L], true</p> <p>== Optimized Logical Plan == Project [timestamp#2137, value#2138L, 1505725650005000 AS current_timestamp#50] +- LogicalRDD [timestamp#2137, value#2138L], true</p> <p>== Physical Plan == *Project [timestamp#2137, value#2138L, 1505725650005000 AS current_timestamp#50] +- Scan ExistingRDD[timestamp#2137,value#2138L]</p>  <p>That seems to be closer to processing time than ingestion time given the definition from the https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html[Apache Flink documentation]:</p>  <p>Processing time refers to the system time of the machine that is executing the respective operation.</p> <p>Ingestion time is the time that events enter Flink.</p>  <p>What do you think?</p>","location":"demo/current_timestamp/#batch-1"},{"title":"Demo: Developing Custom Streaming Sink (and Monitoring SQL Queries in web UI)","text":"<p>The demo shows the steps to develop a custom streaming sink and use it to monitor whether and what SQL queries are executed at runtime (using web UI's SQL tab).</p>","location":"demo/custom-sink-webui/"},{"title":"[NOTE]","text":"<p>The main motivation was to answer the question https://stackoverflow.com/q/46162143/1305344[Why does a single structured query run multiple SQL queries per batch?] that happened to have turned out fairly surprising.</p>","location":"demo/custom-sink-webui/#note"},{"title":"You're very welcome to upvote the question and answers at your earliest convenience. Thanks!","text":"<p>The steps are as follows:</p> <ol> <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt; <li>&lt;&gt;  <p>Findings (aka surprises):</p> <ol> <li>Custom sinks require that you define a checkpoint location using checkpointLocation option (or spark.sql.streaming.checkpointLocation Spark property). Remove the checkpoint directory (or use a different one every start of a streaming query) to have consistent results.</li> </ol> <p>=== [[DemoSink]] Creating Custom Sink -- DemoSink</p>","location":"demo/custom-sink-webui/#youre-very-welcome-to-upvote-the-question-and-answers-at-your-earliest-convenience-thanks"},{"title":"[source, scala]","text":"<p>package pl.japila.spark.sql.streaming</p> <p>case class DemoSink(   sqlContext: SQLContext,   parameters: Map[String, String],   partitionColumns: Seq[String],   outputMode: OutputMode) extends Sink {</p> <p>override def addBatch(batchId: Long, data: DataFrame): Unit = {     println(s\"addBatch($batchId)\")     data.explain()     // Why so many lines just to show the input DataFrame?     data.sparkSession.createDataFrame(       data.sparkSession.sparkContext.parallelize(data.collect()), data.schema)       .show(10)   } }</p>  <p>Save the file under <code>src/main/scala</code> in your project.</p> <p>=== [[DemoSinkProvider]] Creating StreamSinkProvider -- DemoSinkProvider</p>","location":"demo/custom-sink-webui/#source-scala"},{"title":"[source, scala]","text":"<p>package pl.japila.spark.sql.streaming</p> <p>class DemoSinkProvider extends StreamSinkProvider   with DataSourceRegister {</p> <p>override def createSink(     sqlContext: SQLContext,     parameters: Map[String, String],     partitionColumns: Seq[String],     outputMode: OutputMode): Sink = {     DemoSink(sqlContext, parameters, partitionColumns, outputMode)   }</p> <p>override def shortName(): String = \"demo\" }</p>  <p>Save the file under <code>src/main/scala</code> in your project.</p> <p>=== [[registering-sink]] Optional Sink Registration using META-INF/services</p> <p>The step is optional, but greatly improve the experience when using the custom sink so you can use it by its name (rather than a fully-qualified class name or using a special class name for the sink provider).</p> <p>Create <code>org.apache.spark.sql.sources.DataSourceRegister</code> in <code>META-INF/services</code> directory with the following content.</p>","location":"demo/custom-sink-webui/#source-scala_1"},{"title":"[source, scala]","text":"","location":"demo/custom-sink-webui/#source-scala_2"},{"title":"pl.japila.spark.sql.streaming.DemoSinkProvider","text":"<p>Save the file under <code>src/main/resources</code> in your project.</p> <p>=== [[build-sbt]] build.sbt Definition</p> <p>If you use my beloved build tool http://www.scala-sbt.org/[sbt] to manage the project, use the following <code>build.sbt</code>.</p>","location":"demo/custom-sink-webui/#pljapilasparksqlstreamingdemosinkprovider"},{"title":"[source, scala]","text":"<p>organization := \"pl.japila.spark\" name := \"spark-structured-streaming-demo-sink\" version := \"0.1\"</p> <p>scalaVersion := \"2.11.11\"</p>","location":"demo/custom-sink-webui/#source-scala_3"},{"title":"libraryDependencies += \"org.apache.spark\" %% \"spark-sql\" % \"2.2.0\"","text":"<p>=== [[packaging-sink]] Packaging DemoSink</p> <p>The step depends on what build tool you use to manage the project. Use whatever command you use to create a jar file with the above classes compiled and bundled together.</p> <pre><code>$ sbt package\n[info] Loading settings from plugins.sbt ...\n[info] Loading project definition from /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/project\n[info] Loading settings from build.sbt ...\n[info] Set current project to spark-structured-streaming-demo-sink (in build file:/Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/)\n[info] Compiling 1 Scala source to /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/classes ...\n[info] Done compiling.\n[info] Packaging /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/spark-structured-streaming-demo-sink_2.11-0.1.jar ...\n[info] Done packaging.\n[success] Total time: 5 s, completed Sep 12, 2017 9:34:19 AM\n</code></pre> <p>The jar with the sink is <code>/Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/spark-structured-streaming-demo-sink_2.11-0.1.jar</code>.</p> <p>=== [[using-sink]] Using DemoSink in Streaming Query</p> <p>The following code reads data from the <code>rate</code> source and simply outputs the result to our custom <code>DemoSink</code>.</p> <pre><code>// Make sure the DemoSink jar is available\n$ ls /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/spark-structured-streaming-demo-sink_2.11-0.1.jar\n/Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/spark-structured-streaming-demo-sink_2.11-0.1.jar\n\n// \"Install\" the DemoSink using --jars command-line option\n$ ./bin/spark-shell --jars /Users/jacek/dev/sandbox/spark-structured-streaming-custom-sink/target/scala-2.11/spark-structured-streaming-custom-sink_2.11-0.1.jar\n\nscala&gt; spark.version\nres0: String = 2.3.0-SNAPSHOT\n\nimport org.apache.spark.sql.streaming._\nimport scala.concurrent.duration._\nval sq = spark.\n  readStream.\n  format(\"rate\").\n  load.\n  writeStream.\n  format(\"demo\").\n  option(\"checkpointLocation\", \"/tmp/demo-checkpoint\").\n  trigger(Trigger.ProcessingTime(10.seconds)).\n  start\n\n// In the end...\nscala&gt; sq.stop\n17/09/12 09:59:28 INFO StreamExecution: Query [id = 03cd78e3-94e2-439c-9c12-cfed0c996812, runId = 6938af91-9806-4404-965a-5ae7525d5d3f] was stopped\n</code></pre> <p>=== [[webui-sql-queries]] Monitoring SQL Queries using web UI's SQL Tab</p> <p>Open http://localhost:4040/SQL/.</p> <p>You should find that every trigger (aka batch) results in 3 SQL queries. Why?</p> <p>.web UI's SQL Tab and Completed Queries (3 Queries per Batch) image::images/webui-sql-completed-queries-three-per-batch.png[align=\"center\"]</p> <p>The answer lies in what sources and sink a streaming query uses (and differs per streaming query).</p> <p>In our case, &lt;&gt; <code>collects</code> the rows from the input <code>DataFrame</code> and <code>shows</code> it afterwards. That gives 2 SQL queries (as you can see after executing the following batch queries).","location":"demo/custom-sink-webui/#librarydependencies-orgapachespark-spark-sql-220"},{"title":"[source, scala]","text":"<p>// batch non-streaming query val data = (0 to 3).toDF(\"id\")</p> <p>// That gives one SQL query data.collect</p> <p>// That gives one SQL query, too data.show</p>  <p>The remaining query (which is the first among the queries) is executed when you <code>load</code> the data.</p> <p>That can be observed easily when you change &lt;&gt; to not \"touch\" the input <code>data</code> (in <code>addBatch</code>) in any way.","location":"demo/custom-sink-webui/#source-scala_4"},{"title":"[source, scala]","text":"<p>override def addBatch(batchId: Long, data: DataFrame): Unit = {   println(s\"addBatch($batchId)\") }</p>  <p>Re-run the streaming query (using the new <code>DemoSink</code>) and use web UI's SQL tab to see the queries. You should have just one query per batch (and no Spark jobs given nothing is really done in the sink's <code>addBatch</code>).</p> <p>.web UI's SQL Tab and Completed Queries (1 Query per Batch) image::images/webui-sql-completed-queries-one-per-batch.png[align=\"center\"]</p>","location":"demo/custom-sink-webui/#source-scala_5"},{"title":"Demo: Deep Dive into FileStreamSink","text":"<p>This demo shows a streaming query that writes out to FileStreamSink.</p>","location":"demo/deep-dive-into-filestreamsink/"},{"title":"Prerequisites","text":"<p>A sample streaming query reads data using <code>socket</code> data source. Start <code>nc</code>.</p> <pre><code>nc -lk 9999\n</code></pre>","location":"demo/deep-dive-into-filestreamsink/#prerequisites"},{"title":"Configure Logging","text":"<p>Enable logging for <code>FileStreamSink</code>.</p>","location":"demo/deep-dive-into-filestreamsink/#configure-logging"},{"title":"Start Streaming Query","text":"<p>Use <code>spark-shell</code> for fast interactive prototyping.</p> <p>Describe the source.</p> <pre><code>val lines = spark\n  .readStream\n  .format(\"socket\")\n  .option(\"host\", \"localhost\")\n  .option(\"port\", \"9999\")\n  .load\n</code></pre> <p>Describe the sink and start the streaming query.</p> <pre><code>import org.apache.spark.sql.streaming.{OutputMode, Trigger}\nimport concurrent.duration._\nval interval = 15.seconds\nval trigger = Trigger.ProcessingTime(interval)\nval queryName = s\"micro-batch every $interval\"\nval sq = lines\n  .writeStream\n  .format(\"text\")\n  .option(\"checkpointLocation\", \"/tmp/checkpointLocation\")\n  .trigger(trigger)\n  .outputMode(OutputMode.Append) // only Append supported\n  .queryName(queryName)\n  .start(path = \"/tmp/socket-file\")\n</code></pre> <p>Use web UI to monitor the query (http://localhost:4040).</p>","location":"demo/deep-dive-into-filestreamsink/#start-streaming-query"},{"title":"Deep Dive into Internals","text":"<pre><code>import org.apache.spark.sql.streaming.StreamingQuery\nassert(sq.isInstanceOf[StreamingQuery])\n</code></pre> <pre><code>scala&gt; sq.explain(extended = true)\n== Parsed Logical Plan ==\nStreamingDataSourceV2Relation [value#0], org.apache.spark.sql.execution.streaming.sources.TextSocketTable$$anon$1@aa58ed0, TextSocketV2[host: localhost, port: 9999], -1, 0\n\n== Analyzed Logical Plan ==\nvalue: string\nStreamingDataSourceV2Relation [value#0], org.apache.spark.sql.execution.streaming.sources.TextSocketTable$$anon$1@aa58ed0, TextSocketV2[host: localhost, port: 9999], -1, 0\n\n== Optimized Logical Plan ==\nStreamingDataSourceV2Relation [value#0], org.apache.spark.sql.execution.streaming.sources.TextSocketTable$$anon$1@aa58ed0, TextSocketV2[host: localhost, port: 9999], -1, 0\n\n== Physical Plan ==\n*(1) Project [value#0]\n+- MicroBatchScan[value#0] class org.apache.spark.sql.execution.streaming.sources.TextSocketTable$$anon$1\n</code></pre> <pre><code>scala&gt; println(sq.lastProgress)\n{\n  \"id\" : \"f4dc1b6c-6bc7-423a-9bfe-49db2a440bda\",\n  \"runId\" : \"1a05533a-4db0-486d-8c44-7d4a8e49a7bc\",\n  \"name\" : \"micro-batch every 15 seconds\",\n  \"timestamp\" : \"2020-10-17T09:47:30.003Z\",\n  \"batchId\" : 2,\n  \"numInputRows\" : 0,\n  \"inputRowsPerSecond\" : 0.0,\n  \"processedRowsPerSecond\" : 0.0,\n  \"durationMs\" : {\n    \"latestOffset\" : 0,\n    \"triggerExecution\" : 0\n  },\n  \"stateOperators\" : [ ],\n  \"sources\" : [ {\n    \"description\" : \"TextSocketV2[host: localhost, port: 9999]\",\n    \"startOffset\" : 0,\n    \"endOffset\" : 0,\n    \"numInputRows\" : 0,\n    \"inputRowsPerSecond\" : 0.0,\n    \"processedRowsPerSecond\" : 0.0\n  } ],\n  \"sink\" : {\n    \"description\" : \"FileSink[/tmp/socket-file]\",\n    \"numOutputRows\" : -1\n  }\n}\n</code></pre> <pre><code>import org.apache.spark.sql.execution.debug._\nscala&gt; sq.debugCodegen()\nFound 1 WholeStageCodegen subtrees.\n== Subtree 1 / 1 (maxMethodCodeSize:120; maxConstantPoolSize:100(0.15% used); numInnerClasses:0) ==\n*(1) Project [value#0]\n+- MicroBatchScan[value#0] class org.apache.spark.sql.execution.streaming.sources.TextSocketTable$$anon$1\n\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ // codegenStageId=1\n/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n...\n</code></pre>","location":"demo/deep-dive-into-filestreamsink/#deep-dive-into-internals"},{"title":"MicroBatchExecution","text":"<p>Access MicroBatchExecution.</p> <pre><code>import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper\nval streamEngine = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery\n\nimport org.apache.spark.sql.execution.streaming.StreamExecution\nassert(streamEngine.isInstanceOf[StreamExecution])\n\nimport org.apache.spark.sql.execution.streaming.MicroBatchExecution\nval microBatchEngine = streamEngine.asInstanceOf[MicroBatchExecution]\n</code></pre>","location":"demo/deep-dive-into-filestreamsink/#microbatchexecution"},{"title":"IncrementalExecution","text":"<p>Access IncrementalExecution.</p> <pre><code>val qe = microBatchEngine.lastExecution\nimport org.apache.spark.sql.execution.streaming.IncrementalExecution\nassert(qe.isInstanceOf[IncrementalExecution])\nassert(qe != null, \"No physical plan. Waiting for data.\")\n</code></pre>","location":"demo/deep-dive-into-filestreamsink/#incrementalexecution"},{"title":"FileStreamSink","text":"<p>A streaming query (as a StreamExecution) is associated with one sink. That's the FileStreamSink in this demo.</p> <pre><code>import org.apache.spark.sql.execution.streaming.FileStreamSink\nval sink = microBatchEngine.sink.asInstanceOf[FileStreamSink]\nassert(sink.isInstanceOf[FileStreamSink])\n</code></pre> <pre><code>scala&gt; println(sink)\nFileSink[/tmp/socket-file]\n</code></pre> <p>Since <code>FileStreamSink</code> has already been requested to add at least one batch, requesting it to add <code>0</code>th batch again should be skipped.</p> <pre><code>scala&gt; sink.addBatch(batchId = 0, data = spark.range(5).toDF)\nFileStreamSink: Skipping already committed batch 0\n</code></pre>","location":"demo/deep-dive-into-filestreamsink/#filestreamsink"},{"title":"Stop Query","text":"<pre><code>spark.streams.active.foreach(_.stop)\n</code></pre>","location":"demo/deep-dive-into-filestreamsink/#stop-query"},{"title":"Demo: Exploring Checkpointed State","text":"<p>The following demo shows the internals of the checkpointed state of a stateful streaming query.</p> <p>The demo uses the state checkpoint directory that was used in Demo: Streaming Watermark with Aggregation in Append Output Mode.</p> <pre><code>// Change the path to match your configuration\nval checkpointRootLocation = \"/tmp/checkpoint-watermark_demo/state\"\nval version = 1L\n\nimport org.apache.spark.sql.execution.streaming.state.StateStoreId\nval storeId = StateStoreId(\n  checkpointRootLocation,\n  operatorId = 0,\n  partitionId = 0)\n\n// The key and value schemas should match the watermark demo\n// .groupBy(window($\"time\", windowDuration.toString) as \"sliding_window\")\nimport org.apache.spark.sql.types.{TimestampType, StructField, StructType}\nval keySchema = StructType(\n  StructField(\"sliding_window\",\n    StructType(\n      StructField(\"start\", TimestampType, nullable = true) ::\n      StructField(\"end\", TimestampType, nullable = true) :: Nil),\n    nullable = false) :: Nil)\nscala&gt; keySchema.printTreeString\nroot\n |-- sliding_window: struct (nullable = false)\n |    |-- start: timestamp (nullable = true)\n |    |-- end: timestamp (nullable = true)\n\n// .agg(collect_list(\"batch\") as \"batches\", collect_list(\"value\") as \"values\")\nimport org.apache.spark.sql.types.{ArrayType, LongType}\nval valueSchema = StructType(\n  StructField(\"batches\", ArrayType(LongType, true), true) ::\n  StructField(\"values\", ArrayType(LongType, true), true) :: Nil)\nscala&gt; valueSchema.printTreeString\nroot\n |-- batches: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- values: array (nullable = true)\n |    |-- element: long (containsNull = true)\n\nval indexOrdinal = None\nimport org.apache.spark.sql.execution.streaming.state.StateStoreConf\nval storeConf = StateStoreConf(spark.sessionState.conf)\nval hadoopConf = spark.sessionState.newHadoopConf()\nimport org.apache.spark.sql.execution.streaming.state.StateStoreProvider\nval provider = StateStoreProvider.createAndInit(\n  storeId, keySchema, valueSchema, indexOrdinal, storeConf, hadoopConf)\n\n// You may want to use the following higher-level code instead\nimport java.util.UUID\nval queryRunId = UUID.randomUUID\nimport org.apache.spark.sql.execution.streaming.state.StateStoreProviderId\nval storeProviderId = StateStoreProviderId(storeId, queryRunId)\nimport org.apache.spark.sql.execution.streaming.state.StateStore\nval store = StateStore.get(\n  storeProviderId,\n  keySchema,\n  valueSchema,\n  indexOrdinal,\n  version,\n  storeConf,\n  hadoopConf)\n\nimport org.apache.spark.sql.execution.streaming.state.UnsafeRowPair\ndef formatRowPair(rowPair: UnsafeRowPair) = {\n  s\"(${rowPair.key.getLong(0)}, ${rowPair.value.getLong(0)})\"\n}\nstore.iterator.map(formatRowPair).foreach(println)\n\n// WIP: Missing value (per window)\ndef formatRowPair(rowPair: UnsafeRowPair) = {\n  val window = rowPair.key.getStruct(0, 2)\n  import scala.concurrent.duration._\n  val begin = window.getLong(0).millis.toSeconds\n  val end = window.getLong(1).millis.toSeconds\n\n  val value = rowPair.value.getStruct(0, 4)\n  // input is (time, value, batch) all longs\n  val t = value.getLong(1).millis.toSeconds\n  val v = value.getLong(2)\n  val b = value.getLong(3)\n  s\"(key: [$begin, $end], ($t, $v, $b))\"\n}\nstore.iterator.map(formatRowPair).foreach(println)\n</code></pre>","location":"demo/exploring-checkpointed-state/"},{"title":"Demo: Streaming Query for Running Counts (Socket Source and Complete Output Mode)","text":"<p>The following code shows a streaming aggregation (with Dataset.groupBy operator) in complete output mode that reads text lines from a socket (using socket data source) and outputs running counts of the words.</p> <p>NOTE: The example is \"borrowed\" from http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html[the official documentation of Spark]. Changes and errors are only mine.</p> <p>IMPORTANT: Run <code>nc -lk 9999</code> first before running the demo.</p> <pre><code>// START: Only for easier debugging\n// Reduce the number of partitions\n// The state is then only for one partition\n// which should make monitoring easier\nval numShufflePartitions = 1\nimport org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS\nspark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, numShufflePartitions)\n\nassert(spark.sessionState.conf.numShufflePartitions == numShufflePartitions)\n// END: Only for easier debugging\n\nval lines = spark\n  .readStream\n  .format(\"socket\")\n  .option(\"host\", \"localhost\")\n  .option(\"port\", 9999)\n  .load\n\nscala&gt; lines.printSchema\nroot\n |-- value: string (nullable = true)\n\nimport org.apache.spark.sql.functions.explode\nval words = lines\n  .select(explode(split($\"value\", \"\"\"\\W+\"\"\")) as \"word\")\n\nval counts = words.groupBy(\"word\").count\n\nscala&gt; counts.printSchema\nroot\n |-- word: string (nullable = true)\n |-- count: long (nullable = false)\n\n// nc -lk 9999 is supposed to be up at this point\n\nval queryName = \"running_counts\"\nval checkpointLocation = s\"/tmp/checkpoint-$queryName\"\n\n// Delete the checkpoint location from previous executions\nimport java.nio.file.{Files, FileSystems}\nimport java.util.Comparator\nimport scala.collection.JavaConverters._\nval path = FileSystems.getDefault.getPath(checkpointLocation)\nif (Files.exists(path)) {\n  Files.walk(path)\n    .sorted(Comparator.reverseOrder())\n    .iterator\n    .asScala\n    .foreach(p =&gt; p.toFile.delete)\n}\n\nimport org.apache.spark.sql.streaming.OutputMode.Complete\nval runningCounts = counts\n  .writeStream\n  .format(\"console\")\n  .option(\"checkpointLocation\", checkpointLocation)\n  .outputMode(Complete)\n  .start\n\nscala&gt; runningCounts.explain\n== Physical Plan ==\nWriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@42363db7\n+- *(5) HashAggregate(keys=[word#72], functions=[count(1)])\n   +- StateStoreSave [word#72], state info [ checkpoint = file:/tmp/checkpoint-running_counts/state, runId = f3b2e642-1790-4a17-ab61-3d894110b063, opId = 0, ver = 0, numPartitions = 1], Complete, 0, 2\n      +- *(4) HashAggregate(keys=[word#72], functions=[merge_count(1)])\n         +- StateStoreRestore [word#72], state info [ checkpoint = file:/tmp/checkpoint-running_counts/state, runId = f3b2e642-1790-4a17-ab61-3d894110b063, opId = 0, ver = 0, numPartitions = 1], 2\n            +- *(3) HashAggregate(keys=[word#72], functions=[merge_count(1)])\n               +- Exchange hashpartitioning(word#72, 1)\n                  +- *(2) HashAggregate(keys=[word#72], functions=[partial_count(1)])\n                     +- Generate explode(split(value#83, \\W+)), false, [word#72]\n                        +- *(1) Project [value#83]\n                           +- *(1) ScanV2 socket[value#83] (Options: [host=localhost,port=9999])\n\n// Type lines (words) in the terminal with nc\n// Observe the counts in spark-shell\n\n// Use web UI to monitor the state of state (no pun intended)\n// StateStoreSave and StateStoreRestore operators all have state metrics\n// Go to http://localhost:4040/SQL/ and click one of the Completed Queries with Job IDs\n\n// You may also want to check out checkpointed state\n// in /tmp/checkpoint-running_counts/state/0/0\n\n// Eventually...\nrunningCounts.stop()\n</code></pre>","location":"demo/groupBy-running-count-complete/"},{"title":"Demo: groupByKey Streaming Aggregation in Update Mode","text":"<p>The example shows Dataset.groupByKey streaming operator to count rows in Update output mode.</p> <p>In other words, it is an example of using <code>Dataset.groupByKey</code> with <code>count</code> aggregation function to count customer orders (<code>T</code>) per zip code (<code>K</code>).</p> <pre><code>package pl.japila.spark.examples\n\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.streaming.{OutputMode, Trigger}\n\nobject GroupByKeyStreamingApp extends App {\n\n  val inputTopic = \"GroupByKeyApp-input\"\n  val appName = this.getClass.getSimpleName.replace(\"$\", \"\")\n\n  val spark = SparkSession.builder\n    .master(\"local[*]\")\n    .appName(appName)\n    .getOrCreate\n  import spark.implicits._\n\n  case class Order(id: Long, zipCode: String)\n\n  // Input (source node)\n  val orders = spark\n    .readStream\n    .format(\"kafka\")\n    .option(\"startingOffsets\", \"latest\")\n    .option(\"subscribe\", inputTopic)\n    .option(\"kafka.bootstrap.servers\", \":9092\")\n    .load\n    .select($\"offset\" as \"id\", $\"value\" as \"zipCode\") // FIXME Use csv, json, avro\n    .as[Order]\n\n  // Processing logic\n  // groupByKey + count\n  val byZipCode = (o: Order) =&gt; o.zipCode\n  val ordersByZipCode = orders.groupByKey(byZipCode)\n\n  import org.apache.spark.sql.functions.count\n  val typedCountCol = (count(\"zipCode\") as \"count\").as[String]\n  val counts = ordersByZipCode\n    .agg(typedCountCol)\n    .select($\"value\" as \"zip_code\", $\"count\")\n\n  // Output (sink node)\n  import scala.concurrent.duration._\n  counts\n    .writeStream\n    .format(\"console\")\n    .outputMode(OutputMode.Update)  // FIXME Use Complete\n    .queryName(appName)\n    .trigger(Trigger.ProcessingTime(5.seconds))\n    .start\n    .awaitTermination()\n}\n</code></pre>","location":"demo/groupByKey-count-Update/"},{"title":"Credits","text":"<ul> <li>The example with customer orders and postal codes is borrowed from Apache Beam's Using GroupByKey Programming Guide.</li> </ul>","location":"demo/groupByKey-count-Update/#credits"},{"title":"Demo: Kafka Data Source","text":"<p>This demo shows how to use Kafka Data Source in a streaming query.</p>","location":"demo/kafka-data-source/"},{"title":"Start Kafka Cluster","text":"<pre><code>./start-confluent.sh\n</code></pre> <pre><code>$ docker compose ps\nNAME                COMMAND                  SERVICE             STATUS              PORTS\nbroker              \"/etc/confluent/dock\u2026\"   broker              running             0.0.0.0:9092-&gt;9092/tcp, 0.0.0.0:9101-&gt;9101/tcp\nconnect             \"/etc/confluent/dock\u2026\"   connect             running             0.0.0.0:8083-&gt;8083/tcp, 9092/tcp\ncontrol-center      \"/etc/confluent/dock\u2026\"   control-center      running             0.0.0.0:9021-&gt;9021/tcp\nrest-proxy          \"/etc/confluent/dock\u2026\"   rest-proxy          running             0.0.0.0:8082-&gt;8082/tcp\nschema-registry     \"/etc/confluent/dock\u2026\"   schema-registry     running             0.0.0.0:8081-&gt;8081/tcp\nzookeeper           \"/etc/confluent/dock\u2026\"   zookeeper           running             2888/tcp, 0.0.0.0:2181-&gt;2181/tcp, 3888/tcp\n</code></pre>","location":"demo/kafka-data-source/#start-kafka-cluster"},{"title":"Start Spark Shell","text":"<p>Run <code>spark-shell</code> with spark-sql-kafka-0-10 external module.</p> <pre><code>./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\n</code></pre>","location":"demo/kafka-data-source/#start-spark-shell"},{"title":"Define Source","text":"<pre><code>val events = spark\n  .readStream\n  .format(\"kafka\")\n  .option(\"subscribe\", \"demo.kafka-data-source\")\n  .option(\"kafka.bootstrap.servers\", \":9092\")\n  .load\n</code></pre> <pre><code>scala&gt; events.printSchema\nroot\n |-- key: binary (nullable = true)\n |-- value: binary (nullable = true)\n |-- topic: string (nullable = true)\n |-- partition: integer (nullable = true)\n |-- offset: long (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n |-- timestampType: integer (nullable = true)\n</code></pre> <pre><code>val kvs = events.\n  select(\n    $\"key\" cast \"string\",\n    $\"value\" cast \"string\")\n</code></pre> <pre><code>scala&gt; kvs.printSchema\nroot\n |-- key: string (nullable = true)\n |-- value: string (nullable = true)\n</code></pre> <pre><code>scala&gt; kvs.explain\n== Physical Plan ==\n*(1) Project [cast(key#277 as string) AS key#295, cast(value#278 as string) AS value#296]\n+- StreamingRelation kafka, [key#277, value#278, topic#279, partition#280, offset#281L, timestamp#282, timestampType#283]\n</code></pre>","location":"demo/kafka-data-source/#define-source"},{"title":"Start Streaming Query","text":"<pre><code>import java.time.Clock\nval timeOffset = Clock.systemUTC.instant.getEpochSecond\nval queryName = s\"Demo: Kafka Data Source ($timeOffset)\"\nval checkpointLocation = s\"/tmp/demo-checkpoint-$timeOffset\"\n</code></pre> <pre><code>import scala.concurrent.duration._\nimport org.apache.spark.sql.streaming.Trigger\nval sq = kvs\n  .writeStream\n  .format(\"console\")\n  .option(\"checkpointLocation\", checkpointLocation)\n  .option(\"truncate\", false)\n  .queryName(queryName)\n  .trigger(Trigger.ProcessingTime(1.seconds))\n  .start\n</code></pre> <p>The batch 0 is immediately printed out to the console.</p> <pre><code>-------------------------------------------\nBatch: 0\n-------------------------------------------\n+---+-----+\n|key|value|\n+---+-----+\n+---+-----+\n</code></pre>","location":"demo/kafka-data-source/#start-streaming-query"},{"title":"Send Event","text":"<pre><code>echo \"k1:v1\" | kcat -P -b :9092 -K : -t demo.kafka-data-source\n</code></pre> <p><code>spark-shell</code> should print out the event as Batch 1.</p> <pre><code>-------------------------------------------\nBatch: 1\n-------------------------------------------\n+---+-----+\n|key|value|\n+---+-----+\n|k1 |v1   |\n+---+-----+\n</code></pre>","location":"demo/kafka-data-source/#send-event"},{"title":"Cleanup","text":"<pre><code>sq.stop()\n</code></pre> <p>Exit <code>spark-shell</code> and stop the Kafka cluster (Confluent Platform).</p>","location":"demo/kafka-data-source/#cleanup"},{"title":"Demo: Internals of FlatMapGroupsWithStateExec Physical Operator","text":"<p>The following demo shows the internals of FlatMapGroupsWithStateExec physical operator in a Arbitrary Stateful Streaming Aggregation.</p> <pre><code>// Reduce the number of partitions and hence the state stores\n// That is supposed to make debugging state checkpointing easier\nval numShufflePartitions = 1\nimport org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS\nspark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, numShufflePartitions)\nassert(spark.sessionState.conf.numShufflePartitions == numShufflePartitions)\n\n// Define event \"format\"\n// Use :paste mode in spark-shell\nimport java.sql.Timestamp\ncase class Event(time: Timestamp, value: Long)\nimport scala.concurrent.duration._\nobject Event {\n  def apply(secs: Long, value: Long): Event = {\n    Event(new Timestamp(secs.seconds.toMillis), value)\n  }\n}\n\n// Using memory data source for full control of the input\nimport org.apache.spark.sql.execution.streaming.MemoryStream\nimplicit val sqlCtx = spark.sqlContext\nval events = MemoryStream[Event]\nval values = events.toDS\nassert(values.isStreaming, \"values must be a streaming Dataset\")\n\nvalues.printSchema\n/**\nroot\n |-- time: timestamp (nullable = true)\n |-- value: long (nullable = false)\n*/\n\nimport scala.concurrent.duration._\nval delayThreshold = 10.seconds\nval valuesWatermarked = values\n  .withWatermark(eventTime = \"time\", delayThreshold.toString) // required for EventTimeTimeout\n\n// Could use Long directly, but...\n// Let's use case class to make the demo a bit more advanced\ncase class Count(value: Long)\n\nimport java.sql.Timestamp\nimport org.apache.spark.sql.streaming.GroupState\nval keyCounts = (key: Long, values: Iterator[(Timestamp, Long)], state: GroupState[Count]) =&gt; {\n  println(s\"\"\"&gt;&gt;&gt; keyCounts(key = $key, state = ${state.getOption.getOrElse(\"&lt;empty&gt;\")})\"\"\")\n  println(s\"&gt;&gt;&gt; &gt;&gt;&gt; currentProcessingTimeMs: ${state.getCurrentProcessingTimeMs}\")\n  println(s\"&gt;&gt;&gt; &gt;&gt;&gt; currentWatermarkMs: ${state.getCurrentWatermarkMs}\")\n  println(s\"&gt;&gt;&gt; &gt;&gt;&gt; hasTimedOut: ${state.hasTimedOut}\")\n  val count = Count(values.length)\n  Iterator((key, count))\n}\n\nimport org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode}\nval valuesCounted = valuesWatermarked\n  .as[(Timestamp, Long)] // convert DataFrame to Dataset to make groupByKey easier to write\n  .groupByKey { case (time, value) =&gt; value }\n  .flatMapGroupsWithState(\n    OutputMode.Update,\n    timeoutConf = GroupStateTimeout.EventTimeTimeout)(func = keyCounts)\n  .toDF(\"value\", \"count\")\n\nvaluesCounted.explain\n/**\n== Physical Plan ==\n*(2) Project [_1#928L AS value#931L, _2#929 AS count#932]\n+- *(2) SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#928L, if (isnull(assertnotnull(input[0, scala.Tuple2, true])._2)) null else named_struct(value, assertnotnull(assertnotnull(input[0, scala.Tuple2, true])._2).value) AS _2#929]\n   +- FlatMapGroupsWithState $line140.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$4117/181063008@d2cdc82, value#923: bigint, newInstance(class scala.Tuple2), [value#923L], [time#915-T10000ms, value#916L], obj#927: scala.Tuple2, state info [ checkpoint = &lt;unknown&gt;, runId = 9af3d00c-fe1f-46a0-8630-4e0d0af88042, opId = 0, ver = 0, numPartitions = 1], class[value[0]: bigint], 2, Update, EventTimeTimeout, 0, 0\n      +- *(1) Sort [value#923L ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(value#923L, 1)\n            +- AppendColumns $line140.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$4118/2131767153@3e606b4c, newInstance(class scala.Tuple2), [input[0, bigint, false] AS value#923L]\n               +- EventTimeWatermark time#915: timestamp, interval 10 seconds\n                  +- StreamingRelation MemoryStream[time#915,value#916L], [time#915, value#916L]\n*/\n\nval queryName = \"FlatMapGroupsWithStateExec_demo\"\nval checkpointLocation = s\"/tmp/checkpoint-$queryName\"\n\n// Delete the checkpoint location from previous executions\nimport java.nio.file.{Files, FileSystems}\nimport java.util.Comparator\nimport scala.collection.JavaConverters._\nval path = FileSystems.getDefault.getPath(checkpointLocation)\nif (Files.exists(path)) {\n  Files.walk(path)\n    .sorted(Comparator.reverseOrder())\n    .iterator\n    .asScala\n    .foreach(p =&gt; p.toFile.delete)\n}\n\nimport org.apache.spark.sql.streaming.OutputMode.Update\nval streamingQuery = valuesCounted\n  .writeStream\n  .format(\"memory\")\n  .queryName(queryName)\n  .option(\"checkpointLocation\", checkpointLocation)\n  .outputMode(Update)\n  .start\n\nassert(streamingQuery.status.message == \"Waiting for data to arrive\")\n\n// Use web UI to monitor the metrics of the streaming query\n// Go to http://localhost:4040/SQL/ and click one of the Completed Queries with Job IDs\n\n// You may also want to check out checkpointed state\n// in /tmp/checkpoint-FlatMapGroupsWithStateExec_demo/state/0/0\n\nval batch = Seq(\n  Event(secs = 1,  value = 1),\n  Event(secs = 15, value = 2))\nevents.addData(batch)\nstreamingQuery.processAllAvailable()\n\n/**\n&gt;&gt;&gt; keyCounts(key = 1, state = &lt;empty&gt;)\n&gt;&gt;&gt; &gt;&gt;&gt; currentProcessingTimeMs: 1561881557237\n&gt;&gt;&gt; &gt;&gt;&gt; currentWatermarkMs: 0\n&gt;&gt;&gt; &gt;&gt;&gt; hasTimedOut: false\n&gt;&gt;&gt; keyCounts(key = 2, state = &lt;empty&gt;)\n&gt;&gt;&gt; &gt;&gt;&gt; currentProcessingTimeMs: 1561881557237\n&gt;&gt;&gt; &gt;&gt;&gt; currentWatermarkMs: 0\n&gt;&gt;&gt; &gt;&gt;&gt; hasTimedOut: false\n*/\n\nspark.table(queryName).show(truncate = false)\n/**\n+-----+-----+\n|value|count|\n+-----+-----+\n|1    |[1]  |\n|2    |[1]  |\n+-----+-----+\n*/\n\n// With at least one execution we can review the execution plan\nstreamingQuery.explain\n/**\n== Physical Plan ==\n*(2) Project [_1#928L AS value#931L, _2#929 AS count#932]\n+- *(2) SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#928L, if (isnull(assertnotnull(input[0, scala.Tuple2, true])._2)) null else named_struct(value, assertnotnull(assertnotnull(input[0, scala.Tuple2, true])._2).value) AS _2#929]\n   +- FlatMapGroupsWithState $line140.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$4117/181063008@d2cdc82, value#923: bigint, newInstance(class scala.Tuple2), [value#923L], [time#915-T10000ms, value#916L], obj#927: scala.Tuple2, state info [ checkpoint = file:/tmp/checkpoint-FlatMapGroupsWithStateExec_demo/state, runId = 95c3917c-2fd7-45b2-86f6-6c01f0115e1d, opId = 0, ver = 1, numPartitions = 1], class[value[0]: bigint], 2, Update, EventTimeTimeout, 1561881557499, 5000\n      +- *(1) Sort [value#923L ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(value#923L, 1)\n            +- AppendColumns $line140.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$4118/2131767153@3e606b4c, newInstance(class scala.Tuple2), [input[0, bigint, false] AS value#923L]\n               +- EventTimeWatermark time#915: timestamp, interval 10 seconds\n                  +- LocalTableScan &lt;empty&gt;, [time#915, value#916L]\n*/\n\ntype Millis = Long\ndef toMillis(datetime: String): Millis = {\n  import java.time.format.DateTimeFormatter\n  import java.time.LocalDateTime\n  import java.time.ZoneOffset\n  LocalDateTime\n    .parse(datetime, DateTimeFormatter.ISO_DATE_TIME)\n    .toInstant(ZoneOffset.UTC)\n    .toEpochMilli\n}\n\nval currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\")\nval currentWatermarkSecs = toMillis(currentWatermark).millis.toSeconds.seconds\n\nval expectedWatermarkSecs = 5.seconds\nassert(currentWatermarkSecs == expectedWatermarkSecs, s\"Current event-time watermark is $currentWatermarkSecs, but should be $expectedWatermarkSecs (maximum event time - delayThreshold ${delayThreshold.toMillis})\")\n\n// Let's access the FlatMapGroupsWithStateExec physical operator\nimport org.apache.spark.sql.execution.streaming.StreamingQueryWrapper\nimport org.apache.spark.sql.execution.streaming.StreamExecution\nval engine: StreamExecution = streamingQuery\n  .asInstanceOf[StreamingQueryWrapper]\n  .streamingQuery\n\nimport org.apache.spark.sql.execution.streaming.IncrementalExecution\nval lastMicroBatch: IncrementalExecution = engine.lastExecution\n\n// Access executedPlan that is the optimized physical query plan ready for execution\n// All streaming optimizations have been applied at this point\nval plan = lastMicroBatch.executedPlan\n\n// Find the FlatMapGroupsWithStateExec physical operator\nimport org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec\nval flatMapOp = plan.collect { case op: FlatMapGroupsWithStateExec =&gt; op }.head\n\n// Display metrics\nimport org.apache.spark.sql.execution.metric.SQLMetric\ndef formatMetrics(name: String, metric: SQLMetric) = {\n  val desc = metric.name.getOrElse(\"\")\n  val value = metric.value\n  f\"| $name%-30s | $desc%-69s | $value%-10s\"\n}\nflatMapOp.metrics.map { case (name, metric) =&gt; formatMetrics(name, metric) }.foreach(println)\n/**\n| numTotalStateRows              | number of total state rows                                            | 0\n| stateMemory                    | memory used by state total (min, med, max)                            | 390\n| loadedMapCacheHitCount         | count of cache hit on states cache in provider                        | 1\n| numOutputRows                  | number of output rows                                                 | 0\n| stateOnCurrentVersionSizeBytes | estimated size of state only on current version total (min, med, max) | 102\n| loadedMapCacheMissCount        | count of cache miss on states cache in provider                       | 0\n| commitTimeMs                   | time to commit changes total (min, med, max)                          | -2\n| allRemovalsTimeMs              | total time to remove rows total (min, med, max)                       | -2\n| numUpdatedStateRows            | number of updated state rows                                          | 0\n| allUpdatesTimeMs               | total time to update rows total (min, med, max)                       | -2\n*/\n\nval batch = Seq(\n  Event(secs = 1,  value = 1),  // under the watermark (5000 ms) so it's disregarded\n  Event(secs = 6,  value = 3))  // above the watermark so it should be counted\nevents.addData(batch)\nstreamingQuery.processAllAvailable()\n\n/**\n&gt;&gt;&gt; keyCounts(key = 3, state = &lt;empty&gt;)\n&gt;&gt;&gt; &gt;&gt;&gt; currentProcessingTimeMs: 1561881643568\n&gt;&gt;&gt; &gt;&gt;&gt; currentWatermarkMs: 5000\n&gt;&gt;&gt; &gt;&gt;&gt; hasTimedOut: false\n*/\n\nspark.table(queryName).show(truncate = false)\n/**\n+-----+-----+\n|value|count|\n+-----+-----+\n|1    |[1]  |\n|2    |[1]  |\n|3    |[1]  |\n+-----+-----+\n*/\n\nval batch = Seq(\n  Event(secs = 17,  value = 3))  // advances the watermark\nevents.addData(batch)\nstreamingQuery.processAllAvailable()\n\n/**\n&gt;&gt;&gt; keyCounts(key = 3, state = &lt;empty&gt;)\n&gt;&gt;&gt; &gt;&gt;&gt; currentProcessingTimeMs: 1561881672887\n&gt;&gt;&gt; &gt;&gt;&gt; currentWatermarkMs: 5000\n&gt;&gt;&gt; &gt;&gt;&gt; hasTimedOut: false\n*/\n\nval currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\")\nval currentWatermarkSecs = toMillis(currentWatermark).millis.toSeconds.seconds\n\nval expectedWatermarkSecs = 7.seconds\nassert(currentWatermarkSecs == expectedWatermarkSecs, s\"Current event-time watermark is $currentWatermarkSecs, but should be $expectedWatermarkSecs (maximum event time - delayThreshold ${delayThreshold.toMillis})\")\n\nspark.table(queryName).show(truncate = false)\n/**\n+-----+-----+\n|value|count|\n+-----+-----+\n|1    |[1]  |\n|2    |[1]  |\n|3    |[1]  |\n|3    |[1]  |\n+-----+-----+\n*/\n\nval batch = Seq(\n  Event(secs = 18,  value = 3))  // advances the watermark\nevents.addData(batch)\nstreamingQuery.processAllAvailable()\n\n/**\n&gt;&gt;&gt; keyCounts(key = 3, state = &lt;empty&gt;)\n&gt;&gt;&gt; &gt;&gt;&gt; currentProcessingTimeMs: 1561881778165\n&gt;&gt;&gt; &gt;&gt;&gt; currentWatermarkMs: 7000\n&gt;&gt;&gt; &gt;&gt;&gt; hasTimedOut: false\n*/\n\n// Eventually...\nstreamingQuery.stop()\n</code></pre>","location":"demo/spark-sql-streaming-demo-FlatMapGroupsWithStateExec/"},{"title":"Demo: Streaming Aggregation","text":"<p>This demo shows a streaming query with a streaming aggregation (with Dataset.groupBy operator) that processes data from Kafka (using Kafka Data Source).</p>  <p>Note</p> <p>Please start a Kafka cluster and <code>spark-shell</code> as described in Demo: Kafka Data Source.</p>","location":"demo/streaming-aggregation/"},{"title":"Reset numShufflePartitions","text":"<p>This step makes debugging easier since the state is only for one partition (and so it should make monitoring easier).</p> <pre><code>val numShufflePartitions = 1\nimport org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS\nspark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, numShufflePartitions)\n\nassert(spark.sessionState.conf.numShufflePartitions == numShufflePartitions)\n</code></pre>","location":"demo/streaming-aggregation/#reset-numshufflepartitions"},{"title":"Load Events from Kafka","text":"<pre><code>val events = spark\n  .readStream\n  .format(\"kafka\")\n  .option(\"subscribe\", \"demo.streaming-aggregation\")\n  .option(\"kafka.bootstrap.servers\", \":9092\")\n  .load\n  .select($\"value\" cast \"string\")\n  .withColumn(\"tokens\", split($\"value\", \",\"))\n  .withColumn(\"id\", 'tokens(0))\n  .withColumn(\"v\", 'tokens(1) cast \"int\")\n  .withColumn(\"second\", 'tokens(2) cast \"long\")\n  .withColumn(\"event_time\", 'second cast \"timestamp\") // &lt;-- Event time has to be a timestamp\n  .select(\"id\", \"v\", \"second\", \"event_time\")\n</code></pre>  <p>FIXME Consider JSON format for values</p> <p>JSONified values would make more sense. It'd certainly make the demo more verbose (extra JSON-specific \"things\") but perhaps would ease building a connection between events on the command line and their DataFrame representation.</p>","location":"demo/streaming-aggregation/#load-events-from-kafka"},{"title":"Define Windowed Streaming Aggregation","text":"<p>Define a streaming aggregation query (using groupBy high-level operator).</p> <p>The streaming query uses Append output mode and defines a streaming watermark (using Dataset.withWatermark operator). Otherwise, UnsupportedOperationChecker would fail the query.</p> <pre><code>val windowed = events\n  .withWatermark(eventTime = \"event_time\", delayThreshold = \"10 seconds\")\n  .groupBy(\n    $\"id\",\n    window(\n      timeColumn = $\"event_time\",\n      windowDuration = \"5 seconds\"))\n  .agg(\n    collect_list(\"v\") as \"vs\",\n    collect_list(\"second\") as \"seconds\")\n</code></pre> <pre><code>windowed.printSchema\n</code></pre> <pre><code>root\n |-- id: string (nullable = true)\n |-- window: struct (nullable = false)\n |    |-- start: timestamp (nullable = true)\n |    |-- end: timestamp (nullable = true)\n |-- vs: array (nullable = false)\n |    |-- element: integer (containsNull = false)\n |-- seconds: array (nullable = false)\n |    |-- element: long (containsNull = false)\n</code></pre>","location":"demo/streaming-aggregation/#define-windowed-streaming-aggregation"},{"title":"Explain Streaming Query","text":"<p>Use explain operator on a streaming query to know the trigger-specific values.</p> <p><code>ids</code> streaming query knows nothing about the OutputMode or the current streaming watermark yet:</p> <ul> <li>OutputMode is defined on write side</li> <li>Streaming watermark is read from rows at runtime</li> </ul> <p>That's why StatefulOperatorStateInfo is generic (and uses the default Append for output mode). And no batch-specific values are printed out. They will be available right after the first streaming batch.</p> <pre><code>windowed.explain\n</code></pre> <pre><code>== Physical Plan ==\nObjectHashAggregate(keys=[id#26, window#66-T10000ms], functions=[collect_list(v#30, 0, 0), collect_list(second#35L, 0, 0)])\n+- StateStoreSave [id#26, window#66-T10000ms], state info [ checkpoint = &lt;unknown&gt;, runId = f48be620-90b4-4103-a959-def26a434ea8, opId = 0, ver = 0, numPartitions = 1], Append, 0, 2\n   +- ObjectHashAggregate(keys=[id#26, window#66-T10000ms], functions=[merge_collect_list(v#30, 0, 0), merge_collect_list(second#35L, 0, 0)])\n      +- StateStoreRestore [id#26, window#66-T10000ms], state info [ checkpoint = &lt;unknown&gt;, runId = f48be620-90b4-4103-a959-def26a434ea8, opId = 0, ver = 0, numPartitions = 1], 2\n         +- ObjectHashAggregate(keys=[id#26, window#66-T10000ms], functions=[merge_collect_list(v#30, 0, 0), merge_collect_list(second#35L, 0, 0)])\n            +- Exchange hashpartitioning(id#26, window#66-T10000ms, 1), ENSURE_REQUIREMENTS, [id=#75]\n               +- ObjectHashAggregate(keys=[id#26, window#66-T10000ms], functions=[partial_collect_list(v#30, 0, 0), partial_collect_list(second#35L, 0, 0)])\n                  +- *(2) Project [named_struct(start, precisetimestampconversion(((precisetimestampconversion(event_time#41-T10000ms, TimestampType, LongType) - (((precisetimestampconversion(event_time#41-T10000ms, TimestampType, LongType) - 0) + 5000000) % 5000000)) - 0), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(event_time#41-T10000ms, TimestampType, LongType) - (((precisetimestampconversion(event_time#41-T10000ms, TimestampType, LongType) - 0) + 5000000) % 5000000)) - 0) + 5000000), LongType, TimestampType)) AS window#66-T10000ms, id#26, v#30, second#35L]\n                     +- *(2) Filter isnotnull(event_time#41-T10000ms)\n                        +- EventTimeWatermark event_time#41: timestamp, 10 seconds\n                           +- *(1) Project [id#26, v#30, second#35L, cast(second#35L as timestamp) AS event_time#41]\n                              +- *(1) Project [tokens#23[0] AS id#26, cast(tokens#23[1] as int) AS v#30, cast(tokens#23[2] as bigint) AS second#35L]\n                                 +- *(1) Project [split(cast(value#8 as string), ,, -1) AS tokens#23]\n                                    +- StreamingRelation kafka, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n</code></pre>","location":"demo/streaming-aggregation/#explain-streaming-query"},{"title":"Start Streaming Query","text":"<pre><code>import java.time.Clock\nval timeOffset = Clock.systemUTC.instant.getEpochSecond\nval queryName = s\"Demo: Streaming Aggregation ($timeOffset)\"\nval checkpointLocation = s\"/tmp/demo-checkpoint-$timeOffset\"\n\nimport scala.concurrent.duration._\nimport org.apache.spark.sql.streaming.OutputMode.Append\nimport org.apache.spark.sql.streaming.Trigger\nval sq = windowed\n  .writeStream\n  .format(\"console\")\n  .option(\"checkpointLocation\", checkpointLocation)\n  .option(\"truncate\", false)\n  .outputMode(Append)\n  .queryName(queryName)\n  .trigger(Trigger.ProcessingTime(1.seconds))\n  .start\n</code></pre> <p>The streaming query gets executed and prints out Batch 0 to the console.</p> <pre><code>-------------------------------------------\nBatch: 0\n-------------------------------------------\n+---+------+---+-------+\n|id |window|vs |seconds|\n+---+------+---+-------+\n+---+------+---+-------+\n</code></pre>","location":"demo/streaming-aggregation/#start-streaming-query"},{"title":"Start Diagnostic Query","text":"<pre><code>import java.time.Clock\nval timeOffset = Clock.systemUTC.instant.getEpochSecond\nval queryName = s\"Diagnostic Query ($timeOffset)\"\nval checkpointLocation_diag = s\"/tmp/demo-checkpoint-$timeOffset\"\n\nimport scala.concurrent.duration._\nimport org.apache.spark.sql.streaming.OutputMode.Append\nimport org.apache.spark.sql.streaming.Trigger\nevents\n  .writeStream\n  .format(\"console\")\n  .option(\"checkpointLocation\", checkpointLocation_diag)\n  .option(\"truncate\", false)\n  .queryName(queryName)\n  .trigger(Trigger.ProcessingTime(1.seconds))\n  .start\n</code></pre> <p>The query immediately prints out the following Batch 0.</p> <pre><code>-------------------------------------------\nBatch: 0\n-------------------------------------------\n+---+---+------+----------+\n|id |v  |second|event_time|\n+---+---+------+----------+\n+---+---+------+----------+\n</code></pre>","location":"demo/streaming-aggregation/#start-diagnostic-query"},{"title":"Send Events","text":"<p>The window duration is <code>5 seconds</code> with a delay of <code>10 seconds</code> so it really takes 15 seconds to start getting final results (materialization).</p> <pre><code>echo \"1,1,1\" | kcat -P -b :9092 -t demo.streaming-aggregation\n</code></pre> <p>This will make the streaming query to print out Batch 1 to the console.</p> <pre><code>-------------------------------------------\nBatch: 1\n-------------------------------------------\n+---+------+---+-------+\n|id |window|vs |seconds|\n+---+------+---+-------+\n+---+------+---+-------+\n</code></pre> <p>Use <code>6</code> as the event time (that is a second after <code>5 seconds</code> window duration).</p> <pre><code>echo \"1,2,6\" | kcat -P -b :9092 -t demo.streaming-aggregation\n</code></pre> <p>There should be no final result printed out yet (just an empty Batch 2).</p> <pre><code>-------------------------------------------\nBatch: 2\n-------------------------------------------\n+---+------+---+-------+\n|id |window|vs |seconds|\n+---+------+---+-------+\n+---+------+---+-------+\n</code></pre> <p>Use <code>16</code> as the event time (that is a second after <code>5 seconds</code> window duration and <code>10 seconds</code> delay).</p> <pre><code>echo \"1,3,16\" | kcat -P -b :9092 -t demo.streaming-aggregation\n</code></pre> <p>That should produce the first final result (as Batch 4).</p> <pre><code>-------------------------------------------\nBatch: 3\n-------------------------------------------\n+---+------+---+-------+\n|id |window|vs |seconds|\n+---+------+---+-------+\n+---+------+---+-------+\n\n-------------------------------------------\nBatch: 4\n-------------------------------------------\n+---+------------------------------------------+---+-------+\n|id |window                                    |vs |seconds|\n+---+------------------------------------------+---+-------+\n|1  |{1970-01-01 01:00:00, 1970-01-01 01:00:05}|[1]|[1]    |\n+---+------------------------------------------+---+-------+\n</code></pre>","location":"demo/streaming-aggregation/#send-events"},{"title":"Monitor Query","text":"<pre><code>val lastProgress = sq.lastProgress\n</code></pre> <pre><code>println(lastProgress)\n</code></pre> <pre><code>{\n  \"id\" : \"8b27e38c-a73c-49db-8bfd-2d213d11b8a2\",\n  \"runId\" : \"4d3887d9-33e8-43f5-b0f9-d815d06bf039\",\n  \"name\" : \"Demo: Streaming Aggregation (1665063267)\",\n  \"timestamp\" : \"2022-10-06T13:37:39.005Z\",\n  \"batchId\" : 5,\n  \"numInputRows\" : 0,\n  \"inputRowsPerSecond\" : 0.0,\n  \"processedRowsPerSecond\" : 0.0,\n  \"durationMs\" : {\n    \"latestOffset\" : 3,\n    \"triggerExecution\" : 3\n  },\n  \"eventTime\" : {\n    \"watermark\" : \"1970-01-01T00:00:06.000Z\"\n  },\n  \"stateOperators\" : [ {\n    \"operatorName\" : \"stateStoreSave\",\n    \"numRowsTotal\" : 2,\n    \"numRowsUpdated\" : 0,\n    \"allUpdatesTimeMs\" : 6,\n    \"numRowsRemoved\" : 1,\n    \"allRemovalsTimeMs\" : 21,\n    \"commitTimeMs\" : 26,\n    \"memoryUsedBytes\" : 1504,\n    \"numRowsDroppedByWatermark\" : 0,\n    \"numShufflePartitions\" : 1,\n    \"numStateStoreInstances\" : 1,\n    \"customMetrics\" : {\n      \"loadedMapCacheHitCount\" : 8,\n      \"loadedMapCacheMissCount\" : 0,\n      \"stateOnCurrentVersionSizeBytes\" : 784\n    }\n  } ],\n  \"sources\" : [ {\n    \"description\" : \"KafkaV2[Subscribe[demo.streaming-aggregation]]\",\n    \"startOffset\" : {\n      \"demo.streaming-aggregation\" : {\n        \"0\" : 30\n      }\n    },\n    \"endOffset\" : {\n      \"demo.streaming-aggregation\" : {\n        \"0\" : 30\n      }\n    },\n    \"latestOffset\" : {\n      \"demo.streaming-aggregation\" : {\n        \"0\" : 30\n      }\n    },\n    \"numInputRows\" : 0,\n    \"inputRowsPerSecond\" : 0.0,\n    \"processedRowsPerSecond\" : 0.0,\n    \"metrics\" : {\n      \"avgOffsetsBehindLatest\" : \"0.0\",\n      \"maxOffsetsBehindLatest\" : \"0\",\n      \"minOffsetsBehindLatest\" : \"0\"\n    }\n  } ],\n  \"sink\" : {\n    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@606d6243\",\n    \"numOutputRows\" : 0\n  }\n}\n</code></pre> <pre><code>assert(lastProgress.isInstanceOf[org.apache.spark.sql.streaming.StreamingQueryProgress])\nassert(lastProgress.stateOperators.length == 1, \"There should be one stateful operator\")\n</code></pre> <pre><code>println(lastProgress.stateOperators.head.prettyJson)\n</code></pre> <pre><code>{\n  \"operatorName\" : \"stateStoreSave\",\n  \"numRowsTotal\" : 2,\n  \"numRowsUpdated\" : 0,\n  \"allUpdatesTimeMs\" : 6,\n  \"numRowsRemoved\" : 1,\n  \"allRemovalsTimeMs\" : 21,\n  \"commitTimeMs\" : 26,\n  \"memoryUsedBytes\" : 1504,\n  \"numRowsDroppedByWatermark\" : 0,\n  \"numShufflePartitions\" : 1,\n  \"numStateStoreInstances\" : 1,\n  \"customMetrics\" : {\n    \"loadedMapCacheHitCount\" : 8,\n    \"loadedMapCacheMissCount\" : 0,\n    \"stateOnCurrentVersionSizeBytes\" : 784\n  }\n}\n</code></pre> <pre><code>assert(lastProgress.sources.length == 1, \"There should be one streaming source only\")\n</code></pre> <pre><code>println(lastProgress.sources.head.prettyJson)\n</code></pre> <pre><code>{\n  \"description\" : \"KafkaV2[Subscribe[demo.streaming-aggregation]]\",\n  \"startOffset\" : {\n    \"demo.streaming-aggregation\" : {\n      \"0\" : 30\n    }\n  },\n  \"endOffset\" : {\n    \"demo.streaming-aggregation\" : {\n      \"0\" : 30\n    }\n  },\n  \"latestOffset\" : {\n    \"demo.streaming-aggregation\" : {\n      \"0\" : 30\n    }\n  },\n  \"numInputRows\" : 0,\n  \"inputRowsPerSecond\" : 0.0,\n  \"processedRowsPerSecond\" : 0.0,\n  \"metrics\" : {\n    \"avgOffsetsBehindLatest\" : \"0.0\",\n    \"maxOffsetsBehindLatest\" : \"0\",\n    \"minOffsetsBehindLatest\" : \"0\"\n  }\n}\n</code></pre>","location":"demo/streaming-aggregation/#monitor-query"},{"title":"Cleanup","text":"<pre><code>spark.streams.active.foreach(_.stop)\n</code></pre>","location":"demo/streaming-aggregation/#cleanup"},{"title":"Demo: Streaming Watermark","text":"<p>This demo shows the internals of streaming watermark with Kafka Data Source.</p>  <p>Note</p> <p>Please start a Kafka cluster and <code>spark-shell</code> as described in Demo: Kafka Data Source.</p>","location":"demo/streaming-watermark/"},{"title":"Streaming Query with Watermark","text":"<pre><code>import java.time.Clock\nval timeOffset = Clock.systemUTC.instant.getEpochSecond\n</code></pre> <pre><code>val queryName = s\"Demo: Streaming Watermark ($timeOffset)\"\nval checkpointLocation = s\"/tmp/demo-checkpoint-$timeOffset\"\n</code></pre> <pre><code>import org.apache.spark.sql.streaming.Trigger\nimport scala.concurrent.duration._\nval eventTimeCol = (($\"tokens\"(2) cast \"long\") + lit(timeOffset)) cast \"timestamp\" as \"event_time\"\nval sq = spark\n  .readStream\n  .format(\"kafka\")\n  .option(\"subscribe\", \"demo.streaming-watermark\")\n  .option(\"kafka.bootstrap.servers\", \":9092\")\n  .load\n  .select($\"value\" cast \"string\")\n  .select(split($\"value\", \",\") as \"tokens\")\n  .select(\n    $\"tokens\"(0) as \"id\" cast \"long\",\n    $\"tokens\"(1) as \"name\",\n    eventTimeCol)\n  .withWatermark(eventTime = \"event_time\", delayThreshold = \"5 seconds\")\n  .writeStream\n  .format(\"console\")\n  .queryName(queryName)\n  .trigger(Trigger.ProcessingTime(1.seconds))\n  .option(\"checkpointLocation\", checkpointLocation)\n  .option(\"truncate\", false)\n  .start\n</code></pre>","location":"demo/streaming-watermark/#streaming-query-with-watermark"},{"title":"Send Events","text":"<pre><code>echo \"0,zero,0\" | kcat -P -b :9092 -t demo.streaming-watermark\n</code></pre> <pre><code>echo \"10,ten,10\" | kcat -P -b :9092 -t demo.streaming-watermark\n</code></pre>","location":"demo/streaming-watermark/#send-events"},{"title":"Demo: Using File Streaming Source","text":"<p>This demo shows a streaming query that reads files using FileStreamSource.</p>","location":"demo/using-file-streaming-source/"},{"title":"Prerequisites","text":"<p>Make sure that the source directory is available before starting the query.</p> <pre><code>mkdir /tmp/text-logs\n</code></pre>","location":"demo/using-file-streaming-source/#prerequisites"},{"title":"Configure Logging","text":"<p>Enable logging for <code>FileStreamSource</code>.</p>","location":"demo/using-file-streaming-source/#configure-logging"},{"title":"Start Streaming Query","text":"<p>Use <code>spark-shell</code> for fast interactive prototyping.</p> <p>Describe a source to load data from.</p> <pre><code>val lines = spark\n  .readStream\n  .format(\"text\")\n  .option(\"maxFilesPerTrigger\", 1)\n  .load(\"/tmp/text-logs\")\n</code></pre> <p>Show the schema.</p> <pre><code>scala&gt; lines.printSchema\nroot\n |-- value: string (nullable = true)\n</code></pre> <p>Describe the sink (<code>console</code>) and start the streaming query.</p> <pre><code>import org.apache.spark.sql.streaming.Trigger\nimport concurrent.duration._\nval interval = 15.seconds\nval trigger = Trigger.ProcessingTime(interval)\nval queryName = s\"one file every micro-batch (every $interval)\"\nval sq = lines\n  .writeStream\n  .format(\"console\")\n  .option(\"checkpointLocation\", \"/tmp/checkpointLocation\")\n  .trigger(trigger)\n  .queryName(queryName)\n  .start\n</code></pre> <p>Use web UI to monitor the query (http://localhost:4040).</p>","location":"demo/using-file-streaming-source/#start-streaming-query"},{"title":"Stop Query","text":"<pre><code>spark.streams.active.foreach(_.stop)\n</code></pre>","location":"demo/using-file-streaming-source/#stop-query"},{"title":"Demo: Streaming Watermark with Aggregation in Append Output Mode","text":"<p>The following demo shows the behaviour and the internals of streaming watermark with a streaming aggregation in Append output mode.</p> <p>The demo also shows the behaviour and the internals of StateStoreSaveExec physical operator in Append output mode.</p>  <p>Tip</p> <p>The below code is part of StreamingAggregationAppendMode streaming application.</p>  <pre><code>// Reduce the number of partitions and hence the state stores\n// That is supposed to make debugging state checkpointing easier\nval numShufflePartitions = 1\nimport org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS\nspark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, numShufflePartitions)\nassert(spark.sessionState.conf.numShufflePartitions == numShufflePartitions)\n\n// Define event \"format\"\n// Use :paste mode in spark-shell\nimport java.sql.Timestamp\ncase class Event(time: Timestamp, value: Long, batch: Long)\nimport scala.concurrent.duration._\nobject Event {\n  def apply(secs: Long, value: Long, batch: Long): Event = {\n    Event(new Timestamp(secs.seconds.toMillis), value, batch)\n  }\n}\n\n// Using memory data source for full control of the input\nimport org.apache.spark.sql.execution.streaming.MemoryStream\nimplicit val sqlCtx = spark.sqlContext\nval events = MemoryStream[Event]\nval values = events.toDS\nassert(values.isStreaming, \"values must be a streaming Dataset\")\n\nvalues.printSchema\n/**\nroot\n |-- time: timestamp (nullable = true)\n |-- value: long (nullable = false)\n |-- batch: long (nullable = false)\n*/\n\n// Streaming aggregation using groupBy operator to demo StateStoreSaveExec operator\n// Define required watermark for late events for Append output mode\n\nimport scala.concurrent.duration._\nval delayThreshold = 10.seconds\nval eventTime = \"time\"\n\nval valuesWatermarked = values\n  .withWatermark(eventTime, delayThreshold.toString) // defines watermark (before groupBy!)\n\n// EventTimeWatermark logical operator is planned as EventTimeWatermarkExec physical operator\n// Note that as a physical operator EventTimeWatermarkExec shows itself without the Exec suffix\nvaluesWatermarked.explain\n/**\n== Physical Plan ==\nEventTimeWatermark time#3: timestamp, interval 10 seconds\n+- StreamingRelation MemoryStream[time#3,value#4L,batch#5L], [time#3, value#4L, batch#5L]\n*/\n\nval windowDuration = 5.seconds\nimport org.apache.spark.sql.functions.window\nval countsPer5secWindow = valuesWatermarked\n  .groupBy(window(col(eventTime), windowDuration.toString) as \"sliding_window\")\n  .agg(collect_list(\"batch\") as \"batches\", collect_list(\"value\") as \"values\")\n\ncountsPer5secWindow.printSchema\n/**\nroot\n |-- sliding_window: struct (nullable = false)\n |    |-- start: timestamp (nullable = true)\n |    |-- end: timestamp (nullable = true)\n |-- batches: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- values: array (nullable = true)\n |    |-- element: long (containsNull = true)\n*/\n\n// valuesPerGroupWindowed is a streaming Dataset with just one source\n// It knows nothing about output mode or watermark yet\n// That's why StatefulOperatorStateInfo is generic\n// and no batch-specific values are printed out\n// That will be available after the first streaming batch\n// Use sq.explain to know the runtime-specific values\ncountsPer5secWindow.explain\n/**\n== Physical Plan ==\nObjectHashAggregate(keys=[window#23-T10000ms], functions=[collect_list(batch#5L, 0, 0), collect_list(value#4L, 0, 0)])\n+- StateStoreSave [window#23-T10000ms], state info [ checkpoint = &lt;unknown&gt;, runId = 50e62943-fe5d-4a02-8498-7134ecbf5122, opId = 0, ver = 0, numPartitions = 1], Append, 0, 2\n   +- ObjectHashAggregate(keys=[window#23-T10000ms], functions=[merge_collect_list(batch#5L, 0, 0), merge_collect_list(value#4L, 0, 0)])\n      +- StateStoreRestore [window#23-T10000ms], state info [ checkpoint = &lt;unknown&gt;, runId = 50e62943-fe5d-4a02-8498-7134ecbf5122, opId = 0, ver = 0, numPartitions = 1], 2\n         +- ObjectHashAggregate(keys=[window#23-T10000ms], functions=[merge_collect_list(batch#5L, 0, 0), merge_collect_list(value#4L, 0, 0)])\n            +- Exchange hashpartitioning(window#23-T10000ms, 1)\n               +- ObjectHashAggregate(keys=[window#23-T10000ms], functions=[partial_collect_list(batch#5L, 0, 0), partial_collect_list(value#4L, 0, 0)])\n                  +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#23-T10000ms, value#4L, batch#5L]\n                     +- *(1) Filter isnotnull(time#3-T10000ms)\n                        +- EventTimeWatermark time#3: timestamp, interval 10 seconds\n                           +- StreamingRelation MemoryStream[time#3,value#4L,batch#5L], [time#3, value#4L, batch#5L]\n*/\n\nval queryName = \"watermark_demo\"\nval checkpointLocation = s\"/tmp/checkpoint-$queryName\"\n\n// Delete the checkpoint location from previous executions\nimport java.nio.file.{Files, FileSystems}\nimport java.util.Comparator\nimport scala.collection.JavaConverters._\nval path = FileSystems.getDefault.getPath(checkpointLocation)\nif (Files.exists(path)) {\n  Files.walk(path)\n    .sorted(Comparator.reverseOrder())\n    .iterator\n    .asScala\n    .foreach(p =&gt; p.toFile.delete)\n}\n\n// FIXME Use foreachBatch for batchId and the output Dataset\n// Start the query and hence StateStoreSaveExec\nimport scala.concurrent.duration._\nimport org.apache.spark.sql.streaming.OutputMode\nval streamingQuery = countsPer5secWindow\n  .writeStream\n  .format(\"memory\")\n  .queryName(queryName)\n  .option(\"checkpointLocation\", checkpointLocation)\n  .outputMode(OutputMode.Append) // &lt;-- Use Append output mode\n  .start\n\nassert(streamingQuery.status.message == \"Waiting for data to arrive\")\n\ntype Millis = Long\ndef toMillis(datetime: String): Millis = {\n  import java.time.format.DateTimeFormatter\n  import java.time.LocalDateTime\n  import java.time.ZoneOffset\n  LocalDateTime\n    .parse(datetime, DateTimeFormatter.ISO_DATE_TIME)\n    .toInstant(ZoneOffset.UTC)\n    .toEpochMilli\n}\n\n// Use web UI to monitor the state of state (no pun intended)\n// StateStoreSave and StateStoreRestore operators all have state metrics\n// Go to http://localhost:4040/SQL/ and click one of the Completed Queries with Job IDs\n\n// You may also want to check out checkpointed state\n// in /tmp/checkpoint-watermark_demo/state/0/0\n\n// The demo is aimed to show the following:\n// 1. The current watermark\n// 2. Check out the stats:\n// - expired state (below the current watermark, goes to output and purged later)\n// - late state (dropped as if never received and processed)\n// - saved state rows (above the current watermark)\n\nval batch = Seq(\n  Event(1,  1, batch = 1),\n  Event(15, 2, batch = 1))\nevents.addData(batch)\nstreamingQuery.processAllAvailable()\n\nprintln(streamingQuery.lastProgress.stateOperators(0).prettyJson)\n/**\n{\n  \"numRowsTotal\" : 1,\n  \"numRowsUpdated\" : 0,\n  \"memoryUsedBytes\" : 1102,\n  \"customMetrics\" : {\n    \"loadedMapCacheHitCount\" : 2,\n    \"loadedMapCacheMissCount\" : 0,\n    \"stateOnCurrentVersionSizeBytes\" : 414\n  }\n}\n*/\n\nval currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\")\nval currentWatermarkMs = toMillis(currentWatermark)\n\nval maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds\nval expectedMaxTime = 15\nassert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\")\n\nval expectedWatermarkMs = 5.seconds.toMillis\nassert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\")\n\n// FIXME Saved State Rows\n// Use the metrics of the StateStoreSave operator\n// Or simply streamingQuery.lastProgress.stateOperators.head\nspark.table(queryName).orderBy(\"sliding_window\").show(truncate = false)\n/**\n+------------------------------------------+-------+------+\n|sliding_window                            |batches|values|\n+------------------------------------------+-------+------+\n|[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1]    |[1]   |\n+------------------------------------------+-------+------+\n*/\n\n// With at least one execution we can review the execution plan\nstreamingQuery.explain\n/**\nscala&gt; streamingQuery.explain\n== Physical Plan ==\nObjectHashAggregate(keys=[window#18-T10000ms], functions=[collect_list(batch#5L, 0, 0), collect_list(value#4L, 0, 0)])\n+- StateStoreSave [window#18-T10000ms], state info [ checkpoint = file:/tmp/checkpoint-watermark_demo/state, runId = 73bb0ede-20f2-400d-8003-aa2fbebdd2e1, opId = 0, ver = 1, numPartitions = 1], Append, 5000, 2\n   +- ObjectHashAggregate(keys=[window#18-T10000ms], functions=[merge_collect_list(batch#5L, 0, 0), merge_collect_list(value#4L, 0, 0)])\n      +- StateStoreRestore [window#18-T10000ms], state info [ checkpoint = file:/tmp/checkpoint-watermark_demo/state, runId = 73bb0ede-20f2-400d-8003-aa2fbebdd2e1, opId = 0, ver = 1, numPartitions = 1], 2\n         +- ObjectHashAggregate(keys=[window#18-T10000ms], functions=[merge_collect_list(batch#5L, 0, 0), merge_collect_list(value#4L, 0, 0)])\n            +- Exchange hashpartitioning(window#18-T10000ms, 1)\n               +- ObjectHashAggregate(keys=[window#18-T10000ms], functions=[partial_collect_list(batch#5L, 0, 0), partial_collect_list(value#4L, 0, 0)])\n                  +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#18-T10000ms, value#4L, batch#5L]\n                     +- *(1) Filter isnotnull(time#3-T10000ms)\n                        +- EventTimeWatermark time#3: timestamp, interval 10 seconds\n                           +- LocalTableScan &lt;empty&gt;, [time#3, value#4L, batch#5L]\n*/\n\nimport org.apache.spark.sql.execution.streaming.StreamingQueryWrapper\nval engine = streamingQuery\n  .asInstanceOf[StreamingQueryWrapper]\n  .streamingQuery\nimport org.apache.spark.sql.execution.streaming.StreamExecution\nassert(engine.isInstanceOf[StreamExecution])\n\nval lastMicroBatch = engine.lastExecution\nimport org.apache.spark.sql.execution.streaming.IncrementalExecution\nassert(lastMicroBatch.isInstanceOf[IncrementalExecution])\n\n// Access executedPlan that is the optimized physical query plan ready for execution\n// All streaming optimizations have been applied at this point\n// We just need the EventTimeWatermarkExec physical operator\nval plan = lastMicroBatch.executedPlan\n\n// Let's find the EventTimeWatermarkExec physical operator in the plan\n// There should be one only\nimport org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec\nval watermarkOp = plan.collect { case op: EventTimeWatermarkExec =&gt; op }.head\n\n// Let's check out the event-time watermark stats\n// They correspond to the concrete EventTimeWatermarkExec operator for a micro-batch\nval stats = watermarkOp.eventTimeStats.value\nimport org.apache.spark.sql.execution.streaming.EventTimeStats\nassert(stats.isInstanceOf[EventTimeStats])\n\nprintln(stats)\n/**\nEventTimeStats(-9223372036854775808,9223372036854775807,0.0,0)\n*/\n\nval batch = Seq(\n  Event(1,  1, batch = 2),\n  Event(15, 2, batch = 2),\n  Event(35, 3, batch = 2))\nevents.addData(batch)\nstreamingQuery.processAllAvailable()\n\nval currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\")\nval currentWatermarkMs = toMillis(currentWatermark)\n\nval maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds\nval expectedMaxTime = 35\nassert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\")\n\nval expectedWatermarkMs = 25.seconds.toMillis\nassert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\")\n\n// FIXME Expired State\n// FIXME Late Events\n// FIXME Saved State Rows\nspark.table(queryName).orderBy(\"sliding_window\").show(truncate = false)\n/**\n+------------------------------------------+-------+------+\n|sliding_window                            |batches|values|\n+------------------------------------------+-------+------+\n|[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1]    |[1]   |\n|[1970-01-01 01:00:15, 1970-01-01 01:00:20]|[1, 2] |[2, 2]|\n+------------------------------------------+-------+------+\n*/\n\n// Check out the event-time watermark stats\nval plan = engine.lastExecution.executedPlan\nimport org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec\nval watermarkOp = plan.collect { case op: EventTimeWatermarkExec =&gt; op }.head\nval stats = watermarkOp.eventTimeStats.value\nimport org.apache.spark.sql.execution.streaming.EventTimeStats\nassert(stats.isInstanceOf[EventTimeStats])\n\nprintln(stats)\n/**\nEventTimeStats(-9223372036854775808,9223372036854775807,0.0,0)\n*/\n\nval batch = Seq(\n  Event(15,1, batch = 3),\n  Event(15,2, batch = 3),\n  Event(20,3, batch = 3),\n  Event(26,4, batch = 3))\nevents.addData(batch)\nstreamingQuery.processAllAvailable()\n\nval currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\")\nval currentWatermarkMs = toMillis(currentWatermark)\n\nval maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds\nval expectedMaxTime = 26\nassert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\")\n\n// Current event-time watermark should be the same as previously\n// val expectedWatermarkMs = 25.seconds.toMillis\n// The current max time is merely 26 so subtracting delayThreshold gives merely 16\nassert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\")\n\n// FIXME Expired State\n// FIXME Late Events\n// FIXME Saved State Rows\nspark.table(queryName).orderBy(\"sliding_window\").show(truncate = false)\n/**\n+------------------------------------------+-------+------+\n|sliding_window                            |batches|values|\n+------------------------------------------+-------+------+\n|[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1]    |[1]   |\n|[1970-01-01 01:00:15, 1970-01-01 01:00:20]|[1, 2] |[2, 2]|\n+------------------------------------------+-------+------+\n*/\n\n// Check out the event-time watermark stats\nval plan = engine.lastExecution.executedPlan\nimport org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec\nval watermarkOp = plan.collect { case op: EventTimeWatermarkExec =&gt; op }.head\nval stats = watermarkOp.eventTimeStats.value\nimport org.apache.spark.sql.execution.streaming.EventTimeStats\nassert(stats.isInstanceOf[EventTimeStats])\n\nprintln(stats)\n/**\nEventTimeStats(26000,15000,19000.0,4)\n*/\n\nval batch = Seq(\n  Event(36, 1, batch = 4))\nevents.addData(batch)\nstreamingQuery.processAllAvailable()\n\nval currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\")\nval currentWatermarkMs = toMillis(currentWatermark)\n\nval maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds\nval expectedMaxTime = 36\nassert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\")\n\nval expectedWatermarkMs = 26.seconds.toMillis\nassert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\")\n\n// FIXME Expired State\n// FIXME Late Events\n// FIXME Saved State Rows\nspark.table(queryName).orderBy(\"sliding_window\").show(truncate = false)\n/**\n+------------------------------------------+-------+------+\n|sliding_window                            |batches|values|\n+------------------------------------------+-------+------+\n|[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1]    |[1]   |\n|[1970-01-01 01:00:15, 1970-01-01 01:00:20]|[1, 2] |[2, 2]|\n+------------------------------------------+-------+------+\n*/\n\n// Check out the event-time watermark stats\nval plan = engine.lastExecution.executedPlan\nimport org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec\nval watermarkOp = plan.collect { case op: EventTimeWatermarkExec =&gt; op }.head\nval stats = watermarkOp.eventTimeStats.value\nimport org.apache.spark.sql.execution.streaming.EventTimeStats\nassert(stats.isInstanceOf[EventTimeStats])\n\nprintln(stats)\n/**\nEventTimeStats(-9223372036854775808,9223372036854775807,0.0,0)\n*/\n\nval batch = Seq(\n  Event(50, 1, batch = 5)\n)\nevents.addData(batch)\nstreamingQuery.processAllAvailable()\n\nval currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\")\nval currentWatermarkMs = toMillis(currentWatermark)\n\nval maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds\nval expectedMaxTime = 50\nassert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\")\n\nval expectedWatermarkMs = 40.seconds.toMillis\nassert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\")\n\n// FIXME Expired State\n// FIXME Late Events\n// FIXME Saved State Rows\nspark.table(queryName).orderBy(\"sliding_window\").show(truncate = false)\n/**\n+------------------------------------------+-------+------+\n|sliding_window                            |batches|values|\n+------------------------------------------+-------+------+\n|[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1]    |[1]   |\n|[1970-01-01 01:00:15, 1970-01-01 01:00:20]|[1, 2] |[2, 2]|\n|[1970-01-01 01:00:25, 1970-01-01 01:00:30]|[3]    |[4]   |\n|[1970-01-01 01:00:35, 1970-01-01 01:00:40]|[2, 4] |[3, 1]|\n+------------------------------------------+-------+------+\n*/\n\n// Check out the event-time watermark stats\nval plan = engine.lastExecution.executedPlan\nimport org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec\nval watermarkOp = plan.collect { case op: EventTimeWatermarkExec =&gt; op }.head\nval stats = watermarkOp.eventTimeStats.value\nimport org.apache.spark.sql.execution.streaming.EventTimeStats\nassert(stats.isInstanceOf[EventTimeStats])\n\nprintln(stats)\n/**\nEventTimeStats(-9223372036854775808,9223372036854775807,0.0,0)\n*/\n\n// Eventually...\nstreamingQuery.stop()\n</code></pre>","location":"demo/watermark-aggregation-append/"},{"title":"Execution Planning Strategies","text":"","location":"execution-planning-strategies/"},{"title":"FlatMapGroupsWithStateStrategy Execution Planning Strategy","text":"<p><code>FlatMapGroupsWithStateStrategy</code> is an execution planning strategy (Spark SQL) that plans streaming queries with FlatMapGroupsWithState logical operators to FlatMapGroupsWithStateExec physical operator (with undefined <code>StatefulOperatorStateInfo</code>, <code>batchTimestampMs</code>, and <code>eventTimeWatermark</code>).</p> <p><code>FlatMapGroupsWithStateStrategy</code> is part of <code>extraPlanningStrategies</code> of the SparkPlanner (of IncrementalExecution).</p>","location":"execution-planning-strategies/FlatMapGroupsWithStateStrategy/"},{"title":"spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion <p><code>FlatMapGroupsWithStateStrategy</code> uses spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion for the state format version of FlatMapGroupsWithStateExec.</p>","text":"","location":"execution-planning-strategies/FlatMapGroupsWithStateStrategy/#sparksqlstreamingflatmapgroupswithstatestateformatversion"},{"title":"Demo <pre><code>import org.apache.spark.sql.streaming.GroupState\nval stateFunc = (key: Long, values: Iterator[(Timestamp, Long)], state: GroupState[Long]) =&gt; {\n  Iterator((key, values.size))\n}\nimport java.sql.Timestamp\nimport org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode}\nval numGroups = spark.\n  readStream.\n  format(\"rate\").\n  load.\n  as[(Timestamp, Long)].\n  groupByKey { case (time, value) =&gt; value % 2 }.\n  flatMapGroupsWithState(OutputMode.Update, GroupStateTimeout.NoTimeout)(stateFunc)\n</code></pre> <pre><code>scala&gt; numGroups.explain(true)\n== Parsed Logical Plan ==\n'SerializeFromObject [assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1 AS _1#267L, assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2 AS _2#268]\n+- 'FlatMapGroupsWithState &lt;function3&gt;, unresolveddeserializer(upcast(getcolumnbyordinal(0, LongType), LongType, - root class: \"scala.Long\"), value#262L), unresolveddeserializer(newInstance(class scala.Tuple2), timestamp#253, value#254L), [value#262L], [timestamp#253, value#254L], obj#266: scala.Tuple2, class[value[0]: bigint], Update, false, NoTimeout\n   +- AppendColumns &lt;function1&gt;, class scala.Tuple2, [StructField(_1,TimestampType,true), StructField(_2,LongType,false)], newInstance(class scala.Tuple2), [input[0, bigint, false] AS value#262L]\n      +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@38bcac50,rate,List(),None,List(),None,Map(),None), rate, [timestamp#253, value#254L]\n\n...\n\n== Physical Plan ==\n*SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#267L, assertnotnull(input[0, scala.Tuple2, true])._2 AS _2#268]\n+- FlatMapGroupsWithState &lt;function3&gt;, value#262: bigint, newInstance(class scala.Tuple2), [value#262L], [timestamp#253, value#254L], obj#266: scala.Tuple2, StatefulOperatorStateInfo(&lt;unknown&gt;,84b5dccb-3fa6-4343-a99c-6fa5490c9b33,0,0), class[value[0]: bigint], Update, NoTimeout, 0, 0\n   +- *Sort [value#262L ASC NULLS FIRST], false, 0\n      +- Exchange hashpartitioning(value#262L, 200)\n         +- AppendColumns &lt;function1&gt;, newInstance(class scala.Tuple2), [input[0, bigint, false] AS value#262L]\n            +- StreamingRelation rate, [timestamp#253, value#254L]\n</code></pre>","text":"","location":"execution-planning-strategies/FlatMapGroupsWithStateStrategy/#demo"},{"title":"StatefulAggregationStrategy Execution Planning Strategy","text":"<p><code>StatefulAggregationStrategy</code> is an execution planning strategy (Spark SQL) to plan streaming queries with EventTimeWatermark and <code>Aggregate</code> (Spark SQL) logical operators.</p>    Logical Operator Physical Operator     EventTimeWatermark EventTimeWatermarkExec   <code>Aggregate</code> (Spark SQL) One of the following per selection requirements:<ul><li><code>HashAggregateExec</code> (Spark SQL)</li><li><code>ObjectHashAggregateExec</code> (Spark SQL)</li><li><code>SortAggregateExec</code> (Spark SQL)</li></ul>    <p><code>StatefulAggregationStrategy</code> is used when IncrementalExecution is requested to plan a streaming query.</p>","location":"execution-planning-strategies/StatefulAggregationStrategy/"},{"title":"Accessing StatefulAggregationStrategy","text":"<p><code>StatefulAggregationStrategy</code> is available using <code>SessionState</code>.</p> <pre><code>spark.sessionState.planner.StatefulAggregationStrategy\n</code></pre>","location":"execution-planning-strategies/StatefulAggregationStrategy/#accessing-statefulaggregationstrategy"},{"title":"planStreamingAggregation <pre><code>planStreamingAggregation(\n  groupingExpressions: Seq[NamedExpression],\n  functionsWithoutDistinct: Seq[AggregateExpression],\n  resultExpressions: Seq[NamedExpression],\n  stateFormatVersion: Int,\n  child: SparkPlan): Seq[SparkPlan]\n</code></pre> <p><code>planStreamingAggregation</code> creates a streaming aggregate physical operator for <code>Partial</code> aggregation (with the given <code>child</code> physical operator as the child). The given <code>functionsWithoutDistinct</code> expressions are set up to work in <code>Partial</code> execution mode.</p> <p><code>planStreamingAggregation</code> creates another streaming aggregate physical operator for <code>PartialMerge</code> aggregation (with the partial aggregate physical operator as the child). The given <code>functionsWithoutDistinct</code> expressions are set up to work in <code>PartialMerge</code> execution mode.</p> <p><code>planStreamingAggregation</code> creates a StateStoreRestoreExec physical operator (with the partial-merge aggregate physical operator as the child).</p> <p><code>planStreamingAggregation</code> creates another streaming aggregate physical operator for <code>PartialMerge</code> aggregation (with the <code>StateStoreRestoreExec</code> physical operator as the child). The given <code>functionsWithoutDistinct</code> expressions are set up to work in <code>PartialMerge</code> execution mode.</p> <p><code>planStreamingAggregation</code> creates a StateStoreSaveExec physical operator (with the last partial-merge aggregate physical operator as the child).</p> <p>In the end, <code>planStreamingAggregation</code> creates another streaming aggregate physical operator for <code>Final</code> aggregation (with the <code>StateStoreSaveExec</code> physical operator as the child). The given <code>functionsWithoutDistinct</code> expressions are set up to work in <code>Final</code> execution mode.</p>  <p><code>planStreamingAggregation</code> is used when:</p> <ul> <li><code>StatefulAggregationStrategy</code> execution planning strategy is planning a streaming query with <code>Aggregate</code> (Spark SQL) logical operator with no session window</li> </ul>","text":"","location":"execution-planning-strategies/StatefulAggregationStrategy/#planstreamingaggregation"},{"title":"planStreamingAggregationForSession <pre><code>planStreamingAggregationForSession(\n  groupingExpressions: Seq[NamedExpression],\n  sessionExpression: NamedExpression,\n  functionsWithoutDistinct: Seq[AggregateExpression],\n  resultExpressions: Seq[NamedExpression],\n  stateFormatVersion: Int,\n  mergeSessionsInLocalPartition: Boolean,\n  child: SparkPlan): Seq[SparkPlan]\n</code></pre> <p><code>planStreamingAggregationForSession</code>...FIXME</p>  <p><code>planStreamingAggregationForSession</code> is used when:</p> <ul> <li><code>StatefulAggregationStrategy</code> execution planning strategy is planning a streaming query with <code>Aggregate</code> (Spark SQL) logical operator with session window</li> </ul>","text":"","location":"execution-planning-strategies/StatefulAggregationStrategy/#planstreamingaggregationforsession"},{"title":"Creating Streaming Aggregate Physical Operator <pre><code>createStreamingAggregate(\n  requiredChildDistributionExpressions: Option[Seq[Expression]] = None,\n  groupingExpressions: Seq[NamedExpression] = Nil,\n  aggregateExpressions: Seq[AggregateExpression] = Nil,\n  aggregateAttributes: Seq[Attribute] = Nil,\n  initialInputBufferOffset: Int = 0,\n  resultExpressions: Seq[NamedExpression] = Nil,\n  child: SparkPlan): SparkPlan\n</code></pre> <p><code>createStreamingAggregate</code> creates one of the following physical operators:</p> <ul> <li><code>HashAggregateExec</code> (Spark SQL)</li> <li><code>ObjectHashAggregateExec</code> (Spark SQL)</li> <li><code>SortAggregateExec</code> (Spark SQL)</li> </ul>  <p>Note</p> <p>Learn more about the selection requirements in The Internals of Spark SQL.</p>","text":"","location":"execution-planning-strategies/StatefulAggregationStrategy/#creating-streaming-aggregate-physical-operator"},{"title":"Demo <pre><code>val counts = spark.\n  readStream.\n  format(\"rate\").\n  load.\n  groupBy(window($\"timestamp\", \"5 seconds\") as \"group\").\n  agg(count(\"value\") as \"count\").\n  orderBy(\"group\")\nscala&gt; counts.explain\n== Physical Plan ==\n*Sort [group#6 ASC NULLS FIRST], true, 0\n+- Exchange rangepartitioning(group#6 ASC NULLS FIRST, 200)\n   +- *HashAggregate(keys=[window#13], functions=[count(value#1L)])\n      +- StateStoreSave [window#13], StatefulOperatorStateInfo(&lt;unknown&gt;,736d67c2-6daa-4c4c-9c4b-c12b15af20f4,0,0), Append, 0\n         +- *HashAggregate(keys=[window#13], functions=[merge_count(value#1L)])\n            +- StateStoreRestore [window#13], StatefulOperatorStateInfo(&lt;unknown&gt;,736d67c2-6daa-4c4c-9c4b-c12b15af20f4,0,0)\n               +- *HashAggregate(keys=[window#13], functions=[merge_count(value#1L)])\n                  +- Exchange hashpartitioning(window#13, 200)\n                     +- *HashAggregate(keys=[window#13], functions=[partial_count(value#1L)])\n                        +- *Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#13, value#1L]\n                           +- *Filter isnotnull(timestamp#0)\n                              +- StreamingRelation rate, [timestamp#0, value#1L]\n\nimport org.apache.spark.sql.streaming.{OutputMode, Trigger}\nimport scala.concurrent.duration._\nval consoleOutput = counts.\n  writeStream.\n  format(\"console\").\n  option(\"truncate\", false).\n  trigger(Trigger.ProcessingTime(10.seconds)).\n  queryName(\"counts\").\n  outputMode(OutputMode.Complete).  // &lt;-- required for groupBy\n  start\n\n// Eventually...\nconsoleOutput.stop\n</code></pre>","text":"","location":"execution-planning-strategies/StatefulAggregationStrategy/#demo"},{"title":"StreamingDeduplicationStrategy Execution Planning Strategy","text":"<p><code>StreamingDeduplicationStrategy</code> is an execution planning strategy that can plan streaming queries with <code>Deduplicate</code> logical operators (over streaming queries) to StreamingDeduplicateExec physical operators.</p> <p><code>StreamingDeduplicationStrategy</code> is available using <code>SessionState</code>.</p> <pre><code>spark.sessionState.planner.StreamingDeduplicationStrategy\n</code></pre>","location":"execution-planning-strategies/StreamingDeduplicationStrategy/"},{"title":"StreamingGlobalLimitStrategy Execution Planning Strategy","text":"<p><code>StreamingGlobalLimitStrategy</code> is an execution planning strategy that can plan streaming queries with <code>ReturnAnswer</code> and <code>Limit</code> logical operators (over streaming queries) with the Append output mode to StreamingGlobalLimitExec physical operator.</p> <p><code>StreamingGlobalLimitStrategy</code> is used (and created) when IncrementalExecution is requested to plan a streaming query.</p>","location":"execution-planning-strategies/StreamingGlobalLimitStrategy/"},{"title":"Creating Instance","text":"<p><code>StreamingGlobalLimitStrategy</code> takes a single OutputMode to be created (which is the OutputMode of the IncrementalExecution).</p>","location":"execution-planning-strategies/StreamingGlobalLimitStrategy/#creating-instance"},{"title":"StreamingJoinStrategy Execution Planning Strategy \u2014 Stream-Stream Equi-Joins","text":"<p><code>StreamingJoinStrategy</code> is an execution planning strategy that can plan streaming queries with <code>Join</code> logical operators of two streaming queries to a StreamingSymmetricHashJoinExec physical operator.</p> <p><code>StreamingJoinStrategy</code> throws an <code>AnalysisException</code> when applied to a <code>Join</code> logical operator with no equality predicate:</p> <pre><code>Stream-stream join without equality predicate is not supported\n</code></pre> <p><code>StreamingJoinStrategy</code> is used when IncrementalExecution is requested to plan a streaming query.</p> <p>[[logging]] [TIP] ==== <code>StreamingJoinStrategy</code> does not print out any messages to the logs. <code>StreamingJoinStrategy</code> however uses <code>ExtractEquiJoinKeys</code> (Spark SQL) Scala extractor for destructuring <code>Join</code> logical operators that does print out DEBUG messages to the logs.</p> <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys</code> to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys=ALL\n</code></pre>","location":"execution-planning-strategies/StreamingJoinStrategy/"},{"title":"Refer to &lt;&gt;.","text":"","location":"execution-planning-strategies/StreamingJoinStrategy/#refer-to"},{"title":"StreamingRelationStrategy Execution Planning Strategy","text":"<p><code>StreamingRelationStrategy</code> is an execution planning strategy that can plan streaming queries with StreamingRelation, StreamingExecutionRelation, and StreamingRelationV2 logical operators to StreamingRelationExec physical operators.</p> <p></p> <p><code>StreamingRelationStrategy</code> is used when IncrementalExecution is requested to plan a streaming query.</p>","location":"execution-planning-strategies/StreamingRelationStrategy/"},{"title":"Accessing StreamingRelationStrategy","text":"<p><code>StreamingRelationStrategy</code> is available using <code>SessionState</code> (of a <code>SparkSession</code>).</p> <pre><code>spark.sessionState.planner.StreamingRelationStrategy\n</code></pre>","location":"execution-planning-strategies/StreamingRelationStrategy/#accessing-streamingrelationstrategy"},{"title":"Demo","text":"<pre><code>val rates = spark.\n  readStream.\n  format(\"rate\").\n  load // &lt;-- gives a streaming Dataset with a logical plan with StreamingRelation logical operator\n\n// StreamingRelation logical operator for the rate streaming source\nscala&gt; println(rates.queryExecution.logical.numberedTreeString)\n00 StreamingRelation DataSource(org.apache.spark.sql.SparkSession@31ba0af0,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L]\n\n// StreamingRelationExec physical operator (shown without \"Exec\" suffix)\nscala&gt; rates.explain\n== Physical Plan ==\nStreamingRelation rate, [timestamp#0, value#1L]\n\n// Let's do the planning manually\nimport spark.sessionState.planner.StreamingRelationStrategy\nval physicalPlan = StreamingRelationStrategy.apply(rates.queryExecution.logical).head\nscala&gt; println(physicalPlan.numberedTreeString)\n00 StreamingRelation rate, [timestamp#0, value#1L]\n</code></pre>","location":"execution-planning-strategies/StreamingRelationStrategy/#demo"},{"title":"Features","text":"","location":"features/"},{"title":"ContinuousExecutionRelation Leaf Logical Operator","text":"<p><code>ContinuousExecutionRelation</code> is a <code>MultiInstanceRelation</code> (Spark SQL) leaf logical operator.</p>  <p>Tip</p> <p>Learn more about Leaf Logical Operators in The Internals of Spark SQL book.</p>","location":"logical-operators/ContinuousExecutionRelation/"},{"title":"Creating Instance","text":"<p><code>ContinuousExecutionRelation</code> takes the following to be created:</p> <ul> <li>[[source]] ContinuousReadSupport source</li> <li>[[extraOptions]] Options (<code>Map[String, String]</code>)</li> <li>[[output]] Output attributes (<code>Seq[Attribute]</code>)</li> <li>[[session]] <code>SparkSession</code> (Spark SQL)</li> </ul> <p><code>ContinuousExecutionRelation</code> is created (to represent StreamingRelationV2 with ContinuousReadSupport data source) when <code>ContinuousExecution</code> is created (and requested for the logical plan).</p>","location":"logical-operators/ContinuousExecutionRelation/#creating-instance"},{"title":"Deduplicate Unary Logical Operator","text":"<p><code>Deduplicate</code> is a unary logical operator that represents dropDuplicates operator.</p> <p><code>Deduplicate</code> has &lt;&gt; flag enabled for streaming Datasets. <pre><code>val uniqueRates = spark.\n  readStream.\n  format(\"rate\").\n  load.\n  dropDuplicates(\"value\")  // &lt;-- creates Deduplicate logical operator\n// Note the streaming flag\nscala&gt; println(uniqueRates.queryExecution.logical.numberedTreeString)\n00 Deduplicate [value#33L], true  // &lt;-- streaming flag enabled\n01 +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4785f176,rate,List(),None,List(),None,Map(),None), rate, [timestamp#32, value#33L]\n</code></pre> <p>CAUTION: FIXME Example with duplicates across batches to show that <code>Deduplicate</code> keeps state and withWatermark operator should also be used to limit how much is stored (to not cause OOM)</p>  <p>Note</p> <p><code>UnsupportedOperationChecker</code> ensures that dropDuplicates operator is not used after aggregation on streaming Datasets.</p> <p>The following code is not supported in Structured Streaming and results in an <code>AnalysisException</code>.</p> <pre><code>val counts = spark.\n  readStream.\n  format(\"rate\").\n  load.\n  groupBy(window($\"timestamp\", \"5 seconds\") as \"group\").\n  agg(count(\"value\") as \"value_count\").\n  dropDuplicates  // &lt;-- after groupBy\n\nimport scala.concurrent.duration._\nimport org.apache.spark.sql.streaming.{OutputMode, Trigger}\nval sq = counts.\n  writeStream.\n  format(\"console\").\n  trigger(Trigger.ProcessingTime(10.seconds)).\n  outputMode(OutputMode.Complete).\n  start\norg.apache.spark.sql.AnalysisException: dropDuplicates is not supported after aggregation on a streaming DataFrame/Dataset;;\n</code></pre>   <p>Note</p> <p><code>Deduplicate</code> logical operator is translated (planned) to:</p> <ul> <li> <p>StreamingDeduplicateExec physical operator in StreamingDeduplicationStrategy execution planning strategy for streaming Datasets (aka streaming plans)</p> </li> <li> <p><code>Aggregate</code> physical operator in <code>ReplaceDeduplicateWithAggregate</code> execution planning strategy for non-streaming/batch Datasets (batch plans)</p> </li> </ul>  <p>[[output]] The output schema of <code>Deduplicate</code> is exactly the &lt;&gt;'s output schema.","location":"logical-operators/Deduplicate/"},{"title":"Creating Instance","text":"<p><code>Deduplicate</code> takes the following when created:</p> <ul> <li>[[keys]] Attributes for keys</li> <li>[[child]] Child logical operator (i.e. <code>LogicalPlan</code>)</li> <li>[[streaming]] Flag whether the logical operator is for streaming (enabled) or batch (disabled) mode</li> </ul>","location":"logical-operators/Deduplicate/#creating-instance"},{"title":"EventTimeWatermark Logical Operator","text":"<p><code>EventTimeWatermark</code> is a unary logical operator (Spark SQL) that represents Dataset.withWatermark operator (in a logical query plan of a streaming query).</p> <p><code>EventTimeWatermark</code> marks a user-specified column as holding the event time for a row.</p>","location":"logical-operators/EventTimeWatermark/"},{"title":"Creating Instance","text":"<p><code>EventTimeWatermark</code> takes the following to be created:</p> <ul> <li>User-Defined Event-Time Watermark Column</li> <li> Watermark Delay Threshold <li> Child <code>LogicalPlan</code> (Spark SQL)  <p><code>EventTimeWatermark</code> is created when:</p> <ul> <li>Dataset.withWatermark operator is used</li> </ul>","location":"logical-operators/EventTimeWatermark/#creating-instance"},{"title":"User-Defined Event-Time Watermark Column <p><code>EventTimeWatermark</code> is given an event-time <code>Attribute</code> (Spark SQL) when created.</p> <p>The <code>Attribute</code> is an <code>UnresolvedAttribute</code> from Dataset.withWatermark operator (that is the target of and can immediately be removed using EliminateEventTimeWatermark logical optimization).</p> <p>The event time column has to be defined on a window or a timestamp and so the data type of an event time column can be as follows:</p> <ul> <li><code>StructType</code> with <code>end</code> field of <code>TimestampType</code> type (for windowed aggregation)</li> <li><code>TimestampType</code></li> </ul> <p><code>EventTimeWatermark</code> is used when:</p> <ul> <li><code>ProgressReporter</code> is requested to extract execution statistics (that adds <code>watermark</code> entry with the batchWatermarkMs of the OffsetSeqMetadata of a micro-batch)</li> </ul>","text":"","location":"logical-operators/EventTimeWatermark/#user-defined-event-time-watermark-column"},{"title":"Logical Optimizations","text":"","location":"logical-operators/EventTimeWatermark/#logical-optimizations"},{"title":"EliminateEventTimeWatermark <p><code>EliminateEventTimeWatermark</code> logical optimization removes <code>EventTimeWatermark</code> logical operator from a logical plan if the child logical operator is not streaming (i.e., when Dataset.withWatermark operator is used in a batch query).</p> <pre><code>val logs = spark.\n  read. // &lt;-- batch non-streaming query that makes `EliminateEventTimeWatermark` rule applicable\n  format(\"text\").\n  load(\"logs\")\n\n// logs is a batch Dataset\nassert(!logs.isStreaming)\n\nval q = logs.\n  withWatermark(eventTime = \"timestamp\", delayThreshold = \"30 seconds\") // &lt;-- creates EventTimeWatermark\n</code></pre> <pre><code>scala&gt; println(q.queryExecution.logical.numberedTreeString) // &lt;-- no EventTimeWatermark as it was removed immediately\n00 Relation[value#0] text\n</code></pre>","text":"","location":"logical-operators/EventTimeWatermark/#eliminateeventtimewatermark"},{"title":"PushPredicateThroughNonJoin <p><code>PushPredicateThroughNonJoin</code> can optimize streaming queries with <code>EventTimeWatermark</code> (and push predicates down if they don't reference the eventTime column).</p>","text":"","location":"logical-operators/EventTimeWatermark/#pushpredicatethroughnonjoin"},{"title":"Execution Planning","text":"<p><code>EventTimeWatermark</code> is planned as EventTimeWatermarkExec physical operator by StatefulAggregationStrategy execution planning strategy.</p>","location":"logical-operators/EventTimeWatermark/#execution-planning"},{"title":"Output Schema <pre><code>output: Seq[Attribute]\n</code></pre> <p><code>output</code> is part of the <code>QueryPlan</code> (Spark SQL) abstraction.</p>  <p>When requested for the output attributes, <code>EventTimeWatermark</code> logical operator scans the output attributes of the child logical operator to find the matching attribute based on the eventTime attribute and adds <code>spark.watermarkDelayMs</code> metadata key with the watermark delay interval (converted to milliseconds).</p>  <p><code>output</code> finds the eventTime column in the output schema of the child logical operator and updates the <code>Metadata</code> of the column with spark.watermarkDelayMs key and the milliseconds for the watermark delay.</p> <p><code>output</code> removes spark.watermarkDelayMs key from the other columns (if there is any)</p> <pre><code>// FIXME How to access/show the eventTime column with the metadata updated to include spark.watermarkDelayMs?\nimport org.apache.spark.sql.catalyst.plans.logical.EventTimeWatermark\nval etw = q.queryExecution.logical.asInstanceOf[EventTimeWatermark]\nscala&gt; etw.output.toStructType.printTreeString\nroot\n |-- timestamp: timestamp (nullable = true)\n |-- value: long (nullable = true)\n</code></pre>","text":"","location":"logical-operators/EventTimeWatermark/#output-schema"},{"title":"Watermark Delay Metadata Marker <p><code>spark.watermarkDelayMs</code> metadata key is used to mark one of the output attributes as the watermark attribute (eventTime watermark).</p>","text":"","location":"logical-operators/EventTimeWatermark/#watermark-delay-metadata-marker"},{"title":"FlatMapGroupsWithState Unary Logical Operator","text":"<p><code>FlatMapGroupsWithState</code> is a unary logical operator (Spark SQL) that represents the following operators in a logical query plan of a streaming query:</p> <ul> <li> <p>KeyValueGroupedDataset.mapGroupsWithState</p> </li> <li> <p>KeyValueGroupedDataset.flatMapGroupsWithState</p> </li> </ul>","location":"logical-operators/FlatMapGroupsWithState/"},{"title":"Execution Planning","text":"<p><code>FlatMapGroupsWithState</code> is resolved (planned) to:</p> <ul> <li> <p>FlatMapGroupsWithStateExec unary physical operator for streaming datasets (in FlatMapGroupsWithStateStrategy execution planning strategy)</p> </li> <li> <p><code>MapGroupsExec</code> physical operator for batch datasets (in <code>BasicOperators</code> execution planning strategy)</p> </li> </ul>","location":"logical-operators/FlatMapGroupsWithState/#execution-planning"},{"title":"Creating Instance","text":"<p><code>FlatMapGroupsWithState</code> takes the following to be created:</p> <ul> <li> State function (<code>(Any, Iterator[Any], LogicalGroupState[Any]) =&gt; Iterator[Any]</code>) <li> Catalyst Expression for keys <li> Catalyst Expression for values <li> Grouping Attributes <li> Data Attributes <li> Output Object Attribute <li> State <code>ExpressionEncoder</code> <li> OutputMode <li> <code>isMapGroupsWithState</code> flag (default: <code>false</code>) <li> GroupStateTimeout <li> Child logical operator  <p><code>FlatMapGroupsWithState</code> is created (using apply factory method) for KeyValueGroupedDataset.mapGroupsWithState and KeyValueGroupedDataset.flatMapGroupsWithState operators.</p>","location":"logical-operators/FlatMapGroupsWithState/#creating-instance"},{"title":"Creating SerializeFromObject with FlatMapGroupsWithState <pre><code>apply[K: Encoder, V: Encoder, S: Encoder, U: Encoder](\n  func: (Any, Iterator[Any], LogicalGroupState[Any]) =&gt; Iterator[Any],\n  groupingAttributes: Seq[Attribute],\n  dataAttributes: Seq[Attribute],\n  outputMode: OutputMode,\n  isMapGroupsWithState: Boolean,\n  timeout: GroupStateTimeout,\n  child: LogicalPlan): LogicalPlan\n</code></pre> <p><code>apply</code> creates a <code>SerializeFromObject</code> logical operator with a <code>FlatMapGroupsWithState</code> as its child logical operator.</p> <p>Internally, <code>apply</code> creates <code>SerializeFromObject</code> object consumer (aka unary logical operator) with <code>FlatMapGroupsWithState</code> logical plan.</p> <p>Internally, <code>apply</code> finds <code>ExpressionEncoder</code> for the type <code>S</code> and creates a <code>FlatMapGroupsWithState</code> with <code>UnresolvedDeserializer</code> for the types <code>K</code> and <code>V</code>.</p> <p>In the end, <code>apply</code> creates a <code>SerializeFromObject</code> object consumer with the <code>FlatMapGroupsWithState</code>.</p> <p><code>apply</code> is used for KeyValueGroupedDataset.mapGroupsWithState and KeyValueGroupedDataset.flatMapGroupsWithState operators.</p>","text":"","location":"logical-operators/FlatMapGroupsWithState/#creating-serializefromobject-with-flatmapgroupswithstate"},{"title":"StreamingDataSourceV2Relation Logical Operator","text":"<p><code>StreamingDataSourceV2Relation</code> is a leaf logical operator that represents StreamingRelationV2 logical operator (with tables with a <code>SupportsRead</code> and <code>MICRO_BATCH_READ</code> or <code>CONTINUOUS_READ</code> capabilities) at execution time.</p>  <p>Tip</p> <p>Learn more about Leaf Logical Operators, SupportsRead and Table Capabilities in The Internals of Spark SQL online book.</p>","location":"logical-operators/StreamingDataSourceV2Relation/"},{"title":"Creating Instance","text":"<p><code>StreamingDataSourceV2Relation</code> takes the following to be created:</p> <ul> <li> Output Attributes (Spark SQL) <li> <code>Scan</code> (Spark SQL) <li> SparkDataStream <li> Start Offset (default: undefined) <li> End Offset (default: undefined)  <p><code>StreamingDataSourceV2Relation</code> is created\u00a0when:</p> <ul> <li> <p><code>MicroBatchExecution</code> stream execution engine is requested for an analyzed logical query plan (for StreamingRelationV2 with a <code>SupportsRead</code> table with <code>MICRO_BATCH_READ</code> capability)</p> </li> <li> <p><code>ContinuousExecution</code> stream execution engine is requested for an analyzed logical query plan (for StreamingRelationV2 with a <code>SupportsRead</code> table with <code>CONTINUOUS_READ</code> capability)</p> </li> </ul>","location":"logical-operators/StreamingDataSourceV2Relation/#creating-instance"},{"title":"Computing Stats <pre><code>computeStats(): Statistics\n</code></pre> <p>For Scans with <code>SupportsReportStatistics</code>, <code>computeStats</code> requests the scan to <code>estimateStatistics</code>.</p>  <p>Tip</p> <p>Learn more about Scan and SupportsReportStatistics in The Internals of Spark SQL online book.</p>  <p>For other types of scans, <code>computeStats</code> simply assumes the default size and no row count.</p> <p><code>computeStats</code>\u00a0is part of the <code>LeafNode</code> abstraction.</p>","text":"","location":"logical-operators/StreamingDataSourceV2Relation/#computing-stats"},{"title":"StreamingExecutionRelation Leaf Logical Operator","text":"<p><code>StreamingExecutionRelation</code> is a leaf logical operator (Spark SQL) that represents a streaming source in the logical query plan of a streaming query.</p> <p>The main use of <code>StreamingExecutionRelation</code> logical operator is to be a \"placeholder\" in a logical query plan that will be replaced with the real relation (with new data that has arrived since the last batch) or an empty <code>LocalRelation</code> when <code>StreamExecution</code> is requested to transforming logical plan to include the Sources and MicroBatchReaders with new data.</p>  <p>Note</p> <p>Right after <code>StreamExecution</code> has started running streaming batches it initializes the streaming sources by transforming the analyzed logical plan of the streaming query so that every StreamingRelation logical operator is replaced by the corresponding <code>StreamingExecutionRelation</code>.</p>  <p></p>  <p>Note</p> <p><code>StreamingExecutionRelation</code> is also resolved (planned) to a StreamingRelationExec physical operator in StreamingRelationStrategy execution planning strategy only when explaining a streaming <code>Dataset</code>.</p>","location":"logical-operators/StreamingExecutionRelation/"},{"title":"Creating Instance","text":"<p><code>StreamingExecutionRelation</code> takes the following to be created:</p> <ul> <li> <code>BaseStreamingSource</code> <li> Output Attributes (<code>Seq[Attribute]</code>) <li> <code>SparkSession</code>  <p><code>StreamingExecutionRelation</code> is created\u00a0when:</p> <ul> <li><code>MicroBatchExecution</code> stream execution engine is requested for the analyzed logical query plan (for every StreamingRelation)</li> </ul>","location":"logical-operators/StreamingExecutionRelation/#creating-instance"},{"title":"StreamingRelation Leaf Logical Operator","text":"<p><code>StreamingRelation</code> is a leaf logical operator (Spark SQL) that represents a streaming source in a logical plan.</p> <p><code>StreamingRelation</code> is resolved (planned) to a StreamingExecutionRelation (right after <code>StreamExecution</code> starts running batches).</p>","location":"logical-operators/StreamingRelation/"},{"title":"Creating Instance","text":"<p><code>StreamingRelation</code> takes the following to be created:</p> <ul> <li> DataSource <li> Short Name of the Streaming Source <li> Output Attributes (<code>Seq[Attribute]</code>)  <p><code>StreamingRelation</code> is created when:</p> <ul> <li><code>DataStreamReader</code> is requested to load data from a streaming source and creates a streaming query.</li> </ul> <p></p>","location":"logical-operators/StreamingRelation/#creating-instance"},{"title":"Creating StreamingRelation for DataSource <pre><code>apply(\n  dataSource: DataSource): StreamingRelation\n</code></pre> <p><code>apply</code> creates a <code>StreamingRelation</code> for the given DataSource.</p> <p><code>apply</code> is used when:</p> <ul> <li><code>DataStreamReader</code> is requested for a streaming query</li> </ul>","text":"","location":"logical-operators/StreamingRelation/#creating-streamingrelation-for-datasource"},{"title":"isStreaming <pre><code>isStreaming: Boolean\n</code></pre> <p><code>isStreaming</code> is part of the <code>LogicalPlan</code> (Spark SQL) abstraction.</p> <p><code>isStreaming</code> flag is always <code>true</code>.</p> <pre><code>import org.apache.spark.sql.execution.streaming.StreamingRelation\nval relation = rate.queryExecution.logical.asInstanceOf[StreamingRelation]\nassert(relation.isStreaming)\n</code></pre>","text":"","location":"logical-operators/StreamingRelation/#isstreaming"},{"title":"Text Respresentation <pre><code>toString: String\n</code></pre> <p><code>toString</code> gives the source name.</p>","text":"","location":"logical-operators/StreamingRelation/#text-respresentation"},{"title":"Demo <pre><code>val rate = spark.\n  readStream.\n  format(\"rate\").\n  load(\"hello\")\n</code></pre> <pre><code>scala&gt; println(rate.queryExecution.logical.numberedTreeString)\n00 StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@69ab1abc, rate, Map(path -&gt; hello), [timestamp#0, value#1L]\n</code></pre>","text":"","location":"logical-operators/StreamingRelation/#demo"},{"title":"StreamingRelationV2 Leaf Logical Operator","text":"<p><code>StreamingRelationV2</code> is a leaf logical operator that represents <code>SupportsRead</code> streaming tables (with <code>MICRO_BATCH_READ</code> or <code>CONTINUOUS_READ</code> capabilities) in a logical plan of a streaming query.</p>  <p>Tip</p> <p>Learn more about Leaf Logical Operators, SupportsRead and Table Capabilities in The Internals of Spark SQL online book.</p>","location":"logical-operators/StreamingRelationV2/"},{"title":"Creating Instance","text":"<p><code>StreamingRelationV2</code> takes the following to be created:</p> <ul> <li> <code>TableProvider</code> (Spark SQL) <li> Source Name <li> <code>Table</code> (Spark SQL) <li> Extra Options <li> Output Attributes (Spark SQL) <li> StreamingRelation <li> <code>SparkSession</code> (Spark SQL)  <p><code>StreamingRelationV2</code> is created when:</p> <ul> <li><code>DataStreamReader</code> is reqested to load data (for a <code>SupportsRead</code> table with <code>MICRO_BATCH_READ</code> or <code>CONTINUOUS_READ</code> capabilities)</li> <li><code>MemoryStreamBase</code> is requested for a logical query plan</li> </ul>","location":"logical-operators/StreamingRelationV2/#creating-instance"},{"title":"Logical Resolution","text":"<p><code>StreamingRelationV2</code> is resolved to the following leaf logical operators:</p> <ul> <li>StreamingDataSourceV2Relation or StreamingExecutionRelation when <code>MicroBatchExecution</code> stream execution engine is requested for an analyzed logical plan</li> <li>StreamingDataSourceV2Relation when <code>ContinuousExecution</code> stream execution engine is created (and initializes an analyzed logical plan)</li> </ul>","location":"logical-operators/StreamingRelationV2/#logical-resolution"},{"title":"WriteToContinuousDataSource Unary Logical Operator","text":"<p><code>WriteToContinuousDataSource</code> is a unary logical operator (<code>LogicalPlan</code>) that is created when <code>ContinuousExecution</code> is requested to run a streaming query in continuous mode (to create an IncrementalExecution).</p> <p><code>WriteToContinuousDataSource</code> is planned (translated) to a WriteToContinuousDataSourceExec unary physical operator (when <code>DataSourceV2Strategy</code> execution planning strategy is requested to plan a logical query).</p> <p>[[output]] <code>WriteToContinuousDataSource</code> uses empty output schema (which is exactly to say that no output is expected whatsoever).</p>","location":"logical-operators/WriteToContinuousDataSource/"},{"title":"Creating Instance","text":"<p><code>WriteToContinuousDataSource</code> takes the following to be created:</p> <ul> <li>[[query]] Child logical operator (<code>LogicalPlan</code>)</li> </ul>","location":"logical-operators/WriteToContinuousDataSource/#creating-instance"},{"title":"WriteToDataSourceV2 Logical Operator","text":"<p><code>WriteToDataSourceV2</code> is a unary logical operator (Spark SQL) that represents WriteToMicroBatchDataSource unary logical operator at logical optimization time for writing data in Spark Structured Streaming.</p>  <p>Deprecated</p> <p><code>WriteToDataSourceV2</code> is deprecated since Spark SQL 2.4.0 (in favour of <code>AppendData</code> logical operator and alike).</p>","location":"logical-operators/WriteToDataSourceV2/"},{"title":"Creating Instance","text":"<p><code>WriteToDataSourceV2</code> takes the following to be created:</p> <ul> <li> <code>DataSourceV2Relation</code> (Spark SQL) <li> <code>BatchWrite</code> (Spark SQL) <li> Logical Query Plan (Spark SQL) <li> Write <code>CustomMetric</code>s (Spark SQL)  <p><code>WriteToDataSourceV2</code> is created when:</p> <ul> <li><code>V2Writes</code> (Spark SQL) logical optimization is requested to optimize a logical query (with a WriteToMicroBatchDataSource unary logical operator)</li> </ul>","location":"logical-operators/WriteToDataSourceV2/#creating-instance"},{"title":"Query Planning","text":"<p><code>WriteToDataSourceV2</code> is planned as WriteToDataSourceV2Exec physical operator by <code>DataSourceV2Strategy</code> (Spark SQL) execution planning strategy.</p>","location":"logical-operators/WriteToDataSourceV2/#query-planning"},{"title":"WriteToMicroBatchDataSource Logical Operator","text":"<p><code>WriteToMicroBatchDataSource</code> is a unary logical operator (Spark SQL) that is a top-level operator in the analyzed logical query plan of a streaming query with a sink that <code>SupportsWrite</code> (Spark SQL).</p>","location":"logical-operators/WriteToMicroBatchDataSource/"},{"title":"Creating Instance","text":"<p><code>WriteToMicroBatchDataSource</code> takes the following to be created:</p> <ul> <li> <code>DataSourceV2Relation</code> (Spark SQL) leaf logical operator <li> Table with <code>SupportsWrite</code> (Spark SQL) <li> Query <code>LogicalPlan</code> (Spark SQL) <li> Query ID <li> Write Options (<code>Map[String, String]</code>) <li> OutputMode <li> Batch ID  <p><code>WriteToMicroBatchDataSource</code> is created when:</p> <ul> <li><code>MicroBatchExecution</code> is requested for the analyzed logical query plan (with a sink that <code>SupportsWrite</code> (Spark SQL))</li> </ul>","location":"logical-operators/WriteToMicroBatchDataSource/#creating-instance"},{"title":"Query Planning","text":"<p><code>WriteToMicroBatchDataSource</code> is optimized to WriteToDataSourceV2 logical operator by <code>V2Writes</code> (Spark SQL) logical optimization.</p>","location":"logical-operators/WriteToMicroBatchDataSource/#query-planning"},{"title":"WriteToStream","text":"<p><code>WriteToStream</code> is...FIXME</p>","location":"logical-operators/WriteToStream/"},{"title":"Micro-Batch Stream Processing","text":"<p>Micro-Batch Stream Processing is a stream processing model in Spark Structured Streaming that is used for streaming queries with Trigger.Once and Trigger.ProcessingTime triggers.</p> <p>Micro-batch stream processing uses MicroBatchExecution stream execution engine.</p> <p>Micro-batch stream processing supports MicroBatchStream data sources.</p> <p>Micro-batch stream processing is often referred to as Structured Streaming V1.</p>","location":"micro-batch-execution/"},{"title":"Execution Phases","text":"<p>When MicroBatchExecution stream processing engine is requested to run an activated streaming query, the query execution goes through the following execution phases every trigger (micro-batch):</p> <ol> <li>triggerExecution</li> <li>getOffset for Sources or setOffsetRange for...FIXME</li> <li>getEndOffset</li> <li>walCommit</li> <li>getBatch</li> <li>queryPlanning</li> <li>addBatch</li> </ol> <p>Execution phases with execution times are available using StreamingQueryProgress under <code>durationMs</code>.</p> <pre><code>scala&gt; :type sq\norg.apache.spark.sql.streaming.StreamingQuery\nsq.lastProgress.durationMs.get(\"walCommit\")\n</code></pre>  <p>Tip</p> <p>Enable INFO logging level for StreamExecution logger to be notified about durations.</p>  <pre><code>17/08/11 09:04:17 INFO StreamExecution: Streaming query made progress: {\n  \"id\" : \"ec8f8228-90f6-4e1f-8ad2-80222affed63\",\n  \"runId\" : \"f605c134-cfb0-4378-88c1-159d8a7c232e\",\n  \"name\" : \"rates-to-console\",\n  \"timestamp\" : \"2017-08-11T07:04:17.373Z\",\n  \"batchId\" : 0,\n  \"numInputRows\" : 0,\n  \"processedRowsPerSecond\" : 0.0,\n  \"durationMs\" : {          // &lt;-- Durations (in millis)\n    \"addBatch\" : 38,\n    \"getBatch\" : 1,\n    \"getOffset\" : 0,\n    \"queryPlanning\" : 1,\n    \"triggerExecution\" : 62,\n    \"walCommit\" : 19\n  },\n</code></pre>","location":"micro-batch-execution/#execution-phases"},{"title":"Monitoring","text":"<p><code>MicroBatchExecution</code> posts events to announce when a streaming query is started and stopped as well as after every micro-batch. StreamingQueryListener interface can be used to intercept the events and act accordingly.</p> <p>After <code>triggerExecution</code> phase <code>MicroBatchExecution</code> is requested to finish up a streaming batch (trigger) and generate a StreamingQueryProgress (with execution statistics).</p> <p><code>MicroBatchExecution</code> prints out the following DEBUG message to the logs:</p> <pre><code>Execution stats: [executionStats]\n</code></pre> <p><code>MicroBatchExecution</code> posts a QueryProgressEvent with the StreamingQueryProgress and prints out the following INFO message to the logs:</p> <pre><code>Streaming query made progress: [newProgress]\n</code></pre>","location":"micro-batch-execution/#monitoring"},{"title":"Demo","text":"<pre><code>import org.apache.spark.sql.streaming.Trigger\nimport scala.concurrent.duration._\nval sq = spark\n  .readStream\n  .format(\"rate\")\n  .load\n  .writeStream\n  .format(\"console\")\n  .option(\"truncate\", false)\n  .trigger(Trigger.ProcessingTime(1.minute)) // &lt;-- Uses MicroBatchExecution for execution\n  .queryName(\"rate2console\")\n  .start\nassert(sq.isActive)\n</code></pre> <pre><code>scala&gt; sq.explain\n== Physical Plan ==\nWriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@42363db7\n+- *(1) Project [timestamp#54, value#55L]\n   +- *(1) ScanV2 rate[timestamp#54, value#55L]\n</code></pre> <pre><code>sq.stop\n</code></pre>","location":"micro-batch-execution/#demo"},{"title":"MicroBatchExecution","text":"<p><code>MicroBatchExecution</code> is a stream execution engine for Micro-Batch Stream Processing.</p>  <p></p>  <p>Once created, <code>MicroBatchExecution</code> is requested to start.</p>","location":"micro-batch-execution/MicroBatchExecution/"},{"title":"Creating Instance","text":"<p><code>MicroBatchExecution</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (Spark SQL) <li> Trigger <li> <code>Clock</code> <li> Extra Options (<code>Map[String, String]</code>) <li> WriteToStream unary logical plan  <p><code>MicroBatchExecution</code> is created when:</p> <ul> <li><code>StreamingQueryManager</code> is requested to create a streaming query (when <code>DataStreamWriter</code> is requested to start execution of the streaming query) for all triggers but ContinuousTrigger</li> </ul>","location":"micro-batch-execution/MicroBatchExecution/#creating-instance"},{"title":"Initializing Query Progress for New Trigger <pre><code>startTrigger(): Unit\n</code></pre> <p><code>startTrigger</code>\u00a0is part of the ProgressReporter abstraction.</p> <p><code>startTrigger</code>...FIXME</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#initializing-query-progress-for-new-trigger"},{"title":"SparkDataStreams <pre><code>sources: Seq[SparkDataStream]\n</code></pre> <p><code>sources</code> is part of the ProgressReporter abstraction.</p>  <p>Streaming sources and readers (of the StreamingExecutionRelations of the analyzed logical query plan of the streaming query)</p> <p>Default: (empty)</p> <p>Initialized when <code>MicroBatchExecution</code> is requested for the transformed logical query plan</p> <p>Used when:</p> <ul> <li>Populating start offsets (for the available and committed offsets)</li> <li>Constructing or skipping next streaming micro-batch (and persisting offsets to write-ahead log)</li> </ul>","text":"","location":"micro-batch-execution/MicroBatchExecution/#sparkdatastreams"},{"title":"TriggerExecutor <pre><code>triggerExecutor: TriggerExecutor\n</code></pre> <p><code>MicroBatchExecution</code> uses a TriggerExecutor to execute micro-batches and to determine the unique streaming sources.</p> <p><code>triggerExecutor</code> is initialized when <code>MicroBatchExecution</code> is created (based on the given Trigger):</p> <ul> <li>ProcessingTimeExecutor for ProcessingTimeTrigger</li> <li>SingleBatchExecutor for OneTimeTrigger</li> <li>MultiBatchExecutor for AvailableNowTrigger</li> </ul>  <p><code>triggerExecutor</code> throws an <code>IllegalStateException</code> when the Trigger is not one of the built-in implementations.</p> <pre><code>Unknown type of trigger: [trigger]\n</code></pre>","text":"","location":"micro-batch-execution/MicroBatchExecution/#triggerexecutor"},{"title":"Running Activated Streaming Query <pre><code>runActivatedStream(\n  sparkSessionForStream: SparkSession): Unit\n</code></pre> <p><code>runActivatedStream</code> is part of StreamExecution abstraction.</p>  <p><code>runActivatedStream</code> requests the TriggerExecutor to execute micro-batches using the batch runner.</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#running-activated-streaming-query"},{"title":"Batch Runner <p>The batch runner checks whether the streaming query is active or not (and hence terminated).</p> <p>When terminated, the batch runner is waiting for next trigger.</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#batch-runner"},{"title":"startTrigger <p>When active, the batch runner initializes query progress for the new trigger (aka startTrigger).</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#starttrigger"},{"title":"triggerExecution Execution Phase <p>The batch runner starts triggerExecution execution phase.</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#triggerexecution-execution-phase"},{"title":"populateStartOffsets <p>This phase happens only at the start or restart (resume) of a streaming query (when the current batch ID is uninitialized and <code>-1</code>).</p> <p>The batch runner requests the OffsetSeqLog for the latest batch ID and sets the latest seen offset on the SparkDataStreams.</p> <p>The batch runner populates start offsets from checkpoint and prints out the following INFO message to the logs (with the committedOffsets):</p> <pre><code>Stream started from [committedOffsets]\n</code></pre>","text":"","location":"micro-batch-execution/MicroBatchExecution/#populatestartoffsets"},{"title":"getBatchDescriptionString <p>The batch runner sets the human-readable description for any Spark job submitted as part of this micro-batch as the batch description (using SparkContext.setJobDescription).</p> <p>A Spark job can be submitted while streaming sources are pulling new data or as part of a sink (e.g., DataStreamWriter.foreachBatch operator).</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#getbatchdescriptionstring"},{"title":"isCurrentBatchConstructed <p>Unless already constructed, the batch runner constructs the next streaming micro-batch with the value of spark.sql.streaming.noDataMicroBatches.enabled configuration property.</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#iscurrentbatchconstructed"},{"title":"Recording Offsets <p>The batch runner records the trigger offset range for progress reporting (with the committed, available and latestOffsets offsets).</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#recording-offsets"},{"title":"isNewDataAvailable <p>The batch runner remembers whether the current batch has data or not and updates the StreamingQueryStatus (as <code>isDataAvailable</code> property).</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#isnewdataavailable"},{"title":"Running Micro-Batch <p>With the streaming micro-batch constructed, the batch runner updates the status message to one of the following (based on whether the current batch has data or not):</p> <pre><code>Processing new data\n</code></pre> <pre><code>No new data but cleaning up state\n</code></pre> <p>The batch runner runs the streaming micro-batch.</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#running-micro-batch"},{"title":"Waiting for data to arrive <p>Otherwise, the batch runner updates the status message to the following:</p> <pre><code>Waiting for data to arrive\n</code></pre>","text":"","location":"micro-batch-execution/MicroBatchExecution/#waiting-for-data-to-arrive"},{"title":"finishTrigger <p>The batch runner finalizes query progress for the trigger (with the flags that indicate whether the current batch had new data and the isCurrentBatchConstructed).</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#finishtrigger"},{"title":"Closing Up <p>At the final step of runActivatedStream when the isActive was enabled, the batch runner does some closing-up work.</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#closing-up"},{"title":"isCurrentBatchConstructed <p>When the isCurrentBatchConstructed is turned on (<code>true</code>), the batch runner increments the currentBatchId and turns the isCurrentBatchConstructed flag off (<code>false</code>).</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#iscurrentbatchconstructed_1"},{"title":"MultiBatchExecutor <p>For MultiBatchExecutor, the batch runner prints out the following INFO message to the logs:</p> <pre><code>Finished processing all available data for the trigger,\nterminating this Trigger.AvailableNow query\n</code></pre> <p>The batch runner sets the state to TERMINATED.</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#multibatchexecutor"},{"title":"Otherwise <p>With the isCurrentBatchConstructed flag disabled (<code>false</code>) and the non-MultiBatchExecutor triggerExecutor, the batch runner sleeps (as long as configured using the spark.sql.streaming.pollingDelay configuration property).</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#otherwise"},{"title":"Waiting for next trigger <p>When inactive, the batch runner updates the status message to the following:</p> <pre><code>Waiting for next trigger\n</code></pre> <p>In the end, the batch runner returns whether the streaming query is active or not.</p>  <p>Note</p> <p>The state of the streaming query (i.e., whether the streaming query is active or not) can change while a micro-batch is executed (e.g., for MultiBatchExecutor when no next batch was constructed).</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#waiting-for-next-trigger"},{"title":"Populating Start Offsets From Checkpoint (Resuming from Checkpoint) <pre><code>populateStartOffsets(\n  sparkSessionToRunBatches: SparkSession): Unit\n</code></pre> <p><code>populateStartOffsets</code> requests the Offset Write-Ahead Log for the latest committed batch id with metadata (i.e. OffsetSeq).</p>  <p>Note</p> <p>The batch id could not be available in the write-ahead log when a streaming query started with a new log or no batch was persisted (added) to the log before.</p>  <p><code>populateStartOffsets</code> branches off based on whether the latest committed batch was available or not.</p> <p><code>populateStartOffsets</code> is used when <code>MicroBatchExecution</code> is requested to run an activated streaming query (before the first \"zero\" micro-batch).</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#populating-start-offsets-from-checkpoint-resuming-from-checkpoint"},{"title":"Latest Committed Batch Available <p>When the latest committed batch id with the metadata was available in the Offset Write-Ahead Log, <code>populateStartOffsets</code> (re)initializes the internal state as follows:</p> <ul> <li> <p>Sets the current batch ID to the latest committed batch ID found</p> </li> <li> <p>Turns the isCurrentBatchConstructed internal flag on (<code>true</code>)</p> </li> <li> <p>Sets the available offsets to the offsets (from the metadata)</p> </li> </ul> <p>When the latest batch ID found is greater than <code>0</code>, <code>populateStartOffsets</code> requests the Offset Write-Ahead Log for the second latest batch ID with metadata or throws an <code>IllegalStateException</code> if not found.</p> <pre><code>batch [latestBatchId - 1] doesn't exist\n</code></pre> <p><code>populateStartOffsets</code> sets the committed offsets to the second latest committed offsets.</p> <p> <code>populateStartOffsets</code> updates the offset metadata. <p>CAUTION: FIXME Describe me</p> <p><code>populateStartOffsets</code> requests the Offset Commit Log for the latest committed batch id with metadata.</p> <p>CAUTION: FIXME Describe me</p> <p>When the latest committed batch id with metadata was found which is exactly the latest batch ID (found in the Offset Commit Log), <code>populateStartOffsets</code>...FIXME</p> <p>When the latest committed batch id with metadata was found, but it is not exactly the second latest batch ID (found in the Offset Commit Log), <code>populateStartOffsets</code> prints out the following WARN message to the logs:</p> <pre><code>Batch completion log latest batch id is [latestCommittedBatchId], which is not trailing batchid [latestBatchId] by one\n</code></pre> <p>When no commit log present in the Offset Commit Log, <code>populateStartOffsets</code> prints out the following INFO message to the logs:</p> <pre><code>no commit log present\n</code></pre> <p>In the end, <code>populateStartOffsets</code> prints out the following DEBUG message to the logs:</p> <pre><code>Resuming at batch [currentBatchId] with committed offsets [committedOffsets] and available offsets [availableOffsets]\n</code></pre>","text":"","location":"micro-batch-execution/MicroBatchExecution/#latest-committed-batch-available"},{"title":"No Latest Committed Batch <p>When the latest committed batch id with the metadata could not be found in the Offset Write-Ahead Log, it is assumed that the streaming query is started for the very first time (or the checkpoint location has changed).</p> <p><code>populateStartOffsets</code> prints out the following INFO message to the logs:</p> <pre><code>Starting new streaming query.\n</code></pre> <p>[[populateStartOffsets-currentBatchId-0]] <code>populateStartOffsets</code> sets the current batch ID to <code>0</code> and creates a new WatermarkTracker.</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#no-latest-committed-batch"},{"title":"Constructing Or Skipping Next Streaming Micro-Batch <pre><code>constructNextBatch(\n  noDataBatchesEnabled: Boolean): Boolean\n</code></pre> <p><code>constructNextBatch</code> is used when <code>MicroBatchExecution</code> is requested to run the activated streaming query.</p>  <p>Note</p> <p><code>constructNextBatch</code> is only executed when the isCurrentBatchConstructed internal flag is enabled (<code>true</code>).</p>  <p><code>constructNextBatch</code> performs the following steps:</p> <ol> <li> <p>Requesting the latest offsets from every streaming source (of the streaming query)</p> </li> <li> <p>Updating availableOffsets StreamProgress with the latest available offsets</p> </li> <li> <p>Updating batch metadata with the current event-time watermark and batch timestamp</p> </li> <li> <p>Checking whether to construct the next micro-batch or not (skip it)</p> </li> </ol> <p>In the end, <code>constructNextBatch</code> returns whether the next streaming micro-batch was constructed or skipped.</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#constructing-or-skipping-next-streaming-micro-batch"},{"title":"Requesting Latest Offsets from Streaming Sources (getOffset, setOffsetRange and getEndOffset Phases) <p><code>constructNextBatch</code> firstly requests every streaming source for the latest offsets.</p>  <p>Note</p> <p><code>constructNextBatch</code> checks out the latest offset in every streaming data source sequentially, i.e. one data source at a time.</p>  <p></p> <p>For every streaming source (Data Source API V1), <code>constructNextBatch</code> updates the status message to the following:</p> <pre><code>Getting offsets from [source]\n</code></pre>","text":"","location":"micro-batch-execution/MicroBatchExecution/#requesting-latest-offsets-from-streaming-sources-getoffset-setoffsetrange-and-getendoffset-phases"},{"title":"getOffset Phase <p>In getOffset time-tracking section, <code>constructNextBatch</code> requests the <code>Source</code> for the latest offset.</p> <p>For every data source, <code>constructNextBatch</code> updates the status message to the following:</p> <pre><code>Getting offsets from [source]\n</code></pre>","text":"","location":"micro-batch-execution/MicroBatchExecution/#getoffset-phase"},{"title":"setOffsetRange Phase <p>In setOffsetRange time-tracking section, <code>constructNextBatch</code> finds the available offsets of the source (in the available offset internal registry) and, if found, requests the <code>MicroBatchReader</code> to...FIXME (from JSON format). <code>constructNextBatch</code> requests the <code>MicroBatchReader</code> to...FIXME</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#setoffsetrange-phase"},{"title":"getEndOffset Phase <p>In getEndOffset time-tracking section, <code>constructNextBatch</code> requests the <code>MicroBatchReader</code> for...FIXME</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#getendoffset-phase"},{"title":"Updating availableOffsets StreamProgress with Latest Available Offsets <p><code>constructNextBatch</code> updates the availableOffsets StreamProgress with the latest reported offsets.</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#updating-availableoffsets-streamprogress-with-latest-available-offsets"},{"title":"Updating Batch Metadata with Current Event-Time Watermark and Batch Timestamp <p><code>constructNextBatch</code> updates the batch metadata with the current event-time watermark (from the WatermarkTracker) and the batch timestamp.</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#updating-batch-metadata-with-current-event-time-watermark-and-batch-timestamp"},{"title":"Checking Whether to Construct Next Micro-Batch or Not (Skip It) <p><code>constructNextBatch</code> checks whether or not the next streaming micro-batch should be constructed (<code>lastExecutionRequiresAnotherBatch</code>).</p> <p><code>constructNextBatch</code> uses the last IncrementalExecution if the last execution requires another micro-batch (using the batch metadata) and the given <code>noDataBatchesEnabled</code> flag is enabled (<code>true</code>).</p> <p><code>constructNextBatch</code> also checks out whether new data is available (based on available and committed offsets).</p>  <p>Note</p> <p><code>shouldConstructNextBatch</code> local flag is enabled (<code>true</code>) when there is new data available (based on offsets) or the last execution requires another micro-batch (and the given <code>noDataBatchesEnabled</code> flag is enabled).</p>  <p><code>constructNextBatch</code> prints out the following TRACE message to the logs:</p> <pre><code>noDataBatchesEnabled = [noDataBatchesEnabled], lastExecutionRequiresAnotherBatch = [lastExecutionRequiresAnotherBatch], isNewDataAvailable = [isNewDataAvailable], shouldConstructNextBatch = [shouldConstructNextBatch]\n</code></pre> <p><code>constructNextBatch</code> branches off per whether to constructs or skip the next batch (per <code>shouldConstructNextBatch</code> flag in the above TRACE message).</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#checking-whether-to-construct-next-micro-batch-or-not-skip-it"},{"title":"Constructing Next Micro-Batch <p>With the shouldConstructNextBatch flag enabled (<code>true</code>), <code>constructNextBatch</code> updates the status message to the following:</p> <pre><code>Writing offsets to log\n</code></pre> <p>[[constructNextBatch-walCommit]] In walCommit time-tracking section, <code>constructNextBatch</code> requests the availableOffsets StreamProgress to convert to OffsetSeq (with the BaseStreamingSources and the current batch metadata (event-time watermark and timestamp)) that is in turn added to the write-ahead log for the current batch ID.</p> <p><code>constructNextBatch</code> prints out the following INFO message to the logs:</p> <pre><code>Committed offsets for batch [currentBatchId]. Metadata [offsetSeqMetadata]\n</code></pre>  Fixme <p>(<code>if (currentBatchId != 0) ...</code>)</p>   Fixme <p>(<code>if (minLogEntriesToMaintain &lt; currentBatchId) ...</code>)</p>  <p><code>constructNextBatch</code> turns the noNewData internal flag off (<code>false</code>).</p> <p>In case of a failure while adding the available offsets to the write-ahead log, <code>constructNextBatch</code> throws an <code>AssertionError</code>:</p> <pre><code>Concurrent update to the log. Multiple streaming jobs detected for [currentBatchId]\n</code></pre>","text":"","location":"micro-batch-execution/MicroBatchExecution/#constructing-next-micro-batch"},{"title":"Skipping Next Micro-Batch <p>With the shouldConstructNextBatch flag disabled (<code>false</code>), <code>constructNextBatch</code> turns the noNewData flag on (<code>true</code>) and wakes up (notifies) all threads waiting for the awaitProgressLockCondition lock.</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#skipping-next-micro-batch"},{"title":"Running Single Streaming Micro-Batch <pre><code>runBatch(\n  sparkSessionToRunBatch: SparkSession): Unit\n</code></pre> <p><code>runBatch</code> prints out the following DEBUG message to the logs (with the current batch ID):</p> <pre><code>Running batch [currentBatchId]\n</code></pre> <p><code>runBatch</code> then performs the following steps (aka phases):</p> <ol> <li>getBatch Phase -- Creating Logical Query Plans For Unprocessed Data From Sources and MicroBatchReaders</li> <li>Transforming Logical Plan to Include Sources and MicroBatchReaders with New Data</li> <li>Transforming CurrentTimestamp and CurrentDate Expressions (Per Batch Metadata)</li> <li>Adapting Transformed Logical Plan to Sink with StreamWriteSupport</li> <li>Setting Local Properties</li> <li>queryPlanning Phase -- Creating and Preparing IncrementalExecution for Execution</li> <li>nextBatch Phase -- Creating DataFrame (with IncrementalExecution for New Data)</li> <li>addBatch Phase -- Adding DataFrame With New Data to Sink</li> <li>Updating Watermark and Committing Offsets to Offset Commit Log</li> </ol> <p>In the end, <code>runBatch</code> prints out the following DEBUG message to the logs (with the current batch ID):</p> <pre><code>Completed batch [currentBatchId]\n</code></pre>  <p>Note</p> <p><code>runBatch</code> is used exclusively when <code>MicroBatchExecution</code> is requested to run an activated streaming query (and there is a new data to process).</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#running-single-streaming-micro-batch"},{"title":"getBatch Phase -- Creating Logical Query Plans For Unprocessed Data From Sources and MicroBatchReaders <p>In getBatch time-tracking section, <code>runBatch</code> goes over the available offsets and processes every Source and MicroBatchReader (associated with the available offsets) to create logical query plans (<code>newData</code>) for data processing (per offset ranges).</p>  <p>Note</p> <p><code>runBatch</code> requests sources and readers for data per offset range sequentially, one by one.</p>  <p></p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#getbatch-phase-creating-logical-query-plans-for-unprocessed-data-from-sources-and-microbatchreaders"},{"title":"getBatch Phase and Sources <p>For a Source (with the available offsets different from the committedOffsets registry), <code>runBatch</code> does the following:</p> <ul> <li> <p>Requests the committedOffsets for the committed offsets for the <code>Source</code> (if available)</p> </li> <li> <p>Requests the <code>Source</code> for a dataframe for the offset range (the current and available offsets)</p> </li> </ul> <p><code>runBatch</code> prints out the following DEBUG message to the logs.</p> <pre><code>Retrieving data from [source]: [current] -&gt; [available]\n</code></pre> <p>In the end, <code>runBatch</code> returns the <code>Source</code> and the logical plan of the streaming dataset (for the offset range).</p> <p>In case the <code>Source</code> returns a dataframe that is not streaming, <code>runBatch</code> throws an <code>AssertionError</code>:</p> <pre><code>DataFrame returned by getBatch from [source] did not have isStreaming=true\\n[logicalQueryPlan]\n</code></pre>","text":"","location":"micro-batch-execution/MicroBatchExecution/#getbatch-phase-and-sources"},{"title":"getBatch Phase and MicroBatchReaders <p><code>runBatch</code> does the following...FIXME</p> <ul> <li> <p>Requests the committedOffsets for the committed offsets for the <code>MicroBatchReader</code> (if available)</p> </li> <li> <p>Requests the <code>MicroBatchReader</code> to...FIXME (if available)</p> </li> <li> <p>Requests the <code>MicroBatchReader</code> to...FIXME (only for SerializedOffsets)</p> </li> <li> <p>Requests the <code>MicroBatchReader</code> to...FIXME (the current and available offsets)</p> </li> </ul> <p><code>runBatch</code> prints out the following DEBUG message to the logs.</p> <pre><code>Retrieving data from [reader]: [current] -&gt; [availableV2]\n</code></pre> <p><code>runBatch</code> looks up the <code>DataSourceV2</code> and the options for the <code>MicroBatchReader</code> (in the readerToDataSourceMap internal registry).</p> <p>In the end, <code>runBatch</code> requests the <code>MicroBatchReader</code> for...FIXME and creates a StreamingDataSourceV2Relation logical operator (with the read schema, the <code>DataSourceV2</code>, options, and the <code>MicroBatchReader</code>).</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#getbatch-phase-and-microbatchreaders"},{"title":"Transforming Logical Plan to Include Sources and MicroBatchReaders with New Data <p></p> <p><code>runBatch</code> transforms the analyzed logical plan to include Sources and MicroBatchReaders with new data (<code>newBatchesPlan</code> with logical plans to process data that has arrived since the last batch).</p> <p>For every StreamingExecutionRelation, <code>runBatch</code> tries to find the corresponding logical plan for processing new data.</p> <p>If the logical plan is found, <code>runBatch</code> makes the plan a child operator of <code>Project</code> (with <code>Aliases</code>) logical operator and replaces the <code>StreamingExecutionRelation</code>.</p> <p>Otherwise, if not found, <code>runBatch</code> simply creates an empty streaming <code>LocalRelation</code> (for scanning data from an empty local collection).</p> <p>In case the number of columns in dataframes with new data and <code>StreamingExecutionRelation</code>'s do not match, <code>runBatch</code> throws an <code>AssertionError</code>:</p> <pre><code>Invalid batch: [output] != [dataPlan.output]\n</code></pre>","text":"","location":"micro-batch-execution/MicroBatchExecution/#transforming-logical-plan-to-include-sources-and-microbatchreaders-with-new-data"},{"title":"Transforming CurrentTimestamp and CurrentDate Expressions (Per Batch Metadata) <p><code>runBatch</code> replaces all <code>CurrentTimestamp</code> and <code>CurrentDate</code> expressions in the transformed logical plan (with new data) with the current batch timestamp (based on the batch metadata).</p>  <p>Note</p> <p><code>CurrentTimestamp</code> and <code>CurrentDate</code> expressions correspond to <code>current_timestamp</code> and <code>current_date</code> standard function, respectively.</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#transforming-currenttimestamp-and-currentdate-expressions-per-batch-metadata"},{"title":"Adapting Transformed Logical Plan to Sink with StreamWriteSupport <p><code>runBatch</code>...FIXME</p> <p>For a Sink (Data Source API V1), <code>runBatch</code> changes nothing.</p> <p>For any other BaseStreamingSink type, <code>runBatch</code> simply throws an <code>IllegalArgumentException</code>:</p> <pre><code>unknown sink type for [sink]\n</code></pre>","text":"","location":"micro-batch-execution/MicroBatchExecution/#adapting-transformed-logical-plan-to-sink-with-streamwritesupport"},{"title":"Setting Local Properties <p><code>runBatch</code> sets the local properties.</p>    Local Property Value     streaming.sql.batchId currentBatchId   __is_continuous_processing <code>false</code>","text":"","location":"micro-batch-execution/MicroBatchExecution/#setting-local-properties"},{"title":"queryPlanning Phase -- Creating and Preparing IncrementalExecution for Execution <p></p> <p>In queryPlanning time-tracking section, <code>runBatch</code> creates a new IncrementalExecution with the following:</p> <ul> <li> <p>Transformed logical plan</p> </li> <li> <p>Output mode</p> </li> <li> <p><code>state</code> checkpoint directory</p> </li> <li> <p>Run ID</p> </li> <li> <p>Batch ID</p> </li> <li> <p>Batch Metadata (Event-Time Watermark and Timestamp)</p> </li> </ul> <p>In the end (of the <code>queryPlanning</code> phase), <code>runBatch</code> requests the <code>IncrementalExecution</code> to prepare the transformed logical plan for execution (i.e. execute the <code>executedPlan</code> query execution phase).</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#queryplanning-phase-creating-and-preparing-incrementalexecution-for-execution"},{"title":"nextBatch Phase \u2014 Creating DataFrame (with IncrementalExecution for New Data) <p></p> <p><code>runBatch</code> creates a new <code>DataFrame</code> with the new IncrementalExecution.</p> <p>The <code>DataFrame</code> represents the result of executing the current micro-batch of the streaming query.</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#nextbatch-phase-creating-dataframe-with-incrementalexecution-for-new-data"},{"title":"addBatch Phase \u2014 Adding DataFrame With New Data to Sink <p></p> <p>In addBatch time-tracking section, <code>runBatch</code> adds the <code>DataFrame</code> with new data to the BaseStreamingSink.</p> <p>For a Sink (Data Source API V1), <code>runBatch</code> simply requests the <code>Sink</code> to add the DataFrame (with the batch ID).</p> <p><code>runBatch</code> uses <code>SQLExecution.withNewExecutionId</code> to execute and track all the Spark jobs under one execution id (so it is reported as one single multi-job execution, e.g. in web UI).</p>  <p>Note</p> <p><code>SQLExecution.withNewExecutionId</code> posts a <code>SparkListenerSQLExecutionStart</code> event before execution and a <code>SparkListenerSQLExecutionEnd</code> event right afterwards.</p>   <p>Tip</p> <p>Register <code>SparkListener</code> to get notified about the SQL execution events (<code>SparkListenerSQLExecutionStart</code> and <code>SparkListenerSQLExecutionEnd</code>).</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#addbatch-phase-adding-dataframe-with-new-data-to-sink"},{"title":"Updating Watermark and Committing Offsets to Offset Commit Log <p><code>runBatch</code> requests the WatermarkTracker to update event-time watermark (with the <code>executedPlan</code> of the IncrementalExecution).</p> <p><code>runBatch</code> requests the Offset Commit Log to persisting metadata of the streaming micro-batch (with the current batch ID and event-time watermark of the WatermarkTracker).</p> <p>In the end, <code>runBatch</code> adds the available offsets to the committed offsets (and updates the offsets of every source with new data in the current micro-batch).</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#updating-watermark-and-committing-offsets-to-offset-commit-log"},{"title":"Stopping Stream Processing (Execution of Streaming Query) <pre><code>stop(): Unit\n</code></pre> <p><code>stop</code> sets the state to <code>TERMINATED</code>.</p> <p>When the stream execution thread is alive, <code>stop</code> requests the current <code>SparkContext</code> to <code>cancelJobGroup</code> identified by the runId and waits for this thread to die. Just to make sure that there are no more streaming jobs, <code>stop</code> requests the current <code>SparkContext</code> to <code>cancelJobGroup</code> identified by the runId again.</p> <p>In the end, <code>stop</code> prints out the following INFO message to the logs:</p> <pre><code>Query [prettyIdString] was stopped\n</code></pre> <p><code>stop</code> is part of the StreamingQuery abstraction.</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#stopping-stream-processing-execution-of-streaming-query"},{"title":"Checking Whether New Data Is Available <pre><code>isNewDataAvailable: Boolean\n</code></pre> <p><code>isNewDataAvailable</code> returns whether or not there are streaming sources (in the available offsets) for which committed offsets are different from the available offsets or not available (committed) at all.</p> <p><code>isNewDataAvailable</code> is <code>true</code> when there is at least one such streaming source.</p> <p><code>isNewDataAvailable</code> is used when:</p> <ul> <li><code>MicroBatchExecution</code> is requested to run an activated streaming query and construct the next streaming micro-batch</li> </ul>","text":"","location":"micro-batch-execution/MicroBatchExecution/#checking-whether-new-data-is-available"},{"title":"Analyzed Logical Plan <pre><code>logicalPlan: LogicalPlan\n</code></pre> <p><code>logicalPlan</code> is part of the StreamExecution abstraction.</p> <p><code>logicalPlan</code> resolves (replaces) StreamingRelation, StreamingRelationV2 logical operators to StreamingExecutionRelation logical operators. <code>logicalPlan</code> uses the transformed logical plan to set the uniqueSources and sources internal registries to be the BaseStreamingSources of all the <code>StreamingExecutionRelations</code> unique and not, respectively.</p>  Lazy Value  <p><code>logicalPlan</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and cached afterwards.</p> <p>Internally, <code>logicalPlan</code> transforms the analyzed logical plan.</p> <p>For every StreamingRelation logical operator, <code>logicalPlan</code> tries to replace it with the StreamingExecutionRelation that was used earlier for the same <code>StreamingRelation</code> (if used multiple times in the plan) or creates a new one. While creating a new <code>StreamingExecutionRelation</code>, <code>logicalPlan</code> requests the <code>DataSource</code> to create a streaming Source with the metadata path as <code>sources/uniqueID</code> directory in the checkpoint root directory. <code>logicalPlan</code> prints out the following INFO message to the logs:</p> <pre><code>Using Source [source] from DataSourceV1 named '[sourceName]' [dataSourceV1]\n</code></pre> <p>For every StreamingRelationV2 logical operator with a MicroBatchStream data source (which is not on the list of spark.sql.streaming.disabledV2MicroBatchReaders), <code>logicalPlan</code> tries to replace it with the StreamingExecutionRelation that was used earlier for the same <code>StreamingRelationV2</code> (if used multiple times in the plan) or creates a new one. While creating a new <code>StreamingExecutionRelation</code>, <code>logicalPlan</code> requests the <code>MicroBatchStream</code> to create a MicroBatchStream with the metadata path as <code>sources/uniqueID</code> directory in the checkpoint root directory. <code>logicalPlan</code> prints out the following INFO message to the logs:</p> <pre><code>Using MicroBatchReader [reader] from DataSourceV2 named '[sourceName]' [dataSourceV2]\n</code></pre> <p>For every other StreamingRelationV2 leaf logical operator, <code>logicalPlan</code> tries to replace it with the StreamingExecutionRelation that was used earlier for the same <code>StreamingRelationV2</code> (if used multiple times in the plan) or creates a new one. While creating a new <code>StreamingExecutionRelation</code>, <code>logicalPlan</code> requests the <code>StreamingRelation</code> for the underlying DataSource that is in turn requested to create a streaming Source with the metadata path as <code>sources/uniqueID</code> directory in the checkpoint root directory. <code>logicalPlan</code> prints out the following INFO message to the logs:</p> <pre><code>Using Source [source] from DataSourceV2 named '[sourceName]' [dataSourceV2]\n</code></pre> <p><code>logicalPlan</code> requests the transformed analyzed logical plan for all <code>StreamingExecutionRelations</code> that are then requested for BaseStreamingSources, and saves them as the sources internal registry.</p> <p>In the end, <code>logicalPlan</code> sets the uniqueSources internal registry to be the unique <code>BaseStreamingSources</code> above.</p> <p><code>logicalPlan</code> throws an <code>AssertionError</code> when not executed on the stream execution thread.</p> <pre><code>logicalPlan must be initialized in QueryExecutionThread but the current thread was [currentThread]\n</code></pre>","text":"","location":"micro-batch-execution/MicroBatchExecution/#analyzed-logical-plan"},{"title":"streaming.sql.batchId Local Property <p><code>MicroBatchExecution</code> defines streaming.sql.batchId as the name of the local property to be the current batch or epoch IDs (that Spark tasks can use at execution time).</p> <p><code>streaming.sql.batchId</code> is used when:</p> <ul> <li><code>MicroBatchExecution</code> is requested to run a single streaming micro-batch (and sets the property to be the current batch ID)</li> <li><code>DataWritingSparkTask</code> is requested to run (and needs an epoch ID)</li> </ul>","text":"","location":"micro-batch-execution/MicroBatchExecution/#streamingsqlbatchid-local-property"},{"title":"WatermarkTracker <p><code>MicroBatchExecution</code> creates a WatermarkTracker while populating start offsets (when requested to run an activated streaming query).</p> <p>The <code>WatermarkTracker</code> is used then for the following:</p> <ul> <li>Setting watermark while populating start offsets</li> <li>Reading the current watermark while the following:<ul> <li>Constructing or skipping next streaming micro-batch (to update the OffsetSeqMetadata)</li> <li>Running a single streaming micro-batch (to add a CommitMetadata to the Offset Commit Log)</li> </ul> </li> <li>Updating watermark while running a single streaming micro-batch</li> </ul>","text":"","location":"micro-batch-execution/MicroBatchExecution/#watermarktracker"},{"title":"isCurrentBatchConstructed Flag <pre><code>isCurrentBatchConstructed: Boolean\n</code></pre> <p><code>MicroBatchExecution</code> uses <code>isCurrentBatchConstructed</code> internal flag to control whether or not to run a streaming micro-batch.</p> <p>Default: <code>false</code></p> <p>When <code>false</code>, changed to whatever constructing the next streaming micro-batch gives back when running activated streaming query</p> <p>Disabled (<code>false</code>) after running a streaming micro-batch (when enabled after constructing the next streaming micro-batch)</p> <p>Enabled (<code>true</code>) when populating start offsets (when running an activated streaming query) and re-starting a streaming query from a checkpoint (using the Offset Write-Ahead Log)</p> <p>Disabled (<code>false</code>) when populating start offsets (when running an activated streaming query) and re-starting a streaming query from a checkpoint when the latest offset checkpointed (written) to the offset write-ahead log has been successfully processed and committed to the Offset Commit Log</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#iscurrentbatchconstructed-flag"},{"title":"Demo <pre><code>import org.apache.spark.sql.streaming.Trigger\nval query = spark\n  .readStream\n  .format(\"rate\")\n  .load\n  .writeStream\n  .format(\"console\")          // &lt;-- not a StreamWriteSupport sink\n  .option(\"truncate\", false)\n  .trigger(Trigger.Once)      // &lt;-- Gives MicroBatchExecution\n  .queryName(\"rate2console\")\n  .start\n\n// The following gives access to the internals\n// And to MicroBatchExecution\nimport org.apache.spark.sql.execution.streaming.StreamingQueryWrapper\nval engine = query.asInstanceOf[StreamingQueryWrapper].streamingQuery\nimport org.apache.spark.sql.execution.streaming.StreamExecution\nassert(engine.isInstanceOf[StreamExecution])\n\nimport org.apache.spark.sql.execution.streaming.MicroBatchExecution\nval microBatchEngine = engine.asInstanceOf[MicroBatchExecution]\nassert(microBatchEngine.trigger == Trigger.Once)\n</code></pre>","text":"","location":"micro-batch-execution/MicroBatchExecution/#demo"},{"title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.streaming.MicroBatchExecution</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.streaming.MicroBatchExecution=ALL\n</code></pre> <p>Refer to Logging.</p>","text":"","location":"micro-batch-execution/MicroBatchExecution/#logging"},{"title":"MicroBatchWrite","text":"<p><code>MicroBatchWrite</code> is a <code>BatchWrite</code> (Spark SQL) for WriteToDataSourceV2 logical operator in Micro-Batch Stream Processing.</p>  <p>WriteToMicroBatchDataSource</p> <p><code>WriteToDataSourceV2</code> logical operator replaces WriteToMicroBatchDataSource logical operator at logical optimization (using <code>V2Writes</code> logical optimization).</p>  <p><code>MicroBatchWrite</code> is just a very thin wrapper over StreamingWrite and does nothing but delegates all the important execution-specific calls to it.</p>","location":"micro-batch-execution/MicroBatchWrite/"},{"title":"Creating Instance","text":"<p><code>MicroBatchWrite</code> takes the following to be created:</p> <ul> <li> Epoch ID <li> StreamingWrite  <p><code>MicroBatchWrite</code> is created when:</p> <ul> <li><code>V2Writes</code> (Spark SQL) logical optimization is requested to optimize a logical plan (with a WriteToMicroBatchDataSource)</li> </ul>","location":"micro-batch-execution/MicroBatchWrite/#creating-instance"},{"title":"ExecutionStats","text":"<p>== [[ExecutionStats]] ExecutionStats</p> <p><code>ExecutionStats</code> is...FIXME</p>","location":"monitoring/ExecutionStats/"},{"title":"MetricsReporter","text":"<p><code>MetricsReporter</code> is a Metrics Source for streaming queries.</p> <p><code>MetricsReporter</code> uses the last StreamingQueryProgress (of the StreamExecution) if available or simply defaults to a \"zero\" value.</p>","location":"monitoring/MetricsReporter/"},{"title":"Creating Instance","text":"<p><code>MetricsReporter</code> takes the following to be created:</p> <ul> <li> StreamExecution <li> Source Name  <p><code>MetricsReporter</code> is created for stream execution engines.</p>","location":"monitoring/MetricsReporter/#creating-instance"},{"title":"Gauges","text":"","location":"monitoring/MetricsReporter/#gauges"},{"title":"inputRate-total","text":"<p>Reports inputRowsPerSecond (across all streaming sources)</p>","location":"monitoring/MetricsReporter/#inputrate-total"},{"title":"processingRate-total","text":"<p>Reports processedRowsPerSecond (across all streaming sources)</p>","location":"monitoring/MetricsReporter/#processingrate-total"},{"title":"latency","text":"<p>Reports <code>triggerExecution</code> duration of the last StreamingQueryProgress</p>","location":"monitoring/MetricsReporter/#latency"},{"title":"eventTime-watermark","text":"<p>Reports <code>watermark</code> of the last StreamingQueryProgress</p> <p>Format: <code>yyyy-MM-dd'T'HH:mm:ss.SSS'Z'</code></p>","location":"monitoring/MetricsReporter/#eventtime-watermark"},{"title":"states-rowsTotal","text":"<p>Reports the total of numRowsTotal of all StateOperatorProgresses of the last StreamingQueryProgress</p>","location":"monitoring/MetricsReporter/#states-rowstotal"},{"title":"states-usedBytes","text":"<p>Reports the total of memoryUsedBytes of all StateOperatorProgresses of the last StreamingQueryProgress</p>","location":"monitoring/MetricsReporter/#states-usedbytes"},{"title":"ProgressReporter","text":"<p><code>ProgressReporter</code> is an abstraction of execution progress reporters that report statistics of execution of a streaming query.</p>","location":"monitoring/ProgressReporter/"},{"title":"Contract","text":"","location":"monitoring/ProgressReporter/#contract"},{"title":"currentBatchId <pre><code>currentBatchId: Long\n</code></pre> <p>ID of the streaming batch</p> <p>Used when:</p> <ul> <li><code>MicroBatchExecution</code> is requested to plan a query for the batch (while running a batch)</li> <li><code>ContinuousExecution</code> is requested to plan a query for the epoch (while running continuously)</li> <li><code>ProgressReporter</code> is requested for a new StreamingQueryProgress (while finishing a trigger)</li> <li>other usage</li> </ul>","text":"","location":"monitoring/ProgressReporter/#currentbatchid"},{"title":"id <pre><code>id: UUID\n</code></pre> <p>Universally unique identifier (UUID) of the streaming query (that remains unchanged between restarts)</p>","text":"","location":"monitoring/ProgressReporter/#id"},{"title":"lastExecution <pre><code>lastExecution: QueryExecution\n</code></pre> <p>IncrementalExecution of the streaming execution round (a batch or an epoch)</p> <p><code>IncrementalExecution</code> is created and executed in the queryPlanning phase of MicroBatchExecution and ContinuousExecution stream execution engines.</p>","text":"","location":"monitoring/ProgressReporter/#lastexecution"},{"title":"logicalPlan <pre><code>logicalPlan: LogicalPlan\n</code></pre> <p>Logical query plan of the streaming query</p>  <p>Important</p> <p>The most interesting usage of the <code>LogicalPlan</code> is when stream execution engines replace (transform) input StreamingExecutionRelation and StreamingDataSourceV2Relation operators with (operators with) data or <code>LocalRelation</code> (to represent no data at a source).</p>  <p>Used when <code>ProgressReporter</code> is requested for the following:</p> <ul> <li>extract statistics from the most recent query execution (to add <code>watermark</code> metric for streaming watermark)</li> <li>extractSourceToNumInputRows</li> </ul>","text":"","location":"monitoring/ProgressReporter/#logicalplan"},{"title":"name <pre><code>name: String\n</code></pre> <p>Name of the streaming query</p>","text":"","location":"monitoring/ProgressReporter/#name"},{"title":"newData <pre><code>newData: Map[SparkDataStream, LogicalPlan]\n</code></pre> <p>SparkDataStreams (from all data sources) with the more recent unprocessed input data (as <code>LogicalPlan</code>)</p> <p>Used exclusively for MicroBatchExecution (when requested to run a single micro-batch)</p> <p>Used when <code>ProgressReporter</code> is requested to extractSourceToNumInputRows</p>","text":"","location":"monitoring/ProgressReporter/#newdata"},{"title":"offsetSeqMetadata <pre><code>offsetSeqMetadata: OffsetSeqMetadata\n</code></pre> <p>OffsetSeqMetadata (with the current micro-batch event-time watermark and timestamp)</p>","text":"","location":"monitoring/ProgressReporter/#offsetseqmetadata"},{"title":"postEvent <pre><code>postEvent(\n  event: StreamingQueryListener.Event): Unit\n</code></pre> <p>Posts StreamingQueryListener.Event</p> <p>Used when:</p> <ul> <li><code>ProgressReporter</code> is requested to update progress (and posts a QueryProgressEvent)</li> <li><code>StreamExecution</code> is requested to run stream processing (and posts a QueryStartedEvent at the beginning and a QueryTerminatedEvent after a query has been stopped)</li> </ul>","text":"","location":"monitoring/ProgressReporter/#postevent"},{"title":"runId <pre><code>runId: UUID\n</code></pre> <p>Universally unique identifier (UUID) of a single run of the streaming query (that changes every restart)</p>","text":"","location":"monitoring/ProgressReporter/#runid"},{"title":"Sink <pre><code>sink: Table\n</code></pre> <p><code>Table</code> (Spark SQL) this streaming query writes to</p> <p>Used when:</p> <ul> <li><code>ProgressReporter</code> is requested to finish a streaming batch</li> </ul>","text":"","location":"monitoring/ProgressReporter/#sink"},{"title":"sinkCommitProgress <pre><code>sinkCommitProgress: Option[StreamWriterCommitProgress]\n</code></pre> <p><code>StreamWriterCommitProgress</code> with number of output rows:</p> <ul> <li> <p><code>None</code> when <code>MicroBatchExecution</code> stream execution engine is requested to populateStartOffsets</p> </li> <li> <p>Assigned a <code>StreamWriterCommitProgress</code> when <code>MicroBatchExecution</code> stream execution engine is about to complete running a micro-batch</p> </li> </ul> <p>Used when finishTrigger (and updating progress)</p>","text":"","location":"monitoring/ProgressReporter/#sinkcommitprogress"},{"title":"SparkDataStreams <pre><code>sources: Seq[SparkDataStream]\n</code></pre> <p>SparkDataStreams of this streaming query</p>","text":"","location":"monitoring/ProgressReporter/#sparkdatastreams"},{"title":"sparkSession <pre><code>sparkSession: SparkSession\n</code></pre> <p><code>SparkSession</code> (Spark SQL) of the streaming query</p>","text":"","location":"monitoring/ProgressReporter/#sparksession"},{"title":"triggerClock <pre><code>triggerClock: Clock\n</code></pre> <p>Clock of the streaming query</p>","text":"","location":"monitoring/ProgressReporter/#triggerclock"},{"title":"Implementations","text":"<ul> <li>StreamExecution</li> </ul>","location":"monitoring/ProgressReporter/#implementations"},{"title":"spark.sql.streaming.noDataProgressEventInterval <p><code>ProgressReporter</code> uses the spark.sql.streaming.noDataProgressEventInterval configuration property to control how long to wait between two progress events when there is no data (default: <code>10000L</code>) when finishing a trigger.</p>","text":"","location":"monitoring/ProgressReporter/#sparksqlstreamingnodataprogresseventinterval"},{"title":"Demo <pre><code>import org.apache.spark.sql.streaming.Trigger\nimport scala.concurrent.duration._\nval sampleQuery = spark\n  .readStream\n  .format(\"rate\")\n  .load\n  .writeStream\n  .format(\"console\")\n  .option(\"truncate\", false)\n  .trigger(Trigger.ProcessingTime(10.seconds))\n  .start\n\n// Using public API\nimport org.apache.spark.sql.streaming.SourceProgress\nscala&gt; sampleQuery.\n     |   lastProgress.\n     |   sources.\n     |   map { case sp: SourceProgress =&gt;\n     |     s\"source = ${sp.description} =&gt; endOffset = ${sp.endOffset}\" }.\n     |   foreach(println)\nsource = RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8] =&gt; endOffset = 663\n\nscala&gt; println(sampleQuery.lastProgress.sources(0))\nres40: org.apache.spark.sql.streaming.SourceProgress =\n{\n  \"description\" : \"RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8]\",\n  \"startOffset\" : 333,\n  \"endOffset\" : 343,\n  \"numInputRows\" : 10,\n  \"inputRowsPerSecond\" : 0.9998000399920015,\n  \"processedRowsPerSecond\" : 200.0\n}\n\n// With a hack\nimport org.apache.spark.sql.execution.streaming.StreamingQueryWrapper\nval offsets = sampleQuery.\n  asInstanceOf[StreamingQueryWrapper].\n  streamingQuery.\n  availableOffsets.\n  map { case (source, offset) =&gt;\n    s\"source = $source =&gt; offset = $offset\" }\nscala&gt; offsets.foreach(println)\nsource = RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8] =&gt; offset = 293\n</code></pre>","text":"","location":"monitoring/ProgressReporter/#demo"},{"title":"StreamingQueryProgress Queue <pre><code>progressBuffer: Queue[StreamingQueryProgress]\n</code></pre> <p><code>progressBuffer</code> is a scala.collection.mutable.Queue of StreamingQueryProgresses.</p> <p><code>progressBuffer</code> has a new <code>StreamingQueryProgress</code> added when <code>ProgressReporter</code> is requested to update progress of a streaming query.</p> <p>The oldest <code>StreamingQueryProgress</code> is removed (dequeued) above spark.sql.streaming.numRecentProgressUpdates threshold.</p> <p><code>progressBuffer</code> is used when <code>ProgressReporter</code> is requested for the last and the recent StreamingQueryProgresses.</p>","text":"","location":"monitoring/ProgressReporter/#streamingqueryprogress-queue"},{"title":"Current StreamingQueryStatus <pre><code>status: StreamingQueryStatus\n</code></pre> <p><code>status</code> is the current StreamingQueryStatus.</p> <p><code>status</code> is used when <code>StreamingQueryWrapper</code> is requested for the current status of a streaming query.</p>","text":"","location":"monitoring/ProgressReporter/#current-streamingquerystatus"},{"title":"Updating Progress of Streaming Query <pre><code>updateProgress(\n  newProgress: StreamingQueryProgress): Unit\n</code></pre> <p><code>updateProgress</code> records the input <code>newProgress</code> and posts a QueryProgressEvent event.</p> <p></p> <p><code>updateProgress</code> adds the input <code>newProgress</code> to progressBuffer.</p> <p><code>updateProgress</code> removes elements from progressBuffer if their number is or exceeds the value of spark.sql.streaming.numRecentProgressUpdates configuration property.</p> <p><code>updateProgress</code> posts a QueryProgressEvent (with the input <code>newProgress</code>).</p> <p><code>updateProgress</code> prints out the following INFO message to the logs:</p> <pre><code>Streaming query made progress: [newProgress]\n</code></pre> <p><code>updateProgress</code> is used when <code>ProgressReporter</code> is requested to finish up a trigger.</p>","text":"","location":"monitoring/ProgressReporter/#updating-progress-of-streaming-query"},{"title":"Initializing Query Progress for New Trigger <pre><code>startTrigger(): Unit\n</code></pre> <p><code>startTrigger</code> prints out the following DEBUG message to the logs:</p> <pre><code>Starting Trigger Calculation\n</code></pre> <p>.startTrigger's Internal Registry Changes For New Trigger [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Registry | New Value</p> <p>| &lt;&gt; | &lt;&gt; <p>| &lt;&gt; | Requests the &lt;&gt; for the current timestamp (in millis) <p>| &lt;&gt; | Enables (<code>true</code>) the <code>isTriggerActive</code> flag of the &lt;&gt; <p>| &lt;&gt; | <code>null</code> <p>| &lt;&gt; | <code>null</code> <p>| &lt;&gt; | Clears the &lt;&gt; <p>|===</p> <p><code>startTrigger</code> is used when:</p> <ul> <li> <p><code>MicroBatchExecution</code> stream execution engine is requested to run an activated streaming query (at the beginning of every trigger)</p> </li> <li> <p><code>ContinuousExecution</code> stream execution engine is requested to run an activated streaming query (at the beginning of every trigger)</p> </li> </ul> <p><code>StreamExecution</code> starts running batches (as part of TriggerExecutor executing a batch runner).</p>","text":"","location":"monitoring/ProgressReporter/#initializing-query-progress-for-new-trigger"},{"title":"Finishing Up Streaming Batch (Trigger) <pre><code>finishTrigger(\n  hasNewData: Boolean): Unit\n</code></pre> <p><code>finishTrigger</code> sets currentTriggerEndTimestamp to the current time (using triggerClock).</p> <p><code>finishTrigger</code> &lt;&gt;. <p><code>finishTrigger</code> calculates the processing time (in seconds) as the difference between the &lt;&gt; and &lt;&gt; timestamps. <p><code>finishTrigger</code> calculates the input time (in seconds) as the difference between the start time of the &lt;&gt; and &lt;&gt; triggers. <p>.ProgressReporter's finishTrigger and Timestamps image::images/ProgressReporter-finishTrigger-timestamps.png[align=\"center\"]</p> <p><code>finishTrigger</code> prints out the following DEBUG message to the logs:</p> <pre><code>Execution stats: [executionStats]\n</code></pre> <p><code>finishTrigger</code> creates a &lt;&gt; (aka source statistics) for &lt;&gt;. <p><code>finishTrigger</code> creates a &lt;&gt; (aka sink statistics) for the &lt;&gt;. <p><code>finishTrigger</code> creates a StreamingQueryProgress.</p> <p>If there was any data (using the input <code>hasNewData</code> flag), <code>finishTrigger</code> resets &lt;&gt; (i.e. becomes the minimum possible time) and &lt;&gt;. <p>Otherwise, when no data was available (using the input <code>hasNewData</code> flag), <code>finishTrigger</code> &lt;&gt; only when &lt;&gt; passed. <p>In the end, <code>finishTrigger</code> disables <code>isTriggerActive</code> flag of &lt;&gt; (i.e. sets it to <code>false</code>). <p>NOTE: <code>finishTrigger</code> is used exclusively when <code>MicroBatchExecution</code> is requested to &lt;&gt; (after &lt;&gt; at the end of a streaming batch).","text":"","location":"monitoring/ProgressReporter/#finishing-up-streaming-batch-trigger"},{"title":"Time-Tracking Section (Recording Execution Time) <pre><code>reportTimeTaken[T](\n  triggerDetailKey: String)(\n  body: =&gt; T): T\n</code></pre> <p><code>reportTimeTaken</code> measures the time to execute <code>body</code> and records it in the currentDurationsMs internal registry under <code>triggerDetailKey</code> key. If the <code>triggerDetailKey</code> key was recorded already, the current execution time is added.</p> <p>In the end, <code>reportTimeTaken</code> prints out the following DEBUG message to the logs and returns the result of executing <code>body</code>.</p> <pre><code>[triggerDetailKey] took [time] ms\n</code></pre> <p><code>reportTimeTaken</code> is used when stream execution engines are requested to execute the following phases (that appear as <code>triggerDetailKey</code> in the DEBUG message in the logs):</p> <ol> <li> <p><code>MicroBatchExecution</code></p> <ol> <li>triggerExecution</li> <li>getOffset</li> <li>setOffsetRange</li> <li>getEndOffset</li> <li>walCommit</li> <li>getBatch</li> <li>queryPlanning</li> <li>addBatch</li> </ol> </li> <li> <p><code>ContinuousExecution</code></p> <ol> <li>queryPlanning</li> <li>runContinuous</li> </ol> </li> </ol>","text":"","location":"monitoring/ProgressReporter/#time-tracking-section-recording-execution-time"},{"title":"Updating Status Message <pre><code>updateStatusMessage(\n  message: String): Unit\n</code></pre> <p><code>updateStatusMessage</code> simply updates the <code>message</code> in the StreamingQueryStatus internal registry.</p> <p><code>updateStatusMessage</code> is used when:</p> <ul> <li> <p><code>StreamExecution</code> is requested to run stream processing</p> </li> <li> <p><code>MicroBatchExecution</code> is requested to run an activated streaming query or construct the next streaming micro-batch</p> </li> </ul>","text":"","location":"monitoring/ProgressReporter/#updating-status-message"},{"title":"Generating Execution Statistics <pre><code>extractExecutionStats(\n  hasNewData: Boolean): ExecutionStats\n</code></pre> <p><code>extractExecutionStats</code> generates an ExecutionStats of the &lt;&gt; of the streaming query. <p>Internally, <code>extractExecutionStats</code> generate watermark metric (using the event-time watermark of the &lt;&gt;) if there is a EventTimeWatermark unary logical operator in the &lt;&gt; of the streaming query. <p><code>extractExecutionStats</code> extractStateOperatorMetrics.</p> <p><code>extractExecutionStats</code> extractSourceToNumInputRows.</p> <p><code>extractExecutionStats</code> finds the EventTimeWatermarkExec unary physical operator (with non-zero EventTimeStats) and generates max, min, and avg statistics.</p> <p>In the end, <code>extractExecutionStats</code> creates a ExecutionStats with the execution statistics.</p> <p>If the input <code>hasNewData</code> flag is turned off (<code>false</code>), <code>extractExecutionStats</code> returns an ExecutionStats with no input rows and event-time statistics (that require data to be processed to have any sense).</p> <p>NOTE: <code>extractExecutionStats</code> is used exclusively when <code>ProgressReporter</code> is requested to &lt;&gt;.","text":"","location":"monitoring/ProgressReporter/#generating-execution-statistics"},{"title":"Generating StateStoreWriter Metrics (StateOperatorProgress) <pre><code>extractStateOperatorMetrics(\n  hasNewData: Boolean): Seq[StateOperatorProgress]\n</code></pre> <p><code>extractStateOperatorMetrics</code> requests the &lt;&gt; for the optimized execution plan (<code>executedPlan</code>) and finds all StateStoreWriter physical operators and requests them for StateOperatorProgress. <p><code>extractStateOperatorMetrics</code> clears (zeros) the numRowsUpdated metric for the given <code>hasNewData</code> turned off (<code>false</code>).</p> <p><code>extractStateOperatorMetrics</code> returns an empty collection for the &lt;&gt; uninitialized (<code>null</code>). <p><code>extractStateOperatorMetrics</code> is used when <code>ProgressReporter</code> is requested to generate execution statistics.</p>","text":"","location":"monitoring/ProgressReporter/#generating-statestorewriter-metrics-stateoperatorprogress"},{"title":"Recording Trigger Offsets (StreamProgress) <pre><code>recordTriggerOffsets(\n  from: StreamProgress,\n  to: StreamProgress): Unit\n</code></pre> <p><code>recordTriggerOffsets</code> simply sets (records) the &lt;&gt; and &lt;&gt; internal registries to the json representations of the <code>from</code> and <code>to</code> StreamProgresses. <p><code>recordTriggerOffsets</code> is used when:</p> <ul> <li> <p><code>MicroBatchExecution</code> is requested to &lt;&gt;  <li> <p><code>ContinuousExecution</code> is requested to &lt;&gt;","text":"","location":"monitoring/ProgressReporter/#recording-trigger-offsets-streamprogress"},{"title":"Last StreamingQueryProgress <pre><code>lastProgress: StreamingQueryProgress\n</code></pre> <p>The last StreamingQueryProgress</p>","text":"","location":"monitoring/ProgressReporter/#last-streamingqueryprogress"},{"title":"currentDurationsMs <p>scala.collection.mutable.HashMap of action names (aka triggerDetailKey) and their cumulative times (in milliseconds).</p> <p></p> <p>Starts empty when <code>ProgressReporter</code> sets the state for a new batch with new entries added or updated when reporting execution time (of an action).</p> <p><code>currentDurationsMs</code> is available as <code>durationMs</code> of a streaming query.</p> <pre><code>scala&gt; :type q\norg.apache.spark.sql.streaming.StreamingQuery\n\nscala&gt; query.lastProgress.durationMs\nres1: java.util.Map[String,Long] = {triggerExecution=60, queryPlanning=1, getBatch=5, getOffset=0, addBatch=30, walCommit=23}\n\nscala&gt; println(q.lastProgress)\n{\n  \"id\" : \"03fc78fc-fe19-408c-a1ae-812d0e28fcee\",\n  \"runId\" : \"8c247071-afba-40e5-aad2-0e6f45f22488\",\n  \"name\" : null,\n  \"timestamp\" : \"2017-08-14T20:30:00.004Z\",\n  \"batchId\" : 1,\n  \"numInputRows\" : 432,\n  \"inputRowsPerSecond\" : 0.9993568953312452,\n  \"processedRowsPerSecond\" : 1380.1916932907347,\n  \"durationMs\" : {\n    \"addBatch\" : 237,\n    \"getBatch\" : 26,\n    \"getOffset\" : 0,\n    \"queryPlanning\" : 1,\n    \"triggerExecution\" : 313,\n    \"walCommit\" : 45\n  },\n  \"stateOperators\" : [ ],\n  \"sources\" : [ {\n    \"description\" : \"RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8]\",\n    \"startOffset\" : 0,\n    \"endOffset\" : 432,\n    \"numInputRows\" : 432,\n    \"inputRowsPerSecond\" : 0.9993568953312452,\n    \"processedRowsPerSecond\" : 1380.1916932907347\n  } ],\n  \"sink\" : {\n    \"description\" : \"ConsoleSink[numRows=20, truncate=true]\"\n  }\n}\n</code></pre>","text":"","location":"monitoring/ProgressReporter/#currentdurationsms"},{"title":"Internal Properties","text":"","location":"monitoring/ProgressReporter/#internal-properties"},{"title":"currentTriggerEndTimestamp <p>Timestamp of when the current batch/trigger has ended</p> <p>Default: <code>-1L</code></p>","text":"","location":"monitoring/ProgressReporter/#currenttriggerendtimestamp"},{"title":"currentTriggerStartOffsets <pre><code>currentTriggerStartOffsets: Map[BaseStreamingSource, String]\n</code></pre> <p>Start offsets (in JSON format) per streaming source</p> <p>Used exclusively when &lt;&gt; (for a SourceProgress) <p>Reset (<code>null</code>) when &lt;&gt; <p>Initialized when &lt;&gt;","text":"","location":"monitoring/ProgressReporter/#currenttriggerstartoffsets"},{"title":"currentTriggerStartTimestamp <p>Timestamp of when the current batch/trigger has started</p> <p>Default: <code>-1L</code></p>","text":"","location":"monitoring/ProgressReporter/#currenttriggerstarttimestamp"},{"title":"lastTriggerStartTimestamp <p>Timestamp of when the last batch/trigger started</p> <p>Default: <code>-1L</code></p>","text":"","location":"monitoring/ProgressReporter/#lasttriggerstarttimestamp"},{"title":"Logging <p>Configure logging of the concrete stream execution progress reporters to see what happens inside:</p> <ul> <li> <p>ContinuousExecution</p> </li> <li> <p>MicroBatchExecution</p> </li> </ul>","text":"","location":"monitoring/ProgressReporter/#logging"},{"title":"SinkProgress","text":"<p><code>SinkProgress</code> is...FIXME</p>","location":"monitoring/SinkProgress/"},{"title":"SourceProgress","text":"<p><code>SourceProgress</code> is...FIXME</p>","location":"monitoring/SourceProgress/"},{"title":"StateOperatorProgress","text":"<p><code>StateOperatorProgress</code> is metadata about updates made to stateful operators of a single micro-batch (progress) of a StreamingQuery.</p>","location":"monitoring/StateOperatorProgress/"},{"title":"Creating Instance","text":"<p><code>StateOperatorProgress</code> takes the following to be created:</p> <ul> <li> Operator Name <li>numRowsTotal</li> <li> numRowsUpdated <li> allUpdatesTimeMs <li> numRowsRemoved <li> allRemovalsTimeMs <li> commitTimeMs <li> memoryUsedBytes <li> numRowsDroppedByWatermark <li> numShufflePartitions <li> numStateStoreInstances <li> Custom Metrics (default: empty)  <p><code>StateOperatorProgress</code> is created when:</p> <ul> <li><code>StateStoreWriter</code> is requested for a StateOperatorProgress</li> <li><code>StateOperatorProgress</code> is requested for a copy</li> </ul>","location":"monitoring/StateOperatorProgress/#creating-instance"},{"title":"numRowsTotal <p><code>numRowsTotal</code> is the value of numTotalStateRows metric of a StateStoreWriter physical operator (when requested to get progress).</p>","text":"","location":"monitoring/StateOperatorProgress/#numrowstotal"},{"title":"copy <pre><code>copy(\n  newNumRowsUpdated: Long,\n  newNumRowsDroppedByWatermark: Long): StateOperatorProgress\n</code></pre> <p><code>copy</code> creates a copy of this <code>StateOperatorProgress</code> with the numRowsUpdated and numRowsDroppedByWatermark metrics updated.</p>  <p><code>copy</code> is used when:</p> <ul> <li><code>ProgressReporter</code> is requested to extractStateOperatorMetrics</li> <li><code>SessionWindowStateStoreSaveExec</code> is requested for a StateOperatorProgress</li> </ul>","text":"","location":"monitoring/StateOperatorProgress/#copy"},{"title":"StreamingQueryListener \u2014 Intercepting Life Cycle Events of Streaming Queries","text":"<p><code>StreamingQueryListener</code> is an abstraction of listeners to be notified about the life cycle events of all the streaming queries in a Spark Structured Streaming application:</p> <ul> <li>Query started</li> <li>Query made progress</li> <li>Query terminated</li> </ul> <p><code>StreamingQueryListener</code> is used internally by <code>StreamingQueryListenerBus</code> to post a streaming event to all registered StreamingQueryListeners.</p> <p><code>StreamingQueryListener</code> can be used by Spark developers to intercept events in Spark Structured Streaming applications.</p>","location":"monitoring/StreamingQueryListener/"},{"title":"Contract","text":"","location":"monitoring/StreamingQueryListener/#contract"},{"title":"onQueryProgress <pre><code>onQueryProgress(\n  event: QueryProgressEvent): Unit\n</code></pre> <p>Informs that <code>MicroBatchExecution</code> has finished triggerExecution phase (the end of a streaming batch)</p> <p></p>","text":"","location":"monitoring/StreamingQueryListener/#onqueryprogress"},{"title":"onQueryStarted <pre><code>onQueryStarted(\n  event: QueryStartedEvent): Unit\n</code></pre> <p>Informs that <code>DataStreamWriter</code> was requested to start execution of the streaming query (on the stream execution thread)</p> <p></p>  <p>Note</p> <p><code>onQueryStarted</code> is used internally to unblock the starting thread of <code>StreamExecution</code>.</p>","text":"","location":"monitoring/StreamingQueryListener/#onquerystarted"},{"title":"onQueryTerminated <pre><code>onQueryTerminated(\n  event: QueryTerminatedEvent): Unit\n</code></pre> <p>Informs that a streaming query was &lt;&gt; or terminated due to an error <p></p>","text":"","location":"monitoring/StreamingQueryListener/#onqueryterminated"},{"title":"Lifecycle Events <p><code>StreamingQueryListener</code> is informed about the life cycle events when <code>StreamingQueryListenerBus</code> is requested to doPostEvent.</p>","text":"","location":"monitoring/StreamingQueryListener/#lifecycle-events"},{"title":"QueryStartedEvent <ul> <li>id</li> <li>runId</li> <li>name</li> </ul> <p>Intercepted by onQueryStarted</p> <p>Posted when <code>StreamExecution</code> is requested to run stream processing (when <code>DataStreamWriter</code> is requested to start execution of the streaming query on the stream execution thread)</p>","text":"","location":"monitoring/StreamingQueryListener/#querystartedevent"},{"title":"QueryProgressEvent <ul> <li>StreamingQueryProgress</li> </ul> <p>Intercepted by onQueryProgress</p> <p>Posted when <code>ProgressReporter</code> is requested to update progress of a streaming query (after <code>MicroBatchExecution</code> has finished triggerExecution phase at the end of a streaming batch)</p>","text":"","location":"monitoring/StreamingQueryListener/#queryprogressevent"},{"title":"QueryTerminatedEvent <ul> <li>id</li> <li>runId</li> <li>exception if terminated due to an error</li> </ul> <p>Intercepted by onQueryTerminated</p> <p>Posted when <code>StreamExecution</code> is requested to run stream processing (and the streaming query was stopped or terminated due to an error)</p>","text":"","location":"monitoring/StreamingQueryListener/#queryterminatedevent"},{"title":"Registering StreamingQueryListener <p><code>StreamingQueryListener</code> can be registered using StreamingQueryManager.addListener method.</p> <pre><code>val queryListener: StreamingQueryListener = ...\nspark.streams.addListener(queryListener)\n</code></pre>","text":"","location":"monitoring/StreamingQueryListener/#registering-streamingquerylistener"},{"title":"Deregistering StreamingQueryListener <p><code>StreamingQueryListener</code> can be deregistered using StreamingQueryManager.removeListener method.</p> <pre><code>val queryListener: StreamingQueryListener = ...\nspark.streams.removeListener(queryListener)\n</code></pre>","text":"","location":"monitoring/StreamingQueryListener/#deregistering-streamingquerylistener"},{"title":"StreamingQueryProgress","text":"<p><code>StreamingQueryProgress</code> is metadata of a single micro-batch (progress) of a StreamingQuery.</p>","location":"monitoring/StreamingQueryProgress/"},{"title":"Creating Instance","text":"<p><code>StreamingQueryProgress</code> takes the following to be created:</p> <ul> <li> Unique identifier <li> Unique identifier of a query execution <li> Name <li>Batch Timestamp</li> <li> Unique ID of a micro-batch <li> Batch Duration <li> Durations of the internal phases (in ms) <li>EventTime Statistics</li> <li> StateOperatorProgress for every stateful operator <li> SourceProgress for every streaming source <li> SinkProgress <li> Observed Metrics  <p><code>StreamingQueryProgress</code> is created when:</p> <ul> <li><code>StreamExecution</code> is requested to finish a trigger</li> </ul>","location":"monitoring/StreamingQueryProgress/#creating-instance"},{"title":"Batch Timestamp <p><code>StreamingQueryProgress</code> is given a timestamp when created.</p> <p>The time when a trigger has started (in ISO8601 format).</p>","text":"","location":"monitoring/StreamingQueryProgress/#batch-timestamp"},{"title":"Event Time Statistics <p><code>StreamingQueryProgress</code> is given an Event Time Statistics when created.</p>","text":"","location":"monitoring/StreamingQueryProgress/#event-time-statistics"},{"title":"Last and Recent Progresses","text":"<p>Use lastProgress property of a <code>StreamingQuery</code> to access the most recent <code>StreamingQueryProgress</code> update.</p> <pre><code>val sq: StreamingQuery = ...\nsq.lastProgress\n</code></pre> <p>Use recentProgress property of a <code>StreamingQuery</code> to access the most recent <code>StreamingQueryProgress</code> updates.</p> <pre><code>val sq: StreamingQuery = ...\nsq.recentProgress\n</code></pre>","location":"monitoring/StreamingQueryProgress/#last-and-recent-progresses"},{"title":"StreamingQueryListener","text":"<p>Use StreamingQueryListener to be notified about <code>StreamingQueryProgress</code> updates while a streaming query is executed.</p>","location":"monitoring/StreamingQueryProgress/#streamingquerylistener"},{"title":"StreamingQueryStatus","text":"<p><code>StreamingQueryStatus</code> is...FIXME</p>","location":"monitoring/StreamingQueryStatus/"},{"title":"Streaming Operators \u2014 High-Level Declarative Streaming Dataset API","text":"<p>Dataset API defines a set of operators that are used in Spark Structured Streaming and together constitute the High-Level Declarative Streaming Dataset API.</p>","location":"operators/"},{"title":"crossJoin Operator \u2014 Streaming Join","text":"<pre><code>crossJoin(\n  right: Dataset[_]): DataFrame\n</code></pre> <p><code>crossJoin</code> operator...FIXME</p>","location":"operators/crossJoin/"},{"title":"dropDuplicates Operator \u2014 Streaming Deduplication","text":"<pre><code>dropDuplicates(): Dataset[T]\ndropDuplicates(\n  colNames: Seq[String]): Dataset[T]\ndropDuplicates(\n  col1: String,\n  cols: String*): Dataset[T]\n</code></pre> <p><code>dropDuplicates</code> operator drops duplicate records (given a subset of columns)</p>  <p>Note</p> <p>For a streaming Dataset, <code>dropDuplicates</code> will keep all data across triggers as intermediate state to drop duplicates rows. You can use withWatermark operator to limit how late the duplicate data can be and system will accordingly limit the state. In addition, too late data older than watermark will be dropped to avoid any possibility of duplicates.</p>","location":"operators/dropDuplicates/"},{"title":"Demo","text":"<pre><code>// Start a streaming query\n// Using old-fashioned MemoryStream (with the deprecated SQLContext)\nimport org.apache.spark.sql.execution.streaming.MemoryStream\nimport org.apache.spark.sql.SQLContext\nimplicit val sqlContext: SQLContext = spark.sqlContext\nval source = MemoryStream[(Int, Int)]\nval ids = source.toDS.toDF(\"time\", \"id\").\n  withColumn(\"time\", $\"time\" cast \"timestamp\"). // &lt;-- convert time column from Int to Timestamp\n  dropDuplicates(\"id\").\n  withColumn(\"time\", $\"time\" cast \"long\")  // &lt;-- convert time column back from Timestamp to Int\n\n// Conversions are only for display purposes\n// Internally we need timestamps for watermark to work\n// Displaying timestamps could be too much for such a simple task\n\nscala&gt; println(ids.queryExecution.analyzed.numberedTreeString)\n00 Project [cast(time#10 as bigint) AS time#15L, id#6]\n01 +- Deduplicate [id#6], true\n02    +- Project [cast(time#5 as timestamp) AS time#10, id#6]\n03       +- Project [_1#2 AS time#5, _2#3 AS id#6]\n04          +- StreamingExecutionRelation MemoryStream[_1#2,_2#3], [_1#2, _2#3]\n\nimport org.apache.spark.sql.streaming.{OutputMode, Trigger}\nimport scala.concurrent.duration._\nval q = ids.\n  writeStream.\n  format(\"memory\").\n  queryName(\"dups\").\n  outputMode(OutputMode.Append).\n  trigger(Trigger.ProcessingTime(30.seconds)).\n  option(\"checkpointLocation\", \"checkpoint-dir\"). // &lt;-- use checkpointing to save state between restarts\n  start\n\n// Publish duplicate records\nsource.addData(1 -&gt; 1)\nsource.addData(2 -&gt; 1)\nsource.addData(3 -&gt; 1)\n\nq.processAllAvailable()\n\n// Check out how dropDuplicates removes duplicates\n// --&gt; per single streaming batch (easy)\nscala&gt; spark.table(\"dups\").show\n+----+---+\n|time| id|\n+----+---+\n|   1|  1|\n+----+---+\n\nsource.addData(4 -&gt; 1)\nsource.addData(5 -&gt; 2)\n\n// --&gt; across streaming batches (harder)\nscala&gt; spark.table(\"dups\").show\n+----+---+\n|time| id|\n+----+---+\n|   1|  1|\n|   5|  2|\n+----+---+\n\n// Check out the internal state\nscala&gt; println(q.lastProgress.stateOperators(0).prettyJson)\n{\n  \"numRowsTotal\" : 2,\n  \"numRowsUpdated\" : 1,\n  \"memoryUsedBytes\" : 17751\n}\n\n// You could use web UI's SQL tab instead\n// Use Details for Query\n\nsource.addData(6 -&gt; 2)\n\nscala&gt; spark.table(\"dups\").show\n+----+---+\n|time| id|\n+----+---+\n|   1|  1|\n|   5|  2|\n+----+---+\n\n// Check out the internal state\nscala&gt; println(q.lastProgress.stateOperators(0).prettyJson)\n{\n  \"numRowsTotal\" : 2,\n  \"numRowsUpdated\" : 0,\n  \"memoryUsedBytes\" : 17751\n}\n\n// Restart the streaming query\nq.stop\n\nval q = ids.\n  writeStream.\n  format(\"memory\").\n  queryName(\"dups\").\n  outputMode(OutputMode.Complete).  // &lt;-- memory sink supports checkpointing for Complete output mode only\n  trigger(Trigger.ProcessingTime(30.seconds)).\n  option(\"checkpointLocation\", \"checkpoint-dir\"). // &lt;-- use checkpointing to save state between restarts\n  start\n\n// Doh! MemorySink is fine, but Complete is only available with a streaming aggregation\n// Answer it if you know why --&gt; https://stackoverflow.com/q/45756997/1305344\n\n// It's a high time to work on https://issues.apache.org/jira/browse/SPARK-21667\n// to understand the low-level details (and the reason, it seems)\n\n// Disabling operation checks and starting over\n// ./bin/spark-shell -c spark.sql.streaming.unsupportedOperationCheck=false\n// it works now --&gt; no exception!\n\nscala&gt; spark.table(\"dups\").show\n+----+---+\n|time| id|\n+----+---+\n+----+---+\n\nsource.addData(0 -&gt; 1)\n// wait till the batch is triggered\nscala&gt; spark.table(\"dups\").show\n+----+---+\n|time| id|\n+----+---+\n|   0|  1|\n+----+---+\n\nsource.addData(1 -&gt; 1)\nsource.addData(2 -&gt; 1)\n// wait till the batch is triggered\nscala&gt; spark.table(\"dups\").show\n+----+---+\n|time| id|\n+----+---+\n+----+---+\n\n// What?! No rows?! It doesn't look as if it worked fine :(\n\n// Use groupBy to pass the requirement of having streaming aggregation for Complete output mode\nval counts = ids.groupBy(\"id\").agg(first($\"time\") as \"first_time\")\nscala&gt; counts.explain\n== Physical Plan ==\n*HashAggregate(keys=[id#246], functions=[first(time#255L, false)])\n+- StateStoreSave [id#246], StatefulOperatorStateInfo(&lt;unknown&gt;,3585583b-42d7-4547-8d62-255581c48275,0,0), Append, 0\n   +- *HashAggregate(keys=[id#246], functions=[merge_first(time#255L, false)])\n      +- StateStoreRestore [id#246], StatefulOperatorStateInfo(&lt;unknown&gt;,3585583b-42d7-4547-8d62-255581c48275,0,0)\n         +- *HashAggregate(keys=[id#246], functions=[merge_first(time#255L, false)])\n            +- *HashAggregate(keys=[id#246], functions=[partial_first(time#255L, false)])\n               +- *Project [cast(time#250 as bigint) AS time#255L, id#246]\n                  +- StreamingDeduplicate [id#246], StatefulOperatorStateInfo(&lt;unknown&gt;,3585583b-42d7-4547-8d62-255581c48275,1,0), 0\n                     +- Exchange hashpartitioning(id#246, 200)\n                        +- *Project [cast(_1#242 as timestamp) AS time#250, _2#243 AS id#246]\n                           +- StreamingRelation MemoryStream[_1#242,_2#243], [_1#242, _2#243]\nval q = counts.\n  writeStream.\n  format(\"memory\").\n  queryName(\"dups\").\n  outputMode(OutputMode.Complete).  // &lt;-- memory sink supports checkpointing for Complete output mode only\n  trigger(Trigger.ProcessingTime(30.seconds)).\n  option(\"checkpointLocation\", \"checkpoint-dir\"). // &lt;-- use checkpointing to save state between restarts\n  start\n\nsource.addData(0 -&gt; 1)\nsource.addData(1 -&gt; 1)\n// wait till the batch is triggered\nscala&gt; spark.table(\"dups\").show\n+---+----------+\n| id|first_time|\n+---+----------+\n|  1|         0|\n+---+----------+\n\n// Publish duplicates\n// Check out how dropDuplicates removes duplicates\n\n// Stop the streaming query\n// Specify event time watermark to remove old duplicates\n</code></pre>","location":"operators/dropDuplicates/#demo"},{"title":"Dataset.explain Operator \u2014 Explaining Streaming Queries","text":"<p><pre><code>explain(): Unit // &lt;1&gt;\nexplain(\n  extended: Boolean): Unit\n</code></pre> &lt;1&gt; Calls <code>explain</code> with <code>extended</code> flag disabled</p> <p><code>Dataset.explain</code> operator explains query plans, i.e. prints the logical and (with <code>extended</code> flag enabled) physical query plans to the console.</p> <p>Internally, <code>explain</code> creates a <code>ExplainCommand</code> runnable command with the logical plan and <code>extended</code> flag.</p> <p><code>explain</code> then executes the plan with <code>ExplainCommand</code> runnable command and collects the results that are printed out to the standard output.</p>","location":"operators/explain/"},{"title":"[NOTE]","text":"<p><code>explain</code> uses <code>SparkSession</code> to access the current <code>SessionState</code> to execute the plan.</p>","location":"operators/explain/#note"},{"title":"[source, scala]","text":"<p>import org.apache.spark.sql.execution.command.ExplainCommand val explain = ExplainCommand(...) spark.sessionState.executePlan(explain)</p>  <p>====</p> <p>For streaming Datasets, <code>ExplainCommand</code> command simply creates a IncrementalExecution for the <code>SparkSession</code> and the logical plan.</p> <p>NOTE: For the purpose of <code>explain</code>, <code>IncrementalExecution</code> is created with the output mode <code>Append</code>, checkpoint location <code>&lt;unknown&gt;</code>, run id a random number, current batch id <code>0</code> and offset metadata empty. They do not really matter when explaining the load-part of a streaming query.</p>","location":"operators/explain/#source-scala"},{"title":"Demo","text":"<pre><code>val records = spark.\n  readStream.\n  format(\"rate\").\n  load\nscala&gt; records.explain\n== Physical Plan ==\nStreamingRelation rate, [timestamp#0, value#1L]\n\nscala&gt; records.explain(extended = true)\n== Parsed Logical Plan ==\nStreamingRelation DataSource(org.apache.spark.sql.SparkSession@4071aa13,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L]\n\n== Analyzed Logical Plan ==\ntimestamp: timestamp, value: bigint\nStreamingRelation DataSource(org.apache.spark.sql.SparkSession@4071aa13,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L]\n\n== Optimized Logical Plan ==\nStreamingRelation DataSource(org.apache.spark.sql.SparkSession@4071aa13,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L]\n\n== Physical Plan ==\nStreamingRelation rate, [timestamp#0, value#1L]\n</code></pre>","location":"operators/explain/#demo"},{"title":"flatMapGroupsWithState","text":"<p><code>flatMapGroupsWithState</code> is...FIXME</p>","location":"operators/flatMapGroupsWithState/"},{"title":"groupBy Operator \u2014 Streaming Aggregation","text":"<pre><code>groupBy(\n  cols: Column*): RelationalGroupedDataset\ngroupBy(\n  col1: String,\n  cols: String*): RelationalGroupedDataset\n</code></pre> <p><code>groupBy</code> operator aggregates rows by zero, one or more columns.</p>","location":"operators/groupBy/"},{"title":"Demo","text":"<pre><code>val fromTopic1 = spark.\n  readStream.\n  format(\"kafka\").\n  option(\"subscribe\", \"topic1\").\n  option(\"kafka.bootstrap.servers\", \"localhost:9092\").\n  load\n\n// extract event time et al\n// time,key,value\n/*\n2017-08-23T00:00:00.002Z,1,now\n2017-08-23T00:05:00.002Z,1,5 mins later\n2017-08-23T00:09:00.002Z,1,9 mins later\n2017-08-23T00:11:00.002Z,1,11 mins later\n2017-08-23T01:00:00.002Z,1,1 hour later\n// late event = watermark should be (1 hour - 10 minutes) already\n2017-08-23T00:49:59.002Z,1,==&gt; SHOULD NOT BE INCLUDED in aggregation as too late &lt;==\n\nCAUTION: FIXME SHOULD NOT BE INCLUDED is included contrary to my understanding?!\n*/\nval timedValues = fromTopic1.\n  select('value cast \"string\").\n  withColumn(\"tokens\", split('value, \",\")).\n  withColumn(\"time\", to_timestamp('tokens(0))).\n  withColumn(\"key\", 'tokens(1) cast \"int\").\n  withColumn(\"value\", 'tokens(2)).\n  select(\"time\", \"key\", \"value\")\n\n// aggregation with watermark\nval counts = timedValues.\n  withWatermark(\"time\", \"10 minutes\").\n  groupBy(\"key\").\n  agg(collect_list('value) as \"values\", collect_list('time) as \"times\")\n\n// Note that StatefulOperatorStateInfo is mostly generic\n// since no batch-specific values are currently available\n// only after the first streaming batch\nscala&gt; counts.explain\n== Physical Plan ==\nObjectHashAggregate(keys=[key#27], functions=[collect_list(value#33, 0, 0), collect_list(time#22-T600000ms, 0, 0)])\n+- Exchange hashpartitioning(key#27, 200)\n   +- StateStoreSave [key#27], StatefulOperatorStateInfo(&lt;unknown&gt;,25149816-1f14-4901-af13-896286a26d42,0,0), Append, 0\n      +- ObjectHashAggregate(keys=[key#27], functions=[merge_collect_list(value#33, 0, 0), merge_collect_list(time#22-T600000ms, 0, 0)])\n         +- Exchange hashpartitioning(key#27, 200)\n            +- StateStoreRestore [key#27], StatefulOperatorStateInfo(&lt;unknown&gt;,25149816-1f14-4901-af13-896286a26d42,0,0)\n               +- ObjectHashAggregate(keys=[key#27], functions=[merge_collect_list(value#33, 0, 0), merge_collect_list(time#22-T600000ms, 0, 0)])\n                  +- Exchange hashpartitioning(key#27, 200)\n                     +- ObjectHashAggregate(keys=[key#27], functions=[partial_collect_list(value#33, 0, 0), partial_collect_list(time#22-T600000ms, 0, 0)])\n                        +- EventTimeWatermark time#22: timestamp, interval 10 minutes\n                           +- *Project [cast(split(cast(value#1 as string), ,)[0] as timestamp) AS time#22, cast(split(cast(value#1 as string), ,)[1] as int) AS key#27, split(cast(value#1 as string), ,)[2] AS value#33]\n                              +- StreamingRelation kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n\nimport org.apache.spark.sql.streaming._\nimport scala.concurrent.duration._\nval sq = counts.writeStream.\n  format(\"console\").\n  option(\"truncate\", false).\n  trigger(Trigger.ProcessingTime(30.seconds)).\n  outputMode(OutputMode.Update).  // &lt;-- only Update or Complete acceptable because of groupBy aggregation\n  start\n\n// After StreamingQuery was started,\n// the physical plan is complete (with batch-specific values)\nscala&gt; sq.explain\n== Physical Plan ==\nObjectHashAggregate(keys=[key#27], functions=[collect_list(value#33, 0, 0), collect_list(time#22-T600000ms, 0, 0)])\n+- Exchange hashpartitioning(key#27, 200)\n   +- StateStoreSave [key#27], StatefulOperatorStateInfo(file:/private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/temporary-635d6519-b6ca-4686-9b6b-5db0e83cfd51/state,855cec1c-25dc-4a86-ae54-c6cdd4ed02ec,0,0), Update, 0\n      +- ObjectHashAggregate(keys=[key#27], functions=[merge_collect_list(value#33, 0, 0), merge_collect_list(time#22-T600000ms, 0, 0)])\n         +- Exchange hashpartitioning(key#27, 200)\n            +- StateStoreRestore [key#27], StatefulOperatorStateInfo(file:/private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/temporary-635d6519-b6ca-4686-9b6b-5db0e83cfd51/state,855cec1c-25dc-4a86-ae54-c6cdd4ed02ec,0,0)\n               +- ObjectHashAggregate(keys=[key#27], functions=[merge_collect_list(value#33, 0, 0), merge_collect_list(time#22-T600000ms, 0, 0)])\n                  +- Exchange hashpartitioning(key#27, 200)\n                     +- ObjectHashAggregate(keys=[key#27], functions=[partial_collect_list(value#33, 0, 0), partial_collect_list(time#22-T600000ms, 0, 0)])\n                        +- EventTimeWatermark time#22: timestamp, interval 10 minutes\n                           +- *Project [cast(split(cast(value#76 as string), ,)[0] as timestamp) AS time#22, cast(split(cast(value#76 as string), ,)[1] as int) AS key#27, split(cast(value#76 as string), ,)[2] AS value#33]\n                              +- Scan ExistingRDD[key#75,value#76,topic#77,partition#78,offset#79L,timestamp#80,timestampType#81]\n</code></pre>","location":"operators/groupBy/#demo"},{"title":"groupByKey Operator \u2014 Streaming Aggregation","text":"<pre><code>groupByKey(\n  func: T =&gt; K): KeyValueGroupedDataset[K, T]\n</code></pre> <p><code>groupByKey</code> operator aggregates rows by a typed grouping function for Arbitrary Stateful Streaming Aggregation.</p> <p><code>groupByKey</code> creates a KeyValueGroupedDataset (with keys of type <code>K</code> and rows of type <code>T</code>) to apply aggregation functions over groups of rows (of type <code>T</code>) by key (of type <code>K</code>) per the given <code>func</code> key-generating function.</p>  <p>Note</p> <p>The type of the input argument of <code>func</code> is the type of rows in the Dataset (i.e. <code>Dataset[T]</code>).</p>  <p><code>groupByKey</code> simply applies the <code>func</code> function to every row (of type <code>T</code>) and associates it with a logical group per key (of type <code>K</code>).</p> <pre><code>func: T =&gt; K\n</code></pre> <p>Internally, <code>groupByKey</code> creates a structured query with the <code>AppendColumns</code> unary logical operator (with the given <code>func</code> and the analyzed logical plan of the target <code>Dataset</code> that <code>groupByKey</code> was executed on) and creates a new <code>QueryExecution</code>.</p> <p>In the end, <code>groupByKey</code> creates a KeyValueGroupedDataset with the following:</p> <ul> <li> <p>Encoders for <code>K</code> keys and <code>T</code> rows</p> </li> <li> <p>The new <code>QueryExecution</code> (with the <code>AppendColumns</code> unary logical operator)</p> </li> <li> <p>The output schema of the analyzed logical plan</p> </li> <li> <p>The new columns of the <code>AppendColumns</code> logical operator (i.e. the attributes of the key)</p> </li> </ul> <pre><code>scala&gt; :type sq\norg.apache.spark.sql.Dataset[Long]\n\nval baseCode = 'A'.toInt\nval byUpperChar = (n: java.lang.Long) =&gt; (n % 3 + baseCode).toString\nval kvs = sq.groupByKey(byUpperChar)\n\nscala&gt; :type kvs\norg.apache.spark.sql.KeyValueGroupedDataset[String,Long]\n\n// Peeking under the surface of KeyValueGroupedDataset\nimport org.apache.spark.sql.catalyst.plans.logical.AppendColumns\nval appendColumnsOp = kvs.queryExecution.analyzed.collect { case ac: AppendColumns =&gt; ac }.head\nscala&gt; println(appendColumnsOp.newColumns)\nList(value#7)\n</code></pre>","location":"operators/groupByKey/"},{"title":"Demo: Aggregating Orders Per Zip Code","text":"<p>Go to Demo: groupByKey Streaming Aggregation in Update Mode.</p>","location":"operators/groupByKey/#demo-aggregating-orders-per-zip-code"},{"title":"Demo: Aggregating Metrics Per Device","text":"<p>The following example code shows how to apply <code>groupByKey</code> operator to a structured stream of timestamped values of different devices.</p> <pre><code>// input stream\nimport java.sql.Timestamp\nval signals = spark.\n  readStream.\n  format(\"rate\").\n  option(\"rowsPerSecond\", 1).\n  load.\n  withColumn(\"value\", $\"value\" % 10)  // &lt;-- randomize the values (just for fun)\n  withColumn(\"deviceId\", lit(util.Random.nextInt(10))). // &lt;-- 10 devices randomly assigned to values\n  as[(Timestamp, Long, Int)] // &lt;-- convert to a \"better\" type (from \"unpleasant\" Row)\n\n// stream processing using groupByKey operator\n// groupByKey(func: ((Timestamp, Long, Int)) =&gt; K): KeyValueGroupedDataset[K, (Timestamp, Long, Int)]\n// K becomes Int which is a device id\nval deviceId: ((Timestamp, Long, Int)) =&gt; Int = { case (_, _, deviceId) =&gt; deviceId }\nscala&gt; val signalsByDevice = signals.groupByKey(deviceId)\nsignalsByDevice: org.apache.spark.sql.KeyValueGroupedDataset[Int,(java.sql.Timestamp, Long, Int)] = org.apache.spark.sql.KeyValueGroupedDataset@19d40bc6\n</code></pre>","location":"operators/groupByKey/#demo-aggregating-metrics-per-device"},{"title":"join Operator \u2014 Streaming Join","text":"<pre><code>join(\n  right: Dataset[_]): DataFrame\njoin(\n  right: Dataset[_],\n  joinExprs: Column): DataFrame\njoin(\n  right: Dataset[_],\n  joinExprs: Column,\n  joinType: String): DataFrame\njoin(\n  right: Dataset[_],\n  usingColumns: Seq[String]): DataFrame\njoin(\n  right: Dataset[_],\n  usingColumns: Seq[String],\n  joinType: String): DataFrame\njoin(\n  right: Dataset[_],\n  usingColumn: String): DataFrame\n</code></pre> <p>Streaming Join</p>","location":"operators/join/"},{"title":"joinWith Operator \u2014 Streaming Join","text":"<pre><code>joinWith[U](\n  other: Dataset[U],\n  condition: Column): Dataset[(T, U)]\njoinWith[U](\n  other: Dataset[U],\n  condition: Column,\n  joinType: String): Dataset[(T, U)]\n</code></pre> <p>Streaming Join</p>","location":"operators/joinWith/"},{"title":"window","text":"<p>== [[window]] window Function -- Stream Time Windows</p> <p><code>window</code> is a standard function that generates tumbling, sliding or delayed stream time window ranges (on a timestamp column).</p>","location":"operators/window/"},{"title":"[source, scala]","text":"<p>window(   timeColumn: Column,   windowDuration: String): Column  // &lt;1&gt; window(   timeColumn: Column,   windowDuration: String,   slideDuration: String): Column   // &lt;2&gt; window(   timeColumn: Column,   windowDuration: String,   slideDuration: String,   startTime: String): Column       // &lt;3&gt;</p>  <p>&lt;1&gt; Creates a tumbling time window with <code>slideDuration</code> as <code>windowDuration</code> and <code>0 second</code> for <code>startTime</code> &lt;2&gt; Creates a sliding time window with <code>0 second</code> for <code>startTime</code> &lt;3&gt; Creates a delayed time window</p>","location":"operators/window/#source-scala"},{"title":"[NOTE]","text":"<p>From https://msdn.microsoft.com/en-us/library/azure/dn835055.aspx[Tumbling Window (Azure Stream Analytics)]:</p>","location":"operators/window/#note"},{"title":"&gt; Tumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals.","text":"","location":"operators/window/#tumbling-windows-are-a-series-of-fixed-sized-non-overlapping-and-contiguous-time-intervals"},{"title":"[NOTE]","text":"<p>From https://flink.apache.org/news/2015/12/04/Introducing-windows.html[Introducing Stream Windows in Apache Flink]:</p>  <p>Tumbling windows group elements of a stream into finite sets where each set corresponds to an interval.</p>","location":"operators/window/#note_1"},{"title":"&gt; Tumbling windows discretize a stream into non-overlapping windows.","text":"","location":"operators/window/#tumbling-windows-discretize-a-stream-into-non-overlapping-windows"},{"title":"[source, scala]","text":"<p>scala&gt; val timeColumn = window($\"time\", \"5 seconds\") timeColumn: org.apache.spark.sql.Column = timewindow(time, 5000000, 5000000, 0) AS <code>window</code></p>  <p><code>timeColumn</code> should be of <code>TimestampType</code>, i.e. with https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html[java.sql.Timestamp] values.</p> <p>TIP: Use ++https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html#from-java.time.Instant-++[java.sql.Timestamp.from] or ++https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html#valueOf-java.time.LocalDateTime-++[java.sql.Timestamp.valueOf] factory methods to create <code>Timestamp</code> instances.</p>","location":"operators/window/#source-scala_1"},{"title":"[source, scala]","text":"<p>// https://docs.oracle.com/javase/8/docs/api/java/time/LocalDateTime.html import java.time.LocalDateTime // https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html import java.sql.Timestamp val levels = Seq(   // (year, month, dayOfMonth, hour, minute, second)   ((2012, 12, 12, 12, 12, 12), 5),   ((2012, 12, 12, 12, 12, 14), 9),   ((2012, 12, 12, 13, 13, 14), 4),   ((2016, 8,  13, 0, 0, 0), 10),   ((2017, 5,  27, 0, 0, 0), 15)).   map { case ((yy, mm, dd, h, m, s), a) =&gt; (LocalDateTime.of(yy, mm, dd, h, m, s), a) }.   map { case (ts, a) =&gt; (Timestamp.valueOf(ts), a) }.   toDF(\"time\", \"level\") scala&gt; levels.show +-------------------+-----+ |               time|level| +-------------------+-----+ |2012-12-12 12:12:12|    5| |2012-12-12 12:12:14|    9| |2012-12-12 13:13:14|    4| |2016-08-13 00:00:00|   10| |2017-05-27 00:00:00|   15| +-------------------+-----+</p> <p>val q = levels.select(window($\"time\", \"5 seconds\"), $\"level\") scala&gt; q.show(truncate = false) +---------------------------------------------+-----+ |window                                       |level| +---------------------------------------------+-----+ |[2012-12-12 12:12:10.0,2012-12-12 12:12:15.0]|5    | |[2012-12-12 12:12:10.0,2012-12-12 12:12:15.0]|9    | |[2012-12-12 13:13:10.0,2012-12-12 13:13:15.0]|4    | |[2016-08-13 00:00:00.0,2016-08-13 00:00:05.0]|10   | |[2017-05-27 00:00:00.0,2017-05-27 00:00:05.0]|15   | +---------------------------------------------+-----+</p> <p>scala&gt; q.printSchema root  |-- window: struct (nullable = true)  |    |-- start: timestamp (nullable = true)  |    |-- end: timestamp (nullable = true)  |-- level: integer (nullable = false)</p> <p>// calculating the sum of levels every 5 seconds val sums = levels.   groupBy(window($\"time\", \"5 seconds\")).   agg(sum(\"level\") as \"level_sum\").   select(\"window.start\", \"window.end\", \"level_sum\") scala&gt; sums.show +-------------------+-------------------+---------+ |              start|                end|level_sum| +-------------------+-------------------+---------+ |2012-12-12 13:13:10|2012-12-12 13:13:15|        4| |2012-12-12 12:12:10|2012-12-12 12:12:15|       14| |2016-08-13 00:00:00|2016-08-13 00:00:05|       10| |2017-05-27 00:00:00|2017-05-27 00:00:05|       15| +-------------------+-------------------+---------+</p>  <p><code>windowDuration</code> and <code>slideDuration</code> are strings specifying the width of the window for duration and sliding identifiers, respectively.</p> <p>TIP: Use <code>CalendarInterval</code> for valid window identifiers.</p> <p>There are a couple of rules governing the durations:</p> <ol> <li> <p>The window duration must be greater than 0</p> </li> <li> <p>The slide duration must be greater than 0.</p> </li> <li> <p>The start time must be greater than or equal to 0.</p> </li> <li> <p>The slide duration must be less than or equal to the window duration.</p> </li> <li> <p>The start time must be less than the slide duration.</p> </li> </ol> <p>NOTE: Only one <code>window</code> expression is supported in a query.</p> <p>NOTE: <code>null</code> values are filtered out in <code>window</code> expression.</p> <p>Internally, <code>window</code> creates a spark-sql-Column.md[Column] with <code>TimeWindow</code> Catalyst expression under <code>window</code> alias.</p>","location":"operators/window/#source-scala_2"},{"title":"[source, scala]","text":"<p>scala&gt; val timeColumn = window($\"time\", \"5 seconds\") timeColumn: org.apache.spark.sql.Column = timewindow(time, 5000000, 5000000, 0) AS <code>window</code></p> <p>val windowExpr = timeColumn.expr scala&gt; println(windowExpr.numberedTreeString) 00 timewindow('time, 5000000, 5000000, 0) AS window#23 01 +- timewindow('time, 5000000, 5000000, 0) 02    +- 'time</p>  <p>Internally, <code>TimeWindow</code> Catalyst expression is simply a struct type with two fields, i.e. <code>start</code> and <code>end</code>, both of <code>TimestampType</code> type.</p>","location":"operators/window/#source-scala_3"},{"title":"[source, scala]","text":"<p>scala&gt; println(windowExpr.dataType) StructType(StructField(start,TimestampType,true), StructField(end,TimestampType,true))</p> <p>scala&gt; println(windowExpr.dataType.prettyJson) {   \"type\" : \"struct\",   \"fields\" : [ {     \"name\" : \"start\",     \"type\" : \"timestamp\",     \"nullable\" : true,     \"metadata\" : { }   }, {     \"name\" : \"end\",     \"type\" : \"timestamp\",     \"nullable\" : true,     \"metadata\" : { }   } ] }</p>","location":"operators/window/#source-scala_4"},{"title":"[NOTE]","text":"<p><code>TimeWindow</code> time window Catalyst expression is planned (i.e. converted) in <code>TimeWindowing</code> logical optimization rule (i.e. <code>Rule[LogicalPlan]</code>) of the Spark SQL logical query plan analyzer.</p>","location":"operators/window/#note_2"},{"title":"Find more about the\u2009Spark SQL logical query plan analyzer in https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-sql-Analyzer.html[Mastering Apache Spark 2] gitbook.","text":"<p>==== [[window-example]] Example -- Traffic Sensor</p> <p>NOTE: The example is borrowed from https://flink.apache.org/news/2015/12/04/Introducing-windows.html[Introducing Stream Windows in Apache Flink].</p> <p>The example shows how to use <code>window</code> function to model a traffic sensor that counts every 15 seconds the number of vehicles passing a certain location.</p>","location":"operators/window/#find-more-about-the-spark-sql-logical-query-plan-analyzer-in-httpsjaceklaskowskigitbooksiomastering-apache-sparkspark-sql-analyzerhtmlmastering-apache-spark-2-gitbook"},{"title":"withWatermark Operator \u2014 Event-Time Watermark","text":"<pre><code>withWatermark(\n  eventTime: String,\n  delayThreshold: String): Dataset[T]\n</code></pre> <p><code>withWatermark</code> specifies a streaming watermark (on the given <code>eventTime</code> column with a delay threshold).</p> <p><code>withWatermark</code> specifies the <code>eventTime</code> column for event time watermark and <code>delayThreshold</code> for event lateness.</p> <p><code>eventTime</code> specifies the column to use for watermark and can be either part of <code>Dataset</code> from the source or custom-generated using <code>current_time</code> or <code>current_timestamp</code> functions.</p>  <p>Note</p> <p>Watermark tracks a point in time before which it is assumed no more late events are supposed to arrive (and if they have, the late events are considered really late and simply dropped).</p>   <p>Note</p> <p>Spark Structured Streaming uses watermark for the following:</p> <ul> <li> <p>To know when a given time window aggregation (using groupBy operator with window standard function) can be finalized and thus emitted when using output modes that do not allow updates, like Append output mode.</p> </li> <li> <p>To minimize the amount of state that we need to keep for ongoing aggregations, e.g. mapGroupsWithState (for implicit state management), flatMapGroupsWithState (for user-defined state management) and dropDuplicates operators.</p> </li> </ul>  <p>The current watermark is computed by looking at the maximum <code>eventTime</code> seen across all of the partitions in a query minus a user-specified <code>delayThreshold</code>. Due to the cost of coordinating this value across partitions, the actual watermark used is only guaranteed to be at least <code>delayThreshold</code> behind the actual event time.</p>  <p>Note</p> <p>In some cases Spark may still process records that arrive more than <code>delayThreshold</code> late.</p>","location":"operators/withWatermark/"},{"title":"writeStream Operator","text":"<pre><code>writeStream: DataStreamWriter[T]\n</code></pre> <p><code>writeStream</code> creates a DataStreamWriter for persisting the result of a streaming query to an external data system</p>","location":"operators/writeStream/"},{"title":"EventTimeWatermarkExec Physical Operator","text":"<p><code>EventTimeWatermarkExec</code> is a unary physical operator that represents EventTimeWatermark logical operator at execution time.</p>  <p>Tip</p> <p>A unary physical operator (<code>UnaryExecNode</code>) is a physical operator with a single child physical operator.</p> <p>Learn more about Unary Physical Operators (and physical operators in general) in The Internals of Spark SQL online book.</p>  <p><code>EventTimeWatermarkExec</code> operator is used to extract (project) the values of the event-time watermark column and add them all to the EventTimeStatsAccum accumulator (and produce a EventTimeStats).</p>","location":"physical-operators/EventTimeWatermarkExec/"},{"title":"Creating Instance","text":"<p><code>EventTimeWatermarkExec</code> takes the following to be created:</p> <ul> <li> Catalyst <code>Attribute</code> for event time (Spark SQL) <li> Delay Interval (Spark SQL) <li> Child Physical Operator (Spark SQL)  <p>When created, <code>EventTimeWatermarkExec</code> registers the EventTimeStatsAccum accumulator (with the current <code>SparkContext</code>).</p> <p><code>EventTimeWatermarkExec</code> is created\u00a0when StatefulAggregationStrategy execution planning strategy is executed (requested to plan a EventTimeWatermark logical operator for execution).</p>","location":"physical-operators/EventTimeWatermarkExec/#creating-instance"},{"title":"EventTimeStats Accumulator <pre><code>eventTimeStats: EventTimeStatsAccum\n</code></pre> <p><code>EventTimeWatermarkExec</code> creates an EventTimeStatsAccum accumulator when created.</p> <p>When executed, <code>EventTimeWatermarkExec</code> uses the <code>EventTimeStatsAccum</code> to extract and accumulate eventTime values (as <code>Long</code>s) from every row in a streaming batch.</p>  <p>Note</p> <p>Since the execution (data processing) happens on Spark executors, the only way to establish communication between the tasks (on the executors) and the driver is to use accumulator facility.</p> <p>Learn more about Accumulators in The Internals of Apache Spark online book.</p>  <p><code>eventTimeStats</code> is registered (with the current <code>SparkContext</code>) when <code>EventTimeWatermarkExec</code> is created. <code>eventTimeStats</code> uses no name (unnamed accumulator).</p> <p><code>eventTimeStats</code> is used to transfer the statistics (maximum, minimum, average and update count) of the long values in the event-time watermark column to be used for the following:</p> <ul> <li> <p><code>ProgressReporter</code> is requested for the most recent execution statistics (for <code>max</code>, <code>min</code>, <code>avg</code>, and <code>watermark</code> event-time watermark statistics)</p> </li> <li> <p><code>WatermarkTracker</code> is requested to updateWatermark</p> </li> </ul>","text":"","location":"physical-operators/EventTimeWatermarkExec/#eventtimestats-accumulator"},{"title":"Executing Physical Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> is part of the <code>SparkPlan</code> (Spark SQL) abstraction.</p> <p><code>doExecute</code> executes the child physical operator and maps over the partitions (using <code>RDD.mapPartitions</code>).</p> <p><code>doExecute</code> creates an unsafe projection (per partition) for the column with the event time in the output schema of the child physical operator. The unsafe projection is to extract event times from the (stream of) internal rows of the child physical operator.</p> <p>For every row in a partition, <code>doExecute</code> requests the eventTimeStats accumulator to accumulate the event time.</p>  <p>Note</p> <p>The event time value is in seconds (not millis as the value is divided by <code>1000</code> ).</p>","text":"","location":"physical-operators/EventTimeWatermarkExec/#executing-physical-operator"},{"title":"Output Attributes <pre><code>output: Seq[Attribute]\n</code></pre> <p><code>output</code> is part of the <code>QueryPlan</code> (Spark SQL) abstraction.</p> <p><code>output</code> requests the child physical operator for the output attributes to find the event time attribute and any other column with metadata that contains spark.watermarkDelayMs key.</p> <p>For the event time attribute, <code>output</code> updates the metadata to include the delay interval for the spark.watermarkDelayMs key.</p> <p>For any other column (not the event time attribute) with the spark.watermarkDelayMs key, <code>output</code> removes the key from the attribute metadata.</p>","text":"","location":"physical-operators/EventTimeWatermarkExec/#output-attributes"},{"title":"Demo <p>Check out Demo: Streaming Watermark with Aggregation in Append Output Mode to deep dive into the internals of Streaming Watermark.</p>","text":"","location":"physical-operators/EventTimeWatermarkExec/#demo"},{"title":"FlatMapGroupsWithStateExec Physical Operator","text":"<p><code>FlatMapGroupsWithStateExec</code> is a unary physical operator (Spark SQL) that represents FlatMapGroupsWithState logical operator at execution time.</p> <p><code>FlatMapGroupsWithStateExec</code> is an <code>ObjectProducerExec</code> (Spark SQL) physical operator that produces a single output object.</p>  <p>Tip</p> <p>Check out Demo: Internals of FlatMapGroupsWithStateExec Physical Operator.</p>   <p>Note</p> <p><code>FlatMapGroupsWithStateExec</code> is given an OutputMode when created, but it does not seem to be used at all. Check out the question What's the purpose of OutputMode in flatMapGroupsWithState? How/where is it used? on StackOverflow.</p>","location":"physical-operators/FlatMapGroupsWithStateExec/"},{"title":"Creating Instance","text":"<p><code>FlatMapGroupsWithStateExec</code> takes the following to be created:</p> <ul> <li> User-defined state function that is applied to every group (of type <code>(Any, Iterator[Any], LogicalGroupState[Any]) =&gt; Iterator[Any]</code>) <li> Deserializer expression for keys <li> Deserializer expression for values <li> Grouping attributes (as used for grouping in KeyValueGroupedDataset for <code>mapGroupsWithState</code> or <code>flatMapGroupsWithState</code> operators) <li> Data attributes <li> Output object attribute (that is the reference to the single object field this operator outputs) <li> Optional StatefulOperatorStateInfo <li> State encoder (<code>ExpressionEncoder[Any]</code>) <li> State format version <li> OutputMode <li> GroupStateTimeout <li> Optional Batch Processing Time <li> Optional Event-Time Watermark <li> Child physical operator  <p><code>FlatMapGroupsWithStateExec</code> is created when FlatMapGroupsWithStateStrategy execution planning strategy is executed (and plans a FlatMapGroupsWithState logical operator for execution).</p>","location":"physical-operators/FlatMapGroupsWithStateExec/#creating-instance"},{"title":"Executing Physical Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> first initializes the metrics (which happens on the driver).</p> <p><code>doExecute</code> then requests the child physical operator to execute (and generate an <code>RDD[InternalRow]</code>).</p> <p><code>doExecute</code> uses StateStoreOps to create a StateStoreRDD with a <code>storeUpdateFunction</code> that does the following (for a partition):</p> <ol> <li> <p>Creates an InputProcessor for a given StateStore</p> </li> <li> <p>(only when the GroupStateTimeout is EventTimeTimeout) Filters out late data based on the event-time watermark, i.e. rows from a given <code>Iterator[InternalRow]</code> that are older than the event-time watermark are excluded from the steps that follow</p> </li> <li> <p>Requests the <code>InputProcessor</code> to create an iterator of a new data processed from the (possibly filtered) iterator</p> </li> <li> <p>Requests the <code>InputProcessor</code> to create an iterator of a timed-out state data</p> </li> <li> <p>Creates an iterator by concatenating the above iterators (with the new data processed first)</p> </li> <li> <p>In the end, creates a <code>CompletionIterator</code> that executes a completion function (<code>completionFunction</code>) after it has successfully iterated through all the elements (i.e. when a client has consumed all the rows). The completion method requests the given <code>StateStore</code> to commit changes followed by setting the store-specific metrics</p> </li> </ol> <p><code>doExecute</code> is part of Spark SQL's <code>SparkPlan</code> abstraction.</p>","text":"","location":"physical-operators/FlatMapGroupsWithStateExec/#executing-physical-operator"},{"title":"Performance Metrics <p><code>FlatMapGroupsWithStateExec</code> uses the performance metrics of StateStoreWriter.</p> <p></p>","text":"","location":"physical-operators/FlatMapGroupsWithStateExec/#performance-metrics"},{"title":"StateStoreWriter <p><code>FlatMapGroupsWithStateExec</code> is a stateful physical operator that can write to a state store (and <code>MicroBatchExecution</code> requests whether to run another batch or not based on the GroupStateTimeout).</p> <p><code>FlatMapGroupsWithStateExec</code> uses the GroupStateTimeout (and possibly the updated metadata) when asked whether to run another batch or not (when <code>MicroBatchExecution</code> is requested to construct the next streaming micro-batch when requested to run the activated streaming query).</p>","text":"","location":"physical-operators/FlatMapGroupsWithStateExec/#statestorewriter"},{"title":"Streaming Event-Time Watermark Support <p><code>FlatMapGroupsWithStateExec</code> is a physical operator that supports streaming event-time watermark.</p> <p><code>FlatMapGroupsWithStateExec</code> is given the optional event time watermark when created.</p> <p>The event-time watermark is initially undefined (<code>None</code>) when planned for execution (in FlatMapGroupsWithStateStrategy execution planning strategy).</p>  <p>Note</p> <p><code>FlatMapGroupsWithStateStrategy</code> converts FlatMapGroupsWithState unary logical operator to <code>FlatMapGroupsWithStateExec</code> physical operator with undefined StatefulOperatorStateInfo, batchTimestampMs, and eventTimeWatermark.</p>  <p>The event-time watermark (with the StatefulOperatorStateInfo and the batchTimestampMs) is only defined to the current event-time watermark of the given OffsetSeqMetadata when <code>IncrementalExecution</code> query execution pipeline is requested to apply the state preparation rule (as part of the preparations rules).</p>  <p>Note</p> <p>The preparations rules are executed (applied to a physical query plan) at the <code>executedPlan</code> phase of Structured Query Execution Pipeline to generate an optimized physical query plan ready for execution).</p> <p>Read up on Structured Query Execution Pipeline in The Internals of Spark SQL online book.</p>  <p><code>IncrementalExecution</code> is used as the lastExecution of the available streaming query execution engines. It is created in the queryPlanning phase (of the MicroBatchExecution and ContinuousExecution execution engines) based on the current OffsetSeqMetadata.</p>  <p>Note</p> <p>The optional event-time watermark can only be defined when the state preparation rule is executed which is at the <code>executedPlan</code> phase of Structured Query Execution Pipeline which is also part of the queryPlanning phase.</p>","text":"","location":"physical-operators/FlatMapGroupsWithStateExec/#streaming-event-time-watermark-support"},{"title":"StateManager <pre><code>stateManager: StateManager\n</code></pre> <p>While being created, <code>FlatMapGroupsWithStateExec</code> creates a StateManager (with the state encoder and the isTimeoutEnabled flag).</p> <p>A <code>StateManager</code> is created per state format version that is given while creating a <code>FlatMapGroupsWithStateExec</code> (to choose between the available implementations).</p> <p>The state format version is controlled by spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion internal configuration property.</p> <p>The <code>StateManager</code> is used exclusively when <code>FlatMapGroupsWithStateExec</code> physical operator is executed for the following:</p> <ul> <li> <p>State schema (for the value schema of a StateStoreRDD)</p> </li> <li> <p>State data for a key in a StateStore while processing new data</p> </li> <li> <p>All state data (for all keys) in a StateStore while processing timed-out state data</p> </li> <li> <p>Removing the state for a key from a StateStore when all rows have been processed</p> </li> <li> <p>Persisting the state for a key in a StateStore when all rows have been processed</p> </li> </ul>","text":"","location":"physical-operators/FlatMapGroupsWithStateExec/#statemanager"},{"title":"keyExpressions Method <pre><code>keyExpressions: Seq[Attribute]\n</code></pre> <p><code>keyExpressions</code> simply returns the grouping attributes.</p> <p><code>keyExpressions</code> is part of the WatermarkSupport abstraction.</p>","text":"","location":"physical-operators/FlatMapGroupsWithStateExec/#keyexpressions-method"},{"title":"Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not <pre><code>shouldRunAnotherBatch(\n  newMetadata: OffsetSeqMetadata): Boolean\n</code></pre> <p><code>shouldRunAnotherBatch</code> uses the GroupStateTimeout as follows:</p> <ul> <li> <p>With EventTimeTimeout, <code>shouldRunAnotherBatch</code> is <code>true</code> only when the event-time watermark is defined and is older (below) the event-time watermark of the given <code>OffsetSeqMetadata</code></p> </li> <li> <p>With NoTimeout (and other GroupStateTimeouts if there were any), <code>shouldRunAnotherBatch</code> is always <code>false</code></p> </li> <li> <p>With ProcessingTimeTimeout, <code>shouldRunAnotherBatch</code> is always <code>true</code></p> </li> </ul> <p><code>shouldRunAnotherBatch</code> is part of the StateStoreWriter abstraction.</p>","text":"","location":"physical-operators/FlatMapGroupsWithStateExec/#checking-out-whether-last-batch-execution-requires-another-non-data-batch-or-not"},{"title":"Internal Properties","text":"","location":"physical-operators/FlatMapGroupsWithStateExec/#internal-properties"},{"title":"isTimeoutEnabled Flag <p>Flag that says whether the GroupStateTimeout is not NoTimeout</p> <p>Used when:</p> <ul> <li><code>FlatMapGroupsWithStateExec</code> is created (and creates the internal StateManager)</li> <li><code>InputProcessor</code> is requested to processTimedOutState</li> </ul>","text":"","location":"physical-operators/FlatMapGroupsWithStateExec/#istimeoutenabled-flag"},{"title":"watermarkPresent Flag <p>Flag that says whether the child physical operator has a watermark attribute (among the output attributes).</p> <p>Used when:</p> <ul> <li><code>InputProcessor</code> is requested to callFunctionAndUpdateState</li> </ul>","text":"","location":"physical-operators/FlatMapGroupsWithStateExec/#watermarkpresent-flag"},{"title":"Required Child Output Distribution <pre><code>requiredChildDistribution: Seq[Distribution]\n</code></pre> <p><code>requiredChildDistribution</code> is part of the <code>SparkPlan</code> (Spark SQL) abstraction.</p>  <p><code>requiredChildDistribution</code>...FIXME</p>","text":"","location":"physical-operators/FlatMapGroupsWithStateExec/#required-child-output-distribution"},{"title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec=ALL\n</code></pre> <p>Refer to Logging.</p>","text":"","location":"physical-operators/FlatMapGroupsWithStateExec/#logging"},{"title":"MicroBatchScanExec Physical Operator","text":"<p><code>MicroBatchScanExec</code> is a <code>DataSourceV2ScanExecBase</code> (Spark SQL) that represents StreamingDataSourceV2Relation logical operator at execution.</p> <p><code>MicroBatchScanExec</code> is just a very thin wrapper over MicroBatchStream and does nothing but delegates all the important execution-specific preparation to it.</p>","location":"physical-operators/MicroBatchScanExec/"},{"title":"Creating Instance","text":"<p><code>MicroBatchScanExec</code> takes the following to be created:</p> <ul> <li> Output <code>Attribute</code>s (Spark SQL) <li> <code>Scan</code> (Spark SQL) <li>MicroBatchStream</li> <li> Start Offset <li> End Offset <li> Key-Grouped Partitioning  <p><code>MicroBatchScanExec</code> is created when:</p> <ul> <li><code>DataSourceV2Strategy</code> (Spark SQL) execution planning strategy is requested to plan a logical query plan with a StreamingDataSourceV2Relation</li> </ul>  <p>Note</p> <p>All the properties to create a MicroBatchScanExec are copied straight from a StreamingDataSourceV2Relation directly.</p>","location":"physical-operators/MicroBatchScanExec/#creating-instance"},{"title":"MicroBatchStream <p><code>MicroBatchScanExec</code> is given a MicroBatchStream when created.</p> <p>The <code>MicroBatchStream</code> is the SparkDataStream of the StreamingDataSourceV2Relation logical operator (it was created from).</p> <p><code>MicroBatchScanExec</code> uses the <code>MicroBatchStream</code> to handle inputPartitions, readerFactory and inputRDD (that are all the custom overloaded methods of <code>MicroBatchScanExec</code>).</p>","text":"","location":"physical-operators/MicroBatchScanExec/#microbatchstream"},{"title":"Input Partitions <pre><code>inputPartitions: Seq[InputPartition]\n</code></pre> <p><code>inputPartitions</code> is part of the <code>DataSourceV2ScanExecBase</code> (Spark SQL) abstraction.</p>  <p><code>inputPartitions</code> requests the MicroBatchStream to planInputPartitions for the start and end offsets.</p>","text":"","location":"physical-operators/MicroBatchScanExec/#input-partitions"},{"title":"PartitionReaderFactory <pre><code>readerFactory: PartitionReaderFactory\n</code></pre> <p><code>readerFactory</code> is part of the <code>DataSourceV2ScanExecBase</code> (Spark SQL) abstraction.</p>  <p><code>readerFactory</code> requests the MicroBatchStream to createReaderFactory.</p>","text":"","location":"physical-operators/MicroBatchScanExec/#partitionreaderfactory"},{"title":"Input RDD <pre><code>inputRDD: RDD[InternalRow]\n</code></pre> <p><code>inputRDD</code> is part of the <code>DataSourceV2ScanExecBase</code> (Spark SQL) abstraction.</p>  <p><code>inputRDD</code> creates a <code>DataSourceRDD</code> (Spark SQL) for the partitions and the PartitionReaderFactory (and the others).</p>","text":"","location":"physical-operators/MicroBatchScanExec/#input-rdd"},{"title":"SessionWindowStateStoreRestoreExec Physical Operator","text":"","location":"physical-operators/SessionWindowStateStoreRestoreExec/"},{"title":"Required Child Output Distribution <pre><code>requiredChildDistribution: Seq[Distribution]\n</code></pre> <p><code>requiredChildDistribution</code> is part of the <code>SparkPlan</code> (Spark SQL) abstraction.</p>  <p><code>requiredChildDistribution</code>...FIXME</p>","text":"","location":"physical-operators/SessionWindowStateStoreRestoreExec/#required-child-output-distribution"},{"title":"SessionWindowStateStoreSaveExec Physical Operator","text":"","location":"physical-operators/SessionWindowStateStoreSaveExec/"},{"title":"Required Child Output Distribution <pre><code>requiredChildDistribution: Seq[Distribution]\n</code></pre> <p><code>requiredChildDistribution</code> is part of the <code>SparkPlan</code> (Spark SQL) abstraction.</p>  <p><code>requiredChildDistribution</code>...FIXME</p>","text":"","location":"physical-operators/SessionWindowStateStoreSaveExec/#required-child-output-distribution"},{"title":"StateStoreReader","text":"<p>== [[StateStoreReader]] StateStoreReader</p> <p><code>StateStoreReader</code> is...FIXME</p>","location":"physical-operators/StateStoreReader/"},{"title":"StateStoreRestoreExec Physical Operator","text":"<p><code>StateStoreRestoreExec</code> is a unary physical operator (Spark SQL) to restore (read) a streaming state (from a state store) (for the keys from the child physical operator).</p> <p><code>StateStoreRestoreExec</code> is among the physical operators used to execute streaming aggregations.</p> <p></p>","location":"physical-operators/StateStoreRestoreExec/"},{"title":"Creating Instance","text":"<p><code>StateStoreRestoreExec</code> takes the following to be created:</p> <ul> <li> Grouping Key <code>Attribute</code>s (Spark SQL) <li>StatefulOperatorStateInfo</li> <li> spark.sql.streaming.aggregation.stateFormatVersion <li> Child Physical Operator (Spark SQL)  <p><code>StateStoreRestoreExec</code> is created when:</p> <ul> <li><code>StatefulAggregationStrategy</code> execution planning strategy is requested to plan a streaming aggregation</li> <li><code>IncrementalExecution</code> is created</li> </ul>","location":"physical-operators/StateStoreRestoreExec/#creating-instance"},{"title":"StatefulOperatorStateInfo <p><code>StateStoreRestoreExec</code> can be given a StatefulOperatorStateInfo when created.</p> <p>The <code>StatefulOperatorStateInfo</code> is initially undefined when <code>StateStoreRestoreExec</code> is created.</p> <p>The <code>StatefulOperatorStateInfo</code> is specified (so this <code>StateStoreRestoreExec</code> gets a streaming batch-specific execution property) when <code>IncrementalExecution</code> is requested to prepare a streaming physical plan for execution (and state preparation rule is executed when <code>StreamExecution</code> plans a streaming query for a streaming batch).</p> <p></p>","text":"","location":"physical-operators/StateStoreRestoreExec/#statefuloperatorstateinfo"},{"title":"Performance Metrics","text":"<p></p>","location":"physical-operators/StateStoreRestoreExec/#performance-metrics"},{"title":"number of output rows <p>The number of input rows from the child physical operator this <code>StateStoreRestoreExec</code> found the state value for when doExecute</p>  <p>FIXME number of output rows</p> <p>The number of output rows metric seems to be always an even number of the <code>restoredRow</code> from a state store and the <code>row</code> itself (from the child physical operator).</p>","text":"","location":"physical-operators/StateStoreRestoreExec/#number-of-output-rows"},{"title":"StreamingAggregationStateManager <p><code>StateStoreRestoreExec</code> creates a StreamingAggregationStateManager when created.</p> <p>The <code>StreamingAggregationStateManager</code> is created using the grouping key expressions and the output schema of the child physical operator.</p> <p>The <code>StreamingAggregationStateManager</code> is used in doExecute for the following:</p> <ul> <li>State Value Schema to mapPartitionsWithReadStateStore</li> <li>Extract a key (from an input row) and get the value (for the key) for every input row</li> </ul>","text":"","location":"physical-operators/StateStoreRestoreExec/#streamingaggregationstatemanager"},{"title":"Required Child Output Distribution <pre><code>requiredChildDistribution: Seq[Distribution]\n</code></pre> <p><code>requiredChildDistribution</code> is part of the <code>SparkPlan</code> (Spark SQL) abstraction.</p>  <p><code>requiredChildDistribution</code>...FIXME</p>","text":"","location":"physical-operators/StateStoreRestoreExec/#required-child-output-distribution"},{"title":"Executing Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> is part of the <code>SparkPlan</code> (Spark SQL) abstraction.</p>  <p><code>doExecute</code> executes the child operator and creates a StateStoreRDD with <code>storeUpdateFunction</code> that does the following per partition:</p> <ol> <li> <p>Generates an unsafe projection to access the key field (using keyExpressions and the output schema of child operator).</p> </li> <li> <p>For every input row (as <code>InternalRow</code>)</p> <ul> <li>Extracts the key from the row (using the unsafe projection above)</li> <li>Gets the saved state in <code>StateStore</code> for the key if available (it might not be if the key appeared in the input the first time)</li> <li>Increments numOutputRows metric</li> <li>Generates collection made up of the current row and possibly the state for the key if available</li> </ul> </li> </ol>","text":"","location":"physical-operators/StateStoreRestoreExec/#executing-operator"},{"title":"StateStoreSaveExec Physical Operator","text":"<p><code>StateStoreSaveExec</code> is a unary physical operator (Spark SQL) that saves (writes) a streaming state (to a state store) with support for streaming watermark.</p> <p><code>StateStoreSaveExec</code> is among the physical operators used to execute streaming aggregations.</p> <p></p>","location":"physical-operators/StateStoreSaveExec/"},{"title":"Creating Instance","text":"<p><code>StateStoreSaveExec</code> takes the following to be created:</p> <ul> <li> Grouping Key <code>Attribute</code>s (Spark SQL) <li> StatefulOperatorStateInfo <li> OutputMode <li> Event-time Watermark <li> spark.sql.streaming.aggregation.stateFormatVersion <li> Child Physical Operator (Spark SQL)  <p><code>StateStoreSaveExec</code> is created when:</p> <ul> <li><code>StatefulAggregationStrategy</code> execution planning strategy is requested to plan a streaming aggregation</li> <li><code>IncrementalExecution</code> is created</li> </ul>","location":"physical-operators/StateStoreSaveExec/#creating-instance"},{"title":"StreamingAggregationStateManager <p><code>StateStoreSaveExec</code> creates a StreamingAggregationStateManager when created.</p> <p>The <code>StreamingAggregationStateManager</code> is created using the grouping key expressions and the output schema of the child physical operator.</p> <p>The <code>StreamingAggregationStateManager</code> is used in doExecute for the following:</p> <ul> <li>State Value Schema to mapPartitionsWithStateStore</li> </ul> <p>For Complete:</p> <ol> <li>Store all input rows</li> <li>Commit the state changes</li> <li>Fetch the values</li> </ol> <p>For Append:</p> <ol> <li>Store all updated rows</li> <li>Fetch all the key-value pairs</li> <li>Remove old \"watermarked\" aggregates for all removed pairs</li> <li>Commit the state changes</li> </ol> <p>For Update:</p> <ol> <li>Store all updated rows</li> <li>removeKeysOlderThanWatermark of old \"watermarked\" aggregates</li> <li>Commit the state changes</li> </ol>","text":"","location":"physical-operators/StateStoreSaveExec/#streamingaggregationstatemanager"},{"title":"Required Child Output Distribution <pre><code>requiredChildDistribution: Seq[Distribution]\n</code></pre> <p><code>requiredChildDistribution</code> is part of the <code>SparkPlan</code> (Spark SQL) abstraction.</p>  <p><code>requiredChildDistribution</code>...FIXME</p>","text":"","location":"physical-operators/StateStoreSaveExec/#required-child-output-distribution"},{"title":"Performance Metrics <p><code>StateStoreSaveExec</code> uses the performance metrics as the other stateful physical operators that write to a state store.</p> <p></p>","text":"","location":"physical-operators/StateStoreSaveExec/#performance-metrics"},{"title":"total time to update rows <p>Time taken to read the input rows and store them in a state store (possibly filtering out expired \"watermarked\" rows per watermarkPredicateForData predicate)</p> <p>The number of rows stored is the number of updated state rows metric</p> <ul> <li> <p>For Append output mode, the time taken to filter out expired rows (per the required watermarkPredicateForData predicate) and the StreamingAggregationStateManager to store rows in a state store</p> </li> <li> <p>For Complete output mode, the time taken to go over all the input rows and request the StreamingAggregationStateManager to store rows in a state store</p> </li> <li> <p>For Update output mode, the time taken to filter out expired rows (per the optional watermarkPredicateForData predicate) and the StreamingAggregationStateManager to store rows in a state store</p> </li> </ul>","text":"","location":"physical-operators/StateStoreSaveExec/#total-time-to-update-rows"},{"title":"total time to remove rows <ul> <li> <p>For Append output mode, the time taken for the StreamingAggregationStateManager to remove all expired entries from a state store (per watermarkPredicateForKeys predicate) that is the total time of iterating over all entries in the state store (the number of entries removed from a state store is the difference between the number of output rows of the child operator and the number of total state rows metric)</p> </li> <li> <p>For Complete output mode, always <code>0</code></p> </li> <li> <p>For Update output mode, the time taken for the StreamingAggregationStateManager to remove all expired entries from a state store (per watermarkPredicateForKeys predicate)</p> </li> </ul>","text":"","location":"physical-operators/StateStoreSaveExec/#total-time-to-remove-rows"},{"title":"time to commit changes <p>Time taken for the StreamingAggregationStateManager to commit changes to a state store</p>","text":"","location":"physical-operators/StateStoreSaveExec/#time-to-commit-changes"},{"title":"number of output rows <ul> <li> <p>For Append output mode, the metric does not seem to be used</p> </li> <li> <p>For Complete output mode, the number of rows in a StateStore (i.e. all values in a StateStore in the StreamingAggregationStateManager that should be equivalent to the number of total state rows metric)</p> </li> <li> <p>For Update output mode, the number of rows that the StreamingAggregationStateManager was requested to store in a state store (that did not expire per the optional watermarkPredicateForData predicate) that is equivalent to the number of updated state rows metric)</p> </li> </ul>","text":"","location":"physical-operators/StateStoreSaveExec/#number-of-output-rows"},{"title":"number of total state rows <p>Number of entries in a state store at the very end of executing the StateStoreSaveExec operator (aka numTotalStateRows)</p> <p>Corresponds to <code>numRowsTotal</code> attribute in <code>stateOperators</code> in StreamingQueryProgress (and is available as <code>sq.lastProgress.stateOperators</code> for an operator).</p>","text":"","location":"physical-operators/StateStoreSaveExec/#number-of-total-state-rows"},{"title":"number of updated state rows <p>Number of the entries that were stored as updates in a state store in a trigger and for the keys in the result rows of the upstream physical operator (aka numUpdatedStateRows)</p> <ul> <li> <p>For Append output mode, the number of input rows that have not expired yet (per the required watermarkPredicateForData predicate) and that the StreamingAggregationStateManager was requested to store in a state store (the time taken is the total time to update rows metric)</p> </li> <li> <p>For Complete output mode, the number of input rows (which should be exactly the number of output rows from the child operator)</p> </li> <li> <p>For Update output mode, the number of rows that the StreamingAggregationStateManager was requested to store in a state store (that did not expire per the optional watermarkPredicateForData predicate) that is equivalent to the number of output rows metric)</p> </li> </ul> <p>Corresponds to <code>numRowsUpdated</code> attribute in <code>stateOperators</code> in StreamingQueryProgress (and is available as <code>sq.lastProgress.stateOperators</code> for an operator).</p>","text":"","location":"physical-operators/StateStoreSaveExec/#number-of-updated-state-rows"},{"title":"memory used by state <p>Estimated memory used by a StateStore (aka stateMemory) after <code>StateStoreSaveExec</code> finished execution (per the StateStoreMetrics of the StateStore)</p>","text":"","location":"physical-operators/StateStoreSaveExec/#memory-used-by-state"},{"title":"Review Me <p>The optional properties, i.e. the &lt;&gt;, the &lt;&gt;, and the &lt;&gt;, are initially undefined when <code>StateStoreSaveExec</code> is &lt;&gt;. <code>StateStoreSaveExec</code> is updated to hold execution-specific configuration when <code>IncrementalExecution</code> is requested to prepare the logical plan (of a streaming query) for execution (when the state preparation rule is executed). <p></p>  <p>Note</p> <p>Unlike StateStoreRestoreExec operator, <code>StateStoreSaveExec</code> takes output mode and event time watermark when created.</p>  <p>When &lt;&gt;, <code>StateStoreSaveExec</code> creates a StateStoreRDD to map over partitions with <code>storeUpdateFunction</code> that manages the <code>StateStore</code>. <p></p> <p></p>  <p>Note</p> <p>The number of partitions of StateStoreRDD (and hence the number of Spark tasks) is what was defined for the &lt;&gt; physical plan. <p>There will be that many <code>StateStores</code> as there are partitions in <code>StateStoreRDD</code>.</p>  <p>When &lt;&gt;, <code>StateStoreSaveExec</code> executes the &lt;&gt; physical operator and creates a StateStoreRDD (with <code>storeUpdateFunction</code> specific to the output mode). <p>[[stateManager]] <code>StateStoreRestoreExec</code> uses a StreamingAggregationStateManager (that is created for the keyExpressions, the output of the child physical operator and the stateFormatVersion).</p> <p>[[logging]] [TIP] ==== Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.streaming.StateStoreSaveExec</code> to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.streaming.StateStoreSaveExec=ALL\n</code></pre>","text":"","location":"physical-operators/StateStoreSaveExec/#review-me"},{"title":"Refer to &lt;&gt;.","text":"","location":"physical-operators/StateStoreSaveExec/#refer-to"},{"title":"Executing Physical Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> is part of the <code>SparkPlan</code> abstraction (Spark SQL).</p> <p>Internally, <code>doExecute</code> initializes metrics.</p> <p>NOTE: <code>doExecute</code> requires that the optional &lt;&gt; is at this point defined (that should have happened when <code>IncrementalExecution</code> had prepared a streaming aggregation for execution). <p><code>doExecute</code> executes &lt;&gt; physical operator and creates a StateStoreRDD with <code>storeUpdateFunction</code> that: <ol> <li> <p>Generates an unsafe projection to access the key field (using &lt;&gt; and the output schema of &lt;&gt;).  <li> <p>Branches off per &lt;&gt;: &lt;&gt;, &lt;&gt; and &lt;&gt;.   <p><code>doExecute</code> throws an <code>UnsupportedOperationException</code> when executed with an invalid &lt;&gt;: <pre><code>Invalid output mode: [outputMode]\n</code></pre> <p>==== [[doExecute-Append]] Append Output Mode</p> <p>NOTE: Append is the default output mode when not specified explicitly.</p> <p>NOTE: <code>Append</code> output mode requires that a streaming query defines event-time watermark (e.g. using withWatermark operator) on the event-time column that is used in aggregation (directly or using window standard function).</p> <p>For Append output mode, <code>doExecute</code> does the following:</p> <ol> <li> <p>Finds late (aggregate) rows from &lt;&gt; physical operator (that have expired per watermark)  <li> <p>Stores the late rows in the state store and increments the &lt;&gt; metric  <li> <p>Gets all the added (late) rows from the state store</p> </li> <li> <p>Creates an iterator that removes the late rows from the state store when requested the next row and in the end commits the state updates</p> </li>  <p>TIP: Refer to &lt;&gt; for an example of <code>StateStoreSaveExec</code> with <code>Append</code> output mode. <p>CAUTION: FIXME When is \"Filtering state store on:\" printed out?</p>  <ol> <li> <p>Uses watermarkPredicateForData predicate to exclude matching rows and (like in Complete output mode) stores all the remaining rows in <code>StateStore</code>.</p> </li> <li> <p>(like in &lt;&gt; output mode) While storing the rows, increments &lt;&gt; metric (for every row) and records the total time in &lt;&gt; metric.  <li> <p>Takes all the rows from <code>StateStore</code> and returns a <code>NextIterator</code> that:</p> </li> <li> <p>In <code>getNext</code>, finds the first row that matches watermarkPredicateForKeys predicate, removes it from <code>StateStore</code>, and returns it back. + If no row was found, <code>getNext</code> also marks the iterator as finished.</p> </li> <li> <p>In <code>close</code>, records the time to iterate over all the rows in &lt;&gt; metric, commits the updates to <code>StateStore</code> followed by recording the time in &lt;&gt; metric and recording StateStore metrics.","text":"","location":"physical-operators/StateStoreSaveExec/#executing-physical-operator"},{"title":"Complete Output Mode <p>For Complete output mode, <code>doExecute</code> does the following:</p> <ol> <li> <p>Takes all <code>UnsafeRow</code> rows (from the parent iterator)</p> </li> <li> <p>Stores the rows by key in the state store eagerly (i.e. all rows that are available in the parent iterator before proceeding)</p> </li> <li> <p>Commits the state updates</p> </li> <li> <p>In the end, reads the key-row pairs from the state store and passes the rows along (i.e. to the following physical operator)</p> </li> </ol> <p>The number of keys stored in the state store is recorded in &lt;&gt; metric. <p>NOTE: In <code>Complete</code> output mode the &lt;&gt; metric is exactly the &lt;&gt; metric. <p>TIP: Refer to &lt;&gt; for an example of <code>StateStoreSaveExec</code> with <code>Complete</code> output mode.  <ol> <li> <p>Stores all rows (as <code>UnsafeRow</code>) in <code>StateStore</code>.</p> </li> <li> <p>While storing the rows, increments &lt;&gt; metric (for every row) and records the total time in &lt;&gt; metric.  <li> <p>Records <code>0</code> in &lt;&gt; metric.  <li> <p>Commits the state updates to <code>StateStore</code> and records the time in &lt;&gt; metric.  <li> <p>Records StateStore metrics</p> </li> <li> <p>In the end, takes all the rows stored in <code>StateStore</code> and increments numOutputRows metric.</p> </li>","text":"","location":"physical-operators/StateStoreSaveExec/#complete-output-mode"},{"title":"Update Output Mode <p>For Update output mode, <code>doExecute</code> returns an iterator that filters out late aggregate rows (per watermark if defined) and stores the \"young\" rows in the state store (one by one, i.e. every <code>next</code>).</p> <p>With no more rows available, that removes the late rows from the state store (all at once) and commits the state updates.</p> <p>TIP: Refer to &lt;&gt; for an example of <code>StateStoreSaveExec</code> with <code>Update</code> output mode.  <p><code>doExecute</code> returns <code>Iterator</code> of rows that uses watermarkPredicateForData predicate to filter out late rows.</p> <p>In <code>hasNext</code>, when rows are no longer available:</p> <ol> <li> <p>Records the total time to iterate over all the rows in &lt;&gt; metric.  <li> <p>removeKeysOlderThanWatermark and records the time in &lt;&gt; metric.  <li> <p>Commits the updates to <code>StateStore</code> and records the time in &lt;&gt; metric.  <li> <p>Records StateStore metrics</p> </li>  <p>In <code>next</code>, stores a row in <code>StateStore</code> and increments numOutputRows and numUpdatedStateRows metrics.</p> <p>=== [[shouldRunAnotherBatch]] Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not -- <code>shouldRunAnotherBatch</code> Method</p> <pre><code>shouldRunAnotherBatch(\n  newMetadata: OffsetSeqMetadata): Boolean\n</code></pre> <p><code>shouldRunAnotherBatch</code> is positive (<code>true</code>) when all of the following are met:</p> <ul> <li> <p>&lt;&gt; is either Append or Update  <li> <p>&lt;&gt; is defined and is older (below) the current event-time watermark (of the given <code>OffsetSeqMetadata</code>)   <p>Otherwise, <code>shouldRunAnotherBatch</code> is negative (<code>false</code>).</p> <p><code>shouldRunAnotherBatch</code> is part of the StateStoreWriter abstraction.</p>","text":"","location":"physical-operators/StateStoreSaveExec/#update-output-mode"},{"title":"StateStoreWriter Physical Operators","text":"<p><code>StateStoreWriter</code> is an extension of the StatefulOperator abstraction for stateful physical operators that write to a state store and collect the write metrics for execution progress reporting.</p>","location":"physical-operators/StateStoreWriter/"},{"title":"Implementations","text":"<ul> <li>FlatMapGroupsWithStateExec</li> <li>StateStoreSaveExec</li> <li>StreamingDeduplicateExec</li> <li>StreamingGlobalLimitExec</li> <li>StreamingSymmetricHashJoinExec</li> </ul>","location":"physical-operators/StateStoreWriter/#implementations"},{"title":"Performance Metrics    ID Name     statefulOperatorCustomMetrics    numOutputRows number of output rows   numRowsDroppedByWatermark number of rows which are dropped by watermark   numTotalStateRows number of total state rows   numUpdatedStateRows number of updated state rows   allUpdatesTimeMs time to update   numRemovedStateRows number of removed state rows   allRemovalsTimeMs time to remove   commitTimeMs time to commit changes   stateMemory memory used by state   numShufflePartitions number of shuffle partitions   numStateStoreInstances number of state store instances   stateStoreCustomMetrics","text":"","location":"physical-operators/StateStoreWriter/#performance-metrics"},{"title":"numStateStoreInstances <p>Updated in setOperatorMetrics</p> <p>Reported as numStateStoreInstances when reporting progress</p>","text":"","location":"physical-operators/StateStoreWriter/#numstatestoreinstances"},{"title":"numTotalStateRows <p>Sum of the number of keys of all state stores</p> <p>Updated in setStoreMetrics based on the numKeys metric of a StateStore.</p> <p>Reported as numRowsTotal when reporting progress</p>","text":"","location":"physical-operators/StateStoreWriter/#numtotalstaterows"},{"title":"Short Name <pre><code>shortName: String\n</code></pre> <p><code>shortName</code> is <code>defaultName</code> (and is expected to be overriden by the implementations).</p>  <p><code>shortName</code> is used when:</p> <ul> <li><code>StateStoreWriter</code> is requested for a StateOperatorProgress</li> </ul>","text":"","location":"physical-operators/StateStoreWriter/#short-name"},{"title":"Custom Metrics <pre><code>customStatefulOperatorMetrics: Seq[StatefulOperatorCustomMetric]\n</code></pre> <p><code>customStatefulOperatorMetrics</code> is empty (and is expected to be overriden by the implementations).</p>  <p>StreamingDeduplicateExec</p> <p>StreamingDeduplicateExec physical operator is the only implementation with custom metrics.</p>   <p><code>customStatefulOperatorMetrics</code> is used when:</p> <ul> <li><code>StateStoreWriter</code> is requested for the custom metrics of this stateful operator</li> </ul>","text":"","location":"physical-operators/StateStoreWriter/#custom-metrics"},{"title":"Reporting Progress <pre><code>getProgress(): StateOperatorProgress\n</code></pre> <p><code>getProgress</code> collects the current values of the custom metrics (stateStoreCustomMetrics and statefulOperatorCustomMetrics).</p> <p><code>getProgress</code> creates a StateOperatorProgress with the shortName and the following metrics:</p>    Property Metric     numRowsTotal numTotalStateRows   numRowsUpdated numUpdatedStateRows   allUpdatesTimeMs allUpdatesTimeMs   numRowsRemoved numRemovedStateRows   allRemovalsTimeMs allRemovalsTimeMs   commitTimeMs commitTimeMs   memoryUsedBytes stateMemory   numRowsDroppedByWatermark numRowsDroppedByWatermark   numShufflePartitions numShufflePartitions   numStateStoreInstances numStateStoreInstances   customMetrics stateStoreCustomMetrics and statefulOperatorCustomMetrics     <p><code>getProgress</code> is used when:</p> <ul> <li><code>ProgressReporter</code> is requested to extractStateOperatorMetrics (when <code>MicroBatchExecution</code> is requested to run the activated streaming query)</li> </ul>","text":"","location":"physical-operators/StateStoreWriter/#reporting-progress"},{"title":"Does Last Batch Execution Require Extra Non-Data Batch <pre><code>shouldRunAnotherBatch(\n  newMetadata: OffsetSeqMetadata): Boolean\n</code></pre> <p><code>shouldRunAnotherBatch</code> is negative (<code>false</code>) by default (to indicate that another non-data batch is not required given the OffsetSeqMetadata with the event-time watermark and the batch timestamp).</p> <p><code>shouldRunAnotherBatch</code> is used when <code>IncrementalExecution</code> is requested to check out whether the last batch execution requires another batch (when <code>MicroBatchExecution</code> is requested to run the activated streaming query).</p>","text":"","location":"physical-operators/StateStoreWriter/#does-last-batch-execution-require-extra-non-data-batch"},{"title":"Custom Metrics","text":"","location":"physical-operators/StateStoreWriter/#custom-metrics_1"},{"title":"Stateful Operator <pre><code>statefulOperatorCustomMetrics: Map[String, SQLMetric]\n</code></pre> <p><code>statefulOperatorCustomMetrics</code> is the customStatefulOperatorMetrics.</p>  <p><code>statefulOperatorCustomMetrics</code> is used when:</p> <ul> <li><code>StateStoreWriter</code> is requested for the metrics and a progress</li> </ul>","text":"","location":"physical-operators/StateStoreWriter/#stateful-operator"},{"title":"StateStore <pre><code>stateStoreCustomMetrics: Map[String, SQLMetric]\n</code></pre> <p><code>stateStoreCustomMetrics</code> creates a StateStoreProvider (based on spark.sql.streaming.stateStore.providerClass).</p> <p><code>stateStoreCustomMetrics</code> requests the <code>StateStoreProvider</code> for supportedCustomMetrics.</p>  <p><code>stateStoreCustomMetrics</code> is used when:</p> <ul> <li><code>StateStoreWriter</code> is requested for the metrics and a progress</li> </ul>","text":"","location":"physical-operators/StateStoreWriter/#statestore"},{"title":"Recording Metrics","text":"","location":"physical-operators/StateStoreWriter/#recording-metrics"},{"title":"Stateful Operator <pre><code>setOperatorMetrics(\n  numStateStoreInstances: Int = 1): Unit\n</code></pre> <p><code>setOperatorMetrics</code> updates the following metrics:</p> <ul> <li>Increments numShufflePartitions</li> <li>Adds the given <code>numStateStoreInstances</code> to numStateStoreInstances metric</li> </ul>  <p><code>setOperatorMetrics</code> is used when the following physical operators are executed:</p> <ul> <li>FlatMapGroupsWithStateExec</li> <li>StateStoreSaveExec</li> <li>SessionWindowStateStoreSaveExec</li> <li>StreamingDeduplicateExec</li> <li>StreamingGlobalLimitExec</li> <li>StreamingSymmetricHashJoinExec</li> </ul>","text":"","location":"physical-operators/StateStoreWriter/#stateful-operator_1"},{"title":"StateStore <pre><code>setStoreMetrics(\n  store: StateStore): Unit\n</code></pre> <p><code>setStoreMetrics</code> requests the given StateStore for the metrics and records the following metrics:</p> <ul> <li>Adds the number of keys to numTotalStateRows metric</li> <li>Adds the memory used (in bytes) to stateMemory metric</li> </ul> <p><code>setStoreMetrics</code> records (adds) the values of the custom metrics.</p>  <p><code>setStoreMetrics</code> is used when the following physical operators are executed:</p> <ul> <li>FlatMapGroupsWithStateExec</li> <li>StateStoreSaveExec</li> <li>SessionWindowStateStoreSaveExec</li> <li>StreamingDeduplicateExec</li> <li>StreamingGlobalLimitExec</li> </ul>","text":"","location":"physical-operators/StateStoreWriter/#statestore_1"},{"title":"StatefulOpClusteredDistribution","text":"<p><code>StatefulOpClusteredDistribution</code> is a <code>Distribution</code> (Spark SQL).</p> <p><code>StatefulOpClusteredDistribution</code> requires the Expressions are specified or throws an exception:</p> <pre><code>The expressions for hash of a StatefulOpClusteredDistribution should not be Nil.\nAn AllTuples should be used to represent a distribution that only has a single partition.\n</code></pre>","location":"physical-operators/StatefulOpClusteredDistribution/"},{"title":"Creating Instance","text":"<p><code>StatefulOpClusteredDistribution</code> takes the following to be created:</p> <ul> <li> <code>Expression</code>s (Spark SQL) <li>Required number of partitions</li>  <p><code>StatefulOpClusteredDistribution</code> is created when:</p> <ul> <li><code>StatefulOperatorPartitioning</code> is requested to getCompatibleDistribution</li> <li><code>StreamingSymmetricHashJoinExec</code> is requested for the required child output distribution</li> </ul>","location":"physical-operators/StatefulOpClusteredDistribution/#creating-instance"},{"title":"Required Number of Partitions <p><code>StatefulOpClusteredDistribution</code> is given a required number of partitions when created.</p> <p><code>requiredNumPartitions</code> is part of the <code>Distribution</code> (Spark SQL) abstraction.</p>","text":"","location":"physical-operators/StatefulOpClusteredDistribution/#required-number-of-partitions"},{"title":"Partitioning <pre><code>createPartitioning(\n  numPartitions: Int): Partitioning\n</code></pre> <p><code>createPartitioning</code> is part of the <code>Distribution</code> (Spark SQL) abstraction.</p>  <p><code>createPartitioning</code> asserts that the given <code>numPartitions</code> is exactly the required number of partitions or throws an exception otherwise:</p> <pre><code>This StatefulOpClusteredDistribution requires [requiredNumPartitions] partitions,\nbut the actual number of partitions is [numPartitions].\n</code></pre> <p><code>createPartitioning</code> creates a <code>HashPartitioning</code> (Spark SQL) (with the expressions and the numPartitions).</p>","text":"","location":"physical-operators/StatefulOpClusteredDistribution/#partitioning"},{"title":"StatefulOperator Physical Operators","text":"<p><code>StatefulOperator</code> is the &lt;&gt; of &lt;&gt; that &lt;&gt; or &lt;&gt; state (described by &lt;&gt;). <p>[[contract]] .StatefulOperator Contract [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| stateInfo a| [[stateInfo]]</p>","location":"physical-operators/StatefulOperator/"},{"title":"[source, scala]","text":"","location":"physical-operators/StatefulOperator/#source-scala"},{"title":"stateInfo: Option[StatefulOperatorStateInfo]","text":"<p>The StatefulOperatorStateInfo of the physical operator |===</p> <p>[[extensions]] .StatefulOperators (Direct Implementations) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | StatefulOperator | Description</p> <p>| &lt;&gt; | [[StateStoreReader]] <p>| StateStoreWriter | [[StateStoreWriter]] Physical operator that writes to a state store and collects the write metrics for execution progress reporting |===</p>","location":"physical-operators/StatefulOperator/#stateinfo-optionstatefuloperatorstateinfo"},{"title":"StreamingDeduplicateExec Physical Operator","text":"<p><code>StreamingDeduplicateExec</code> is a unary physical operator (Spark SQL) that writes state to StateStore with support for streaming watermark.</p>","location":"physical-operators/StreamingDeduplicateExec/"},{"title":"Required Child Output Distribution <pre><code>requiredChildDistribution: Seq[Distribution]\n</code></pre> <p><code>requiredChildDistribution</code> is part of the <code>SparkPlan</code> (Spark SQL) abstraction.</p>  <p><code>requiredChildDistribution</code>...FIXME</p>","text":"","location":"physical-operators/StreamingDeduplicateExec/#required-child-output-distribution"},{"title":"Review Me <p><code>StreamingDeduplicateExec</code> is &lt;&gt; exclusively when StreamingDeduplicationStrategy execution planning strategy is executed (to plan Deduplicate unary logical operators). <p></p> <pre><code>val uniqueValues = spark.\n  readStream.\n  format(\"rate\").\n  load.\n  dropDuplicates(\"value\")  // &lt;-- creates Deduplicate logical operator\n\nscala&gt; println(uniqueValues.queryExecution.logical.numberedTreeString)\n00 Deduplicate [value#214L], true\n01 +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4785f176,rate,List(),None,List(),None,Map(),None), rate, [timestamp#213, value#214L]\n\nscala&gt; uniqueValues.explain\n== Physical Plan ==\nStreamingDeduplicate [value#214L], StatefulOperatorStateInfo(&lt;unknown&gt;,5a65879c-67bc-4e77-b417-6100db6a52a2,0,0), 0\n+- Exchange hashpartitioning(value#214L, 200)\n   +- StreamingRelation rate, [timestamp#213, value#214L]\n\n// Start the query and hence StreamingDeduplicateExec\nimport scala.concurrent.duration._\nimport org.apache.spark.sql.streaming.{OutputMode, Trigger}\nval sq = uniqueValues.\n  writeStream.\n  format(\"console\").\n  option(\"truncate\", false).\n  trigger(Trigger.ProcessingTime(10.seconds)).\n  outputMode(OutputMode.Update).\n  start\n\n// sorting not supported for non-aggregate queries\n// and so values are unsorted\n\n-------------------------------------------\nBatch: 0\n-------------------------------------------\n+---------+-----+\n|timestamp|value|\n+---------+-----+\n+---------+-----+\n\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+-----------------------+-----+\n|timestamp              |value|\n+-----------------------+-----+\n|2017-07-25 22:12:03.018|0    |\n|2017-07-25 22:12:08.018|5    |\n|2017-07-25 22:12:04.018|1    |\n|2017-07-25 22:12:06.018|3    |\n|2017-07-25 22:12:05.018|2    |\n|2017-07-25 22:12:07.018|4    |\n+-----------------------+-----+\n\n-------------------------------------------\nBatch: 2\n-------------------------------------------\n+-----------------------+-----+\n|timestamp              |value|\n+-----------------------+-----+\n|2017-07-25 22:12:10.018|7    |\n|2017-07-25 22:12:09.018|6    |\n|2017-07-25 22:12:12.018|9    |\n|2017-07-25 22:12:13.018|10   |\n|2017-07-25 22:12:15.018|12   |\n|2017-07-25 22:12:11.018|8    |\n|2017-07-25 22:12:14.018|11   |\n|2017-07-25 22:12:16.018|13   |\n|2017-07-25 22:12:17.018|14   |\n|2017-07-25 22:12:18.018|15   |\n+-----------------------+-----+\n\n// Eventually...\nsq.stop\n</code></pre> <p>[[metrics]] <code>StreamingDeduplicateExec</code> uses the performance metrics of StateStoreWriter.</p> <p>.StreamingDeduplicateExec in web UI (Details for Query) image::images/StreamingDeduplicateExec-webui-query-details.png[align=\"center\"]</p> <p>[[output]] The output schema of <code>StreamingDeduplicateExec</code> is exactly the &lt;&gt;'s output schema. <p>[[outputPartitioning]] The output partitioning of <code>StreamingDeduplicateExec</code> is exactly the &lt;&gt;'s output partitioning.","text":"","location":"physical-operators/StreamingDeduplicateExec/#review-me"},{"title":"[source, scala] <p>/** // Start spark-shell with debugging and Kafka support   SPARK_SUBMIT_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005\" \\   ./bin/spark-shell \\   --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0-SNAPSHOT */ // Reading val topic1 = spark.   readStream.   format(\"kafka\").   option(\"subscribe\", \"topic1\").   option(\"kafka.bootstrap.servers\", \"localhost:9092\").   option(\"startingOffsets\", \"earliest\").   load</p> <p>// Processing with deduplication // Don't use watermark // The following won't work due to https://issues.apache.org/jira/browse/SPARK-21546 /** val records = topic1.   withColumn(\"eventtime\", 'timestamp).  // \u2190 just to put the right name given the purpose   withWatermark(eventTime = \"eventtime\", delayThreshold = \"30 seconds\"). // \u2190 use the renamed eventtime column   dropDuplicates(\"value\").  // dropDuplicates will use watermark                             // only when eventTime column exists   // include the watermark column =&gt; internal design leak?   select('key cast \"string\", 'value cast \"string\", 'eventtime).   as[(String, String, java.sql.Timestamp)] */</p> <p>val records = topic1.   dropDuplicates(\"value\").   select('key cast \"string\", 'value cast \"string\").   as[(String, String)]</p> <p>scala&gt; records.explain == Physical Plan == *Project [cast(key#0 as string) AS key#249, cast(value#1 as string) AS value#250] +- StreamingDeduplicate [value#1], StatefulOperatorStateInfo(,68198b93-6184-49ae-8098-006c32cc6192,0,0), 0    +- Exchange hashpartitioning(value#1, 200)       +- *Project [key#0, value#1]          +- StreamingRelation kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6] <p>// Writing import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val sq = records.   writeStream.   format(\"console\").   option(\"truncate\", false).   trigger(Trigger.ProcessingTime(10.seconds)).   queryName(\"from-kafka-topic1-to-console\").   outputMode(OutputMode.Update).   start</p> <p>// Eventually... sq.stop</p>","text":"","location":"physical-operators/StreamingDeduplicateExec/#source-scala"},{"title":"[TIP]","text":"<p>Enable <code>INFO</code> logging level for <code>org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec</code> to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec=INFO\n</code></pre>","location":"physical-operators/StreamingDeduplicateExec/#tip"},{"title":"Refer to spark-sql-streaming-spark-logging.md[Logging].","text":"<p>=== [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- <code>doExecute</code> Method</p>","location":"physical-operators/StreamingDeduplicateExec/#refer-to-spark-sql-streaming-spark-loggingmdlogging"},{"title":"[source, scala]","text":"","location":"physical-operators/StreamingDeduplicateExec/#source-scala_1"},{"title":"doExecute(): RDD[InternalRow] <p>NOTE: <code>doExecute</code> is part of <code>SparkPlan</code> Contract to generate the runtime representation of an physical operator as a distributed computation over internal binary rows on Apache Spark (i.e. <code>RDD[InternalRow]</code>).</p> <p>Internally, <code>doExecute</code> initializes metrics.</p> <p><code>doExecute</code> executes &lt;&gt; physical operator and creates a StateStoreRDD with <code>storeUpdateFunction</code> that: <ol> <li> <p>Generates an unsafe projection to access the key field (using &lt;&gt; and the output schema of &lt;&gt;).  <li> <p>Filters out rows from <code>Iterator[InternalRow]</code> that match <code>watermarkPredicateForData</code> (when defined and &lt;&gt; is <code>EventTimeTimeout</code>)  <li> <p>For every row (as <code>InternalRow</code>)</p> </li> <li> <p>Extracts the key from the row (using the unsafe projection above)</p> </li> <li> <p>Gets the saved state in <code>StateStore</code> for the key</p> </li> <li> <p>(when there was a state for the key in the row) Filters out (aka drops) the row</p> </li> <li> <p>(when there was no state for the key in the row) Stores a new (and empty) state for the key and increments &lt;&gt; and &lt;&gt; metrics.  <li> <p>In the end, <code>storeUpdateFunction</code> creates a <code>CompletionIterator</code> that executes a completion function (aka <code>completionFunction</code>) after it has successfully iterated through all the elements (i.e. when a client has consumed all the rows). + The completion function does the following:</p> </li> <li> <p>Updates &lt;&gt; metric (that is the total time to execute <code>storeUpdateFunction</code>)  <li> <p>Updates &lt;&gt; metric with the time taken to remove keys older than the watermark from the StateStore  <li> <p>Updates &lt;&gt; metric with the time taken to commit the changes to the StateStore  <li> <p>Sets StateStore-specific metrics</p> </li>","text":"","location":"physical-operators/StreamingDeduplicateExec/#doexecute-rddinternalrow"},{"title":"Creating Instance <p><code>StreamingDeduplicateExec</code> takes the following when created:</p> <ul> <li>[[keyExpressions]] Duplicate keys (as used in dropDuplicates operator)</li> <li>[[child]] Child physical operator (<code>SparkPlan</code>)</li> <li>[[stateInfo]] StatefulOperatorStateInfo</li> <li>[[eventTimeWatermark]] Event-time watermark</li> </ul>","text":"","location":"physical-operators/StreamingDeduplicateExec/#creating-instance"},{"title":"Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not <pre><code>shouldRunAnotherBatch(\n  newMetadata: OffsetSeqMetadata): Boolean\n</code></pre> <p><code>shouldRunAnotherBatch</code>...FIXME</p> <p><code>shouldRunAnotherBatch</code> is part of the StateStoreWriter abstraction.</p>","text":"","location":"physical-operators/StreamingDeduplicateExec/#checking-out-whether-last-batch-execution-requires-another-non-data-batch-or-not"},{"title":"StreamingGlobalLimitExec Physical Operator","text":"<p><code>StreamingGlobalLimitExec</code> is a unary physical operator that represents a <code>Limit</code> (Spark SQL) logical operator of a streaming query at execution time.</p> <p><code>StreamingGlobalLimitExec</code> is a &lt;&gt;. <p><code>StreamingGlobalLimitExec</code> supports &lt;&gt; output mode only. <p>The optional properties, i.e. the &lt;&gt; and the &lt;&gt;, are initially undefined when <code>StreamingGlobalLimitExec</code> is &lt;&gt;. <code>StreamingGlobalLimitExec</code> is updated to hold execution-specific configuration when <code>IncrementalExecution</code> is requested to prepare the logical plan (of a streaming query) for execution (when the state preparation rule is executed).","location":"physical-operators/StreamingGlobalLimitExec/"},{"title":"Creating Instance","text":"<p><code>StreamingGlobalLimitExec</code> takes the following to be created:</p> <ul> <li>[[streamLimit]] Streaming Limit</li> <li>[[child]] Child physical operator (<code>SparkPlan</code>)</li> <li>[[stateInfo]] StatefulOperatorStateInfo (default: <code>None</code>)</li> <li>[[outputMode]] OutputMode (default: <code>None</code>)</li> </ul> <p><code>StreamingGlobalLimitExec</code> is created when StreamingGlobalLimitStrategy execution planning strategy is requested to plan a <code>Limit</code> logical operator (in the logical plan of a streaming query) for execution.</p> <p>=== [[StateStoreWriter]] StreamingGlobalLimitExec as StateStoreWriter</p> <p><code>StreamingGlobalLimitExec</code> is a stateful physical operator that can write to a state store.</p> <p>=== [[metrics]] Performance Metrics</p> <p><code>StreamingGlobalLimitExec</code> uses the performance metrics of the parent StateStoreWriter.</p> <p>=== [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- <code>doExecute</code> Method</p>","location":"physical-operators/StreamingGlobalLimitExec/#creating-instance"},{"title":"[source, scala]","text":"","location":"physical-operators/StreamingGlobalLimitExec/#source-scala"},{"title":"doExecute(): RDD[InternalRow]","text":"<p>NOTE: <code>doExecute</code> is part of <code>SparkPlan</code> Contract to generate the runtime representation of an physical operator as a recipe for distributed computation over internal binary rows on Apache Spark (<code>RDD[InternalRow]</code>).</p> <p><code>doExecute</code>...FIXME</p> <p>=== [[internal-properties]] Internal Properties</p> <p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| keySchema a| [[keySchema]] FIXME</p> <p>Used when...FIXME</p> <p>| valueSchema a| [[valueSchema]] FIXME</p> <p>Used when...FIXME</p> <p>|===</p>","location":"physical-operators/StreamingGlobalLimitExec/#doexecute-rddinternalrow"},{"title":"StreamingRelationExec Leaf Physical Operator","text":"<p><code>StreamingRelationExec</code> is a leaf physical operator (i.e. <code>LeafExecNode</code>) that...FIXME</p>","location":"physical-operators/StreamingRelationExec/"},{"title":"Creating Instance","text":"<p><code>StreamingRelationExec</code> takes the following when created:</p> <ul> <li>[[sourceName]] The name of a streaming source</li> <li>[[output]] Output attributes</li> </ul> <p><code>StreamingRelationExec</code> is created when StreamingRelationStrategy execution planning strategy is executed (to plan <code>StreamingRelation</code> and <code>StreamingExecutionRelation</code> logical operators).</p>","location":"physical-operators/StreamingRelationExec/#creating-instance"},{"title":"Demo","text":"<pre><code>scala&gt; spark.version\nres0: String = 2.3.0-SNAPSHOT\n\nval rates = spark.\n  readStream.\n  format(\"rate\").\n  load\n\n// StreamingRelation logical operator\nscala&gt; println(rates.queryExecution.logical.numberedTreeString)\n00 StreamingRelation DataSource(org.apache.spark.sql.SparkSession@31ba0af0,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L]\n\n// StreamingRelationExec physical operator (shown without \"Exec\" suffix)\nscala&gt; rates.explain\n== Physical Plan ==\nStreamingRelation rate, [timestamp#0, value#1L]\n</code></pre>","location":"physical-operators/StreamingRelationExec/#demo"},{"title":"StreamingSymmetricHashJoinExec Physical Operator","text":"<p><code>StreamingSymmetricHashJoinExec</code> is a binary physical operator (Spark SQL) for stream-stream equi-join at execution time.</p>","location":"physical-operators/StreamingSymmetricHashJoinExec/"},{"title":"Required Child Output Distribution <pre><code>requiredChildDistribution: Seq[Distribution]\n</code></pre> <p><code>requiredChildDistribution</code> is part of the <code>SparkPlan</code> (Spark SQL) abstraction.</p>  <p><code>requiredChildDistribution</code> is two StatefulOpClusteredDistributions for the left and right keys (with the numPartitions of the StatefulOperatorStateInfo).</p>","text":"","location":"physical-operators/StreamingSymmetricHashJoinExec/#required-child-output-distribution"},{"title":"Review Me <p>[[supported-join-types]][[joinType]] <code>StreamingSymmetricHashJoinExec</code> supports <code>Inner</code>, <code>LeftOuter</code>, and <code>RightOuter</code> join types (with the &lt;&gt; and the &lt;&gt; keys using the exact same data types). <p><code>StreamingSymmetricHashJoinExec</code> is &lt;&gt; exclusively when StreamingJoinStrategy execution planning strategy is requested to plan a logical query plan with a <code>Join</code> logical operator of two streaming queries with equality predicates (<code>EqualTo</code> and <code>EqualNullSafe</code>). <p><code>StreamingSymmetricHashJoinExec</code> is given execution-specific configuration (i.e. &lt;&gt;, &lt;&gt;, and &lt;&gt;) when <code>IncrementalExecution</code> is requested to plan a streaming query for execution (and uses the state preparation rule). <p><code>StreamingSymmetricHashJoinExec</code> uses two OneSideHashJoiners (for the &lt;&gt; and &lt;&gt; sides of the join) to manage join state when &lt;&gt;. <p><code>StreamingSymmetricHashJoinExec</code> is a stateful physical operator that writes to a state store.</p>","text":"","location":"physical-operators/StreamingSymmetricHashJoinExec/#review-me"},{"title":"Creating Instance <p><code>StreamingSymmetricHashJoinExec</code> takes the following to be created:</p> <ul> <li>[[leftKeys]] Left keys (Catalyst expressions of the keys on the left side)</li> <li>[[rightKeys]] Right keys (Catalyst expressions of the keys on the right side)</li> <li>Join type</li> <li>[[condition]] Join condition (<code>JoinConditionSplitPredicates</code>)</li> <li>[[stateInfo]] StatefulOperatorStateInfo</li> <li>Event-Time Watermark</li> <li>Watermark Predicates for State Removal</li> <li>[[left]] Physical operator on the left side (<code>SparkPlan</code>)</li> <li>[[right]] Physical operator on the right side (<code>SparkPlan</code>)</li> </ul> <p><code>StreamingSymmetricHashJoinExec</code> initializes the &lt;&gt;. <p>=== [[output]] Output Schema -- <code>output</code> Method</p>","text":"","location":"physical-operators/StreamingSymmetricHashJoinExec/#creating-instance"},{"title":"[source, scala]","text":"","location":"physical-operators/StreamingSymmetricHashJoinExec/#source-scala"},{"title":"output: Seq[Attribute] <p>NOTE: <code>output</code> is part of the <code>QueryPlan</code> Contract to describe the attributes of (the schema of) the output.</p> <p><code>output</code> schema depends on the &lt;&gt;: <ul> <li> <p>For <code>Cross</code> and <code>Inner</code> (<code>InnerLike</code>) joins, it is the output schema of the &lt;&gt; and &lt;&gt; operators  <li> <p>For <code>LeftOuter</code> joins, it is the output schema of the &lt;&gt; operator with the attributes of the &lt;&gt; operator with <code>nullability</code> flag enabled (<code>true</code>)  <li> <p>For <code>RightOuter</code> joins, it is the output schema of the &lt;&gt; operator with the attributes of the &lt;&gt; operator with <code>nullability</code> flag enabled (<code>true</code>)   <p><code>output</code> throws an <code>IllegalArgumentException</code> for other join types:</p> <pre><code>[className] should not take [joinType] as the JoinType\n</code></pre> <p>=== [[outputPartitioning]] Output Partitioning -- <code>outputPartitioning</code> Method</p>","text":"","location":"physical-operators/StreamingSymmetricHashJoinExec/#output-seqattribute"},{"title":"[source, scala]","text":"","location":"physical-operators/StreamingSymmetricHashJoinExec/#source-scala_1"},{"title":"outputPartitioning: Partitioning <p>NOTE: <code>outputPartitioning</code> is part of the <code>SparkPlan</code> Contract to specify how data should be partitioned across different nodes in the cluster.</p> <p><code>outputPartitioning</code> depends on the &lt;&gt;: <ul> <li> <p>For <code>Cross</code> and <code>Inner</code> (<code>InnerLike</code>) joins, it is a <code>PartitioningCollection</code> of the output partitioning of the &lt;&gt; and &lt;&gt; operators  <li> <p>For <code>LeftOuter</code> joins, it is a <code>PartitioningCollection</code> of the output partitioning of the &lt;&gt; operator  <li> <p>For <code>RightOuter</code> joins, it is a <code>PartitioningCollection</code> of the output partitioning of the &lt;&gt; operator   <p><code>outputPartitioning</code> throws an <code>IllegalArgumentException</code> for other join types:</p> <pre><code>[className] should not take [joinType] as the JoinType\n</code></pre> <p>=== [[eventTimeWatermark]] Event-Time Watermark -- <code>eventTimeWatermark</code> Internal Property</p>","text":"","location":"physical-operators/StreamingSymmetricHashJoinExec/#outputpartitioning-partitioning"},{"title":"[source, scala]","text":"","location":"physical-operators/StreamingSymmetricHashJoinExec/#source-scala_2"},{"title":"eventTimeWatermark: Option[Long] <p>When &lt;&gt;, <code>StreamingSymmetricHashJoinExec</code> can be given the event-time watermark of the current streaming micro-batch. <p><code>eventTimeWatermark</code> is an optional property that is specified only after IncrementalExecution was requested to apply the state preparation rule to a physical query plan of a streaming query (to optimize (prepare) the physical plan of the streaming query once for ContinuousExecution and every trigger for MicroBatchExecution in the queryPlanning phase).</p> <p><code>eventTimeWatermark</code> is used when:</p> <ul> <li><code>StreamingSymmetricHashJoinExec</code> is requested to check out whether the last batch execution requires another non-data batch or not</li> <li><code>OneSideHashJoiner</code> is requested to storeAndJoinWithOtherSide</li> </ul>","text":"","location":"physical-operators/StreamingSymmetricHashJoinExec/#eventtimewatermark-optionlong"},{"title":"Watermark Predicates for State Removal <pre><code>stateWatermarkPredicates: JoinStateWatermarkPredicates\n</code></pre> <p>When &lt;&gt;, <code>StreamingSymmetricHashJoinExec</code> is given a &lt;&gt; for the &lt;&gt; and &lt;&gt; join sides (using the StreamingSymmetricHashJoinHelper utility). <p><code>stateWatermarkPredicates</code> contains the left and right predicates only when IncrementalExecution is requested to apply the state preparation rule to a physical query plan of a streaming query (to optimize (prepare) the physical plan of the streaming query once for ContinuousExecution and every trigger for MicroBatchExecution in the queryPlanning phase).</p> <p><code>stateWatermarkPredicates</code> is used when <code>StreamingSymmetricHashJoinExec</code> is requested for the following:</p> <ul> <li> <p>Process partitions of the left and right sides of the stream-stream join (and creating OneSideHashJoiners)</p> </li> <li> <p>Checking out whether the last batch execution requires another non-data batch or not</p> </li> </ul> <p>=== [[metrics]] Performance Metrics (SQLMetrics)</p> <p><code>StreamingSymmetricHashJoinExec</code> uses the performance metrics as other stateful physical operators that write to a state store.</p> <p></p> <p>The following table shows how the performance metrics are computed (and so their exact meaning).</p> <p>[cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Name (in web UI) | Description</p> <p>| total time to update rows a| [[allUpdatesTimeMs]] Processing time of all rows</p> <p>| total time to remove rows a| [[allRemovalsTimeMs]]</p> <p>| time to commit changes a| [[commitTimeMs]]</p> <p>| number of output rows a| [[numOutputRows]] Total number of output rows</p> <p>| number of total state rows a| [[numTotalStateRows]]</p> <p>| number of updated state rows a| [[numUpdatedStateRows]] Number of updated state rows of the left and right <code>OneSideHashJoiners</code></p> <p>| memory used by state a| [[stateMemory]] |===</p> <p>=== [[shouldRunAnotherBatch]] Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not -- <code>shouldRunAnotherBatch</code> Method</p>","text":"","location":"physical-operators/StreamingSymmetricHashJoinExec/#watermark-predicates-for-state-removal"},{"title":"[source, scala] <p>shouldRunAnotherBatch(   newMetadata: OffsetSeqMetadata): Boolean</p>  <p><code>shouldRunAnotherBatch</code> is positive (<code>true</code>) when all of the following are positive:</p> <ul> <li> <p>Either the &lt;&gt; or &lt;&gt; join state watermark predicates are defined (in the &lt;&gt;)  <li> <p>&lt;&gt; threshold (of the <code>StreamingSymmetricHashJoinExec</code> operator) is defined and the current event-time watermark threshold of the given <code>OffsetSeqMetadata</code> is above (greater than) it, i.e. moved above   <p><code>shouldRunAnotherBatch</code> is negative (<code>false</code>) otherwise.</p> <p><code>shouldRunAnotherBatch</code> is part of the StateStoreWriter abstraction.</p>","text":"","location":"physical-operators/StreamingSymmetricHashJoinExec/#source-scala_3"},{"title":"Executing Physical Operator <pre><code>doExecute(): RDD[InternalRow]\n</code></pre> <p><code>doExecute</code> is part of the <code>SparkPlan</code> abstraction (Spark SQL).</p> <p><code>doExecute</code> first requests the <code>StreamingQueryManager</code> for the StateStoreCoordinatorRef to the <code>StateStoreCoordinator</code> RPC endpoint (for the driver).</p> <p><code>doExecute</code> then uses <code>SymmetricHashJoinStateManager</code> utility to get the names of the state stores for the left and right sides of the streaming join.</p> <p>In the end, <code>doExecute</code> requests the &lt;&gt; and &lt;&gt; child physical operators to execute (generate an RDD) and then &lt;&gt; with &lt;&gt; (and with the <code>StateStoreCoordinatorRef</code> and the state stores). <p>=== [[processPartitions]] Processing Partitions of Left and Right Sides of Stream-Stream Join -- <code>processPartitions</code> Internal Method</p>","text":"","location":"physical-operators/StreamingSymmetricHashJoinExec/#executing-physical-operator"},{"title":"[source, scala] <p>processPartitions(   leftInputIter: Iterator[InternalRow],   rightInputIter: Iterator[InternalRow]): Iterator[InternalRow]</p>  <p>[[processPartitions-updateStartTimeNs]] <code>processPartitions</code> records the current time (as updateStartTimeNs for the &lt;&gt; performance metric in &lt;&gt;). <p>[[processPartitions-postJoinFilter]] <code>processPartitions</code> creates a new predicate (postJoinFilter) based on the <code>bothSides</code> of the &lt;&gt; if defined or <code>true</code> literal. <p>[[processPartitions-leftSideJoiner]] <code>processPartitions</code> creates a OneSideHashJoiner for the LeftSide and all other properties for the left-hand join side (<code>leftSideJoiner</code>).</p> <p>[[processPartitions-rightSideJoiner]] <code>processPartitions</code> creates a OneSideHashJoiner for the RightSide and all other properties for the right-hand join side (<code>rightSideJoiner</code>).</p> <p>[[processPartitions-leftOutputIter]][[processPartitions-rightOutputIter]] <code>processPartitions</code> requests the <code>OneSideHashJoiner</code> for the left-hand join side to storeAndJoinWithOtherSide with the right-hand side one (that creates a <code>leftOutputIter</code> row iterator) and the <code>OneSideHashJoiner</code> for the right-hand join side to do the same with the left-hand side one (and creates a <code>rightOutputIter</code> row iterator).</p> <p>[[processPartitions-innerOutputCompletionTimeNs]] <code>processPartitions</code> records the current time (as innerOutputCompletionTimeNs for the &lt;&gt; performance metric in &lt;&gt;). <p>[[processPartitions-innerOutputIter]] <code>processPartitions</code> creates a <code>CompletionIterator</code> with the left and right output iterators (with the rows of the <code>leftOutputIter</code> first followed by <code>rightOutputIter</code>). When no rows are left to process, the <code>CompletionIterator</code> records the completion time.</p> <p>[[processPartitions-outputIter]] <code>processPartitions</code> creates a join-specific output <code>Iterator[InternalRow]</code> of the output rows based on the &lt;&gt; (of the <code>StreamingSymmetricHashJoinExec</code>): <ul> <li> <p>For <code>Inner</code> joins, <code>processPartitions</code> simply uses the &lt;&gt;  <li> <p>For <code>LeftOuter</code> joins, <code>processPartitions</code>...</p> </li> <li> <p>For <code>RightOuter</code> joins, <code>processPartitions</code>...</p> </li> <li> <p>For other joins, <code>processPartitions</code> simply throws an <code>IllegalArgumentException</code>.</p> </li>  <p>[[processPartitions-outputIterWithMetrics]] <code>processPartitions</code> creates an <code>UnsafeProjection</code> for the &lt;&gt; (and the output of the &lt;&gt; and &lt;&gt; child operators) that counts all the rows of the &lt;&gt; (as the &lt;&gt; metric) and generate an output projection. <p>In the end, <code>processPartitions</code> returns a <code>CompletionIterator</code> with with the &lt;&gt; and &lt;&gt; completion function. <p>NOTE: <code>processPartitions</code> is used exclusively when <code>StreamingSymmetricHashJoinExec</code> physical operator is requested to &lt;&gt;. <p>==== [[processPartitions-onOutputCompletion]][[onOutputCompletion]] Calculating Performance Metrics (Output Completion Callback) -- <code>onOutputCompletion</code> Internal Method</p>","text":"","location":"physical-operators/StreamingSymmetricHashJoinExec/#source-scala_4"},{"title":"[source, scala]","text":"","location":"physical-operators/StreamingSymmetricHashJoinExec/#source-scala_5"},{"title":"onOutputCompletion: Unit <p><code>onOutputCompletion</code> calculates the &lt;&gt; performance metric (that is the time since the &lt;&gt; was executed). <p><code>onOutputCompletion</code> adds the time for the inner join to complete (since &lt;&gt; time marker) to the &lt;&gt; performance metric. <p><code>onOutputCompletion</code> records the time to remove old state (per the join state watermark predicate for the &lt;&gt; and the &lt;&gt; streaming queries) and adds it to the &lt;&gt; performance metric. <p>NOTE: <code>onOutputCompletion</code> triggers the old state removal eagerly by iterating over the state rows to be deleted.</p> <p><code>onOutputCompletion</code> records the time for the &lt;&gt; and &lt;&gt; <code>OneSideHashJoiners</code> to commit any state changes that becomes the &lt;&gt; performance metric. <p><code>onOutputCompletion</code> calculates the &lt;&gt; performance metric (as the number of updated state rows of the &lt;&gt; and &lt;&gt; streaming queries). <p><code>onOutputCompletion</code> calculates the &lt;&gt; performance metric (as the sum of the number of keys in the KeyWithIndexToValueStore of the &lt;&gt; and &lt;&gt; streaming queries). <p><code>onOutputCompletion</code> calculates the &lt;&gt; performance metric (as the sum of the memory used by the KeyToNumValuesStore and KeyWithIndexToValueStore of the &lt;&gt; and &lt;&gt; streams). <p>In the end, <code>onOutputCompletion</code> calculates the custom metrics.</p>","text":"","location":"physical-operators/StreamingSymmetricHashJoinExec/#onoutputcompletion-unit"},{"title":"Internal Properties <p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| hadoopConfBcast a| [[hadoopConfBcast]] Hadoop Configuration broadcast (to the Spark cluster)</p> <p>Used exclusively to &lt;&gt; <p>| joinStateManager a| [[joinStateManager]] SymmetricHashJoinStateManager</p> <p>Used when <code>OneSideHashJoiner</code> is requested to storeAndJoinWithOtherSide, removeOldState, commitStateAndGetMetrics, and for the values for a given key</p> <p>| nullLeft a| [[nullLeft]] <code>GenericInternalRow</code> of the size of the output schema of the &lt;&gt; <p>| nullRight a| [[nullRight]] <code>GenericInternalRow</code> of the size of the output schema of the &lt;&gt; <p>| storeConf a| [[storeConf]] StateStoreConf</p> <p>Used exclusively to &lt;&gt; <p>|===</p>","text":"","location":"physical-operators/StreamingSymmetricHashJoinExec/#internal-properties"},{"title":"WatermarkSupport Unary Physical Operators","text":"<p><code>WatermarkSupport</code> is the &lt;&gt; of unary physical operators (<code>UnaryExecNode</code>) with support for streaming event-time watermark.","location":"physical-operators/WatermarkSupport/"},{"title":"[NOTE]","text":"<p>Watermark (aka \"allowed lateness\") is a moving threshold of event time and specifies what data to consider for aggregations, i.e. the threshold of late data so the engine can automatically drop incoming late data given event time and clean up old state accordingly.</p>","location":"physical-operators/WatermarkSupport/#note"},{"title":"Read the official documentation of Spark in http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking[Handling Late Data and Watermarking].","text":"<p>[[properties]] .WatermarkSupport's (Lazily-Initialized) Properties [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Property | Description</p> <p>| [[watermarkExpression]] <code>watermarkExpression</code> a| Optional Catalyst expression that matches rows older than the event time watermark.</p>  <p>Note</p> <p>Use withWatermark operator to specify streaming watermark.</p>   <p>When initialized, <code>watermarkExpression</code> finds spark.watermarkDelayMs watermark attribute in the child output's metadata.</p> <p>If found, <code>watermarkExpression</code> creates <code>evictionExpression</code> with the watermark attribute that is less than or equal &lt;&gt;. <p>The watermark attribute may be of type <code>StructType</code>. If it is, <code>watermarkExpression</code> uses the first field as the watermark.</p> <p><code>watermarkExpression</code> prints out the following INFO message to the logs when spark.watermarkDelayMs watermark attribute is found.</p> <pre><code>INFO [physicalOperator]Exec: Filtering state store on: [evictionExpression]\n</code></pre> <p>NOTE: <code>physicalOperator</code> can be FlatMapGroupsWithStateExec, StateStoreSaveExec.md[StateStoreSaveExec] or StreamingDeduplicateExec.</p> <p>TIP: Enable INFO logging level for one of the stateful physical operators to see the INFO message in the logs.</p> <p>| [[watermarkPredicateForData]] <code>watermarkPredicateForData</code> | Optional <code>Predicate</code> that uses &lt;&gt; and the child output to match rows older than the event-time watermark <p>| [[watermarkPredicateForKeys]] <code>watermarkPredicateForKeys</code> | Optional <code>Predicate</code> that uses &lt;&gt; to match rows older than the event time watermark. |=== <p>=== [[contract]] WatermarkSupport Contract</p>","location":"physical-operators/WatermarkSupport/#read-the-official-documentation-of-spark-in-httpsparkapacheorgdocslateststructured-streaming-programming-guidehtmlhandling-late-data-and-watermarkinghandling-late-data-and-watermarking"},{"title":"[source, scala]","text":"<p>package org.apache.spark.sql.execution.streaming</p> <p>trait WatermarkSupport extends UnaryExecNode {   // only required methods that have no implementation   def eventTimeWatermark: Option[Long]   def keyExpressions: Seq[Attribute] }</p>  <p>.WatermarkSupport Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| [[eventTimeWatermark]] <code>eventTimeWatermark</code> | Used mainly in &lt;&gt; to create a <code>LessThanOrEqual</code> Catalyst binary expression that matches rows older than the watermark. <p>| [[keyExpressions]] <code>keyExpressions</code> | Grouping keys (in FlatMapGroupsWithStateExec), duplicate keys (in StreamingDeduplicateExec) or key attributes (in StateStoreSaveExec) with at most one that may have spark.watermarkDelayMs watermark attribute in metadata</p> <p>Used in &lt;&gt; to create a <code>Predicate</code> to match rows older than the event time watermark. <p>Used also when StateStoreSaveExec and StreamingDeduplicateExec physical operators are executed. |===</p> <p>=== [[removeKeysOlderThanWatermark]][[removeKeysOlderThanWatermark-StateStore]] Removing Keys From StateStore Older Than Watermark -- <code>removeKeysOlderThanWatermark</code> Method</p>","location":"physical-operators/WatermarkSupport/#source-scala"},{"title":"[source, scala]","text":"","location":"physical-operators/WatermarkSupport/#source-scala_1"},{"title":"removeKeysOlderThanWatermark(store: StateStore): Unit","text":"<p><code>removeKeysOlderThanWatermark</code> requests the input <code>store</code> for all rows.</p> <p><code>removeKeysOlderThanWatermark</code> then uses watermarkPredicateForKeys to remove matching rows from the store.</p> <p><code>removeKeysOlderThanWatermark</code> is used when StreamingDeduplicateExec physical operator is requested to execute.</p> <p>=== [[removeKeysOlderThanWatermark-StreamingAggregationStateManager-store]] <code>removeKeysOlderThanWatermark</code> Method</p>","location":"physical-operators/WatermarkSupport/#removekeysolderthanwatermarkstore-statestore-unit"},{"title":"[source, scala]","text":"<p>removeKeysOlderThanWatermark(   storeManager: StreamingAggregationStateManager,   store: StateStore): Unit</p>  <p><code>removeKeysOlderThanWatermark</code>...FIXME</p> <p>NOTE: <code>removeKeysOlderThanWatermark</code> is used exclusively when <code>StateStoreSaveExec</code> physical operator is requested to &lt;&gt;.","location":"physical-operators/WatermarkSupport/#source-scala_2"},{"title":"WriteToContinuousDataSourceExec Physical Operator","text":"<p><code>WriteToContinuousDataSourceExec</code> is a unary physical operator (Spark SQL) that &lt;&gt;. <p><code>WriteToContinuousDataSourceExec</code> is &lt;&gt; exclusively when <code>DataSourceV2Strategy</code> (Spark SQL) execution planning strategy is requested to plan a WriteToContinuousDataSource unary logical operator. <p>[[creating-instance]] <code>WriteToContinuousDataSourceExec</code> takes the following to be created:</p> <ul> <li>[[query]][[child]] Child physical operator (<code>SparkPlan</code>)</li> </ul> <p>[[output]] <code>WriteToContinuousDataSourceExec</code> uses empty output schema (which is exactly to say that no output is expected whatsoever).</p> <p>[[logging]] [TIP] ==== Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.streaming.continuous.WriteToContinuousDataSourceExec</code> to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.streaming.continuous.WriteToContinuousDataSourceExec=ALL\n</code></pre>","location":"physical-operators/WriteToContinuousDataSourceExec/"},{"title":"Refer to &lt;&gt;. <p>=== [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- <code>doExecute</code> Method</p>","text":"","location":"physical-operators/WriteToContinuousDataSourceExec/#refer-to"},{"title":"[source, scala]","text":"","location":"physical-operators/WriteToContinuousDataSourceExec/#source-scala"},{"title":"doExecute(): RDD[InternalRow]","text":"<p>NOTE: <code>doExecute</code> is part of <code>SparkPlan</code> Contract to generate the runtime representation of an physical operator as a distributed computation over internal binary rows on Apache Spark (i.e. <code>RDD[InternalRow]</code>).</p> <p><code>doExecute</code> requests the &lt;&gt; to create a <code>DataWriterFactory</code>. <p><code>doExecute</code> then requests the &lt;&gt; to execute (that gives a <code>RDD[InternalRow]</code>) and uses the <code>RDD[InternalRow]</code> and the <code>DataWriterFactory</code> to create a &lt;&gt;. <p><code>doExecute</code> prints out the following INFO message to the logs:</p> <pre><code>Start processing data source writer: [writer]. The input RDD has [partitions] partitions.\n</code></pre> <p><code>doExecute</code> requests the <code>EpochCoordinatorRef</code> helper for a &lt;&gt; (using the &lt;&gt;). <p>NOTE: The &lt;&gt; runs on the driver as the single point to coordinate epochs across partition tasks. <p><code>doExecute</code> requests the EpochCoordinator RPC endpoint reference to send out a &lt;&gt; message synchronously. <p>In the end, <code>doExecute</code> requests the <code>ContinuousWriteRDD</code> to collect (which simply runs a Spark job on all partitions in an RDD and returns the results in an array).</p> <p>NOTE: Requesting the <code>ContinuousWriteRDD</code> to collect is how a Spark job is ran that in turn runs tasks (one per partition) that are described by the &lt;&gt; method. Since executing <code>collect</code> is meant to run a Spark job (with tasks on executors), it's in the discretion of the tasks themselves to decide when to finish (so if they want to run indefinitely, so be it). What a clever trick!","location":"physical-operators/WriteToContinuousDataSourceExec/#doexecute-rddinternalrow"},{"title":"WriteToDataSourceV2Exec Physical Operator","text":"<p><code>WriteToDataSourceV2Exec</code> is a <code>V2TableWriteExec</code> (Spark SQL) that represents WriteToDataSourceV2 logical operator at execution time.</p>","location":"physical-operators/WriteToDataSourceV2Exec/"},{"title":"Creating Instance","text":"<p><code>WriteToDataSourceV2Exec</code> takes the following to be created:</p> <ul> <li> <code>BatchWrite</code> (Spark SQL) <li> Refresh Cache Function (<code>() =&gt; Unit</code>) <li> Physical Query Plan (Spark SQL) <li> Write <code>CustomMetric</code>s (Spark SQL)  <p><code>WriteToDataSourceV2Exec</code> is created when:</p> <ul> <li><code>DataSourceV2Strategy</code> (Spark SQL) execution planning strategy is requested to plan a logical query plan (that is a WriteToDataSourceV2 logical operator)</li> </ul>","location":"physical-operators/WriteToDataSourceV2Exec/#creating-instance"},{"title":"Executing Physical Operator <pre><code>run(): Seq[InternalRow]\n</code></pre> <p><code>run</code> is part of the <code>V2CommandExec</code> (Spark SQL) abstraction.</p>  <p><code>run</code> writes rows out (Spark SQL) using the BatchWrite and then refreshes the cache (using the refresh cache function).</p> <p>In the end, <code>run</code> returns the rows written out.</p>","text":"","location":"physical-operators/WriteToDataSourceV2Exec/#executing-physical-operator"},{"title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec=ALL\n</code></pre> <p>Refer to Logging.</p>","text":"","location":"physical-operators/WriteToDataSourceV2Exec/#logging"},{"title":"Stateful Stream Processing","text":"<p>Stateful Stream Processing is a stream processing with state (implicit or explicit).</p> <p>In Spark Structured Streaming, a streaming query is stateful when is one of the following (that makes use of StateStores):</p> <ul> <li> <p>Streaming Aggregation</p> </li> <li> <p>Arbitrary Stateful Streaming Aggregation</p> </li> <li> <p>Stream-Stream Join</p> </li> <li> <p>Streaming Deduplication</p> </li> <li> <p>Streaming Limit</p> </li> </ul>","location":"stateful-stream-processing/"},{"title":"Versioned State, StateStores and StateStoreProviders <p>Spark Structured Streaming uses StateStores for versioned and fault-tolerant key-value state stores.</p> <p>State stores are checkpointed incrementally to avoid state loss and for increased performance.</p> <p>State stores are managed by StateStoreProviders with HDFSBackedStateStoreProvider being the default and only known implementation. <code>HDFSBackedStateStoreProvider</code> uses Hadoop DFS-compliant file system for state checkpointing and fault-tolerance.</p> <p>State store providers manage versioned state per stateful operator (and partition it operates on).</p> <p>The lifecycle of a <code>StateStoreProvider</code> begins when <code>StateStore</code> utility (on a Spark executor) is requested for the StateStore by provider ID and version.</p>  <p>Important</p> <p>It is worth to notice that since <code>StateStore</code> and <code>StateStoreProvider</code> utilities are Scala objects that makes it possible that there can only be one instance of <code>StateStore</code> and <code>StateStoreProvider</code> on a single JVM. Scala objects are (sort of) singletons which means that there will be exactly one instance of each per JVM and that is exactly the JVM of a Spark executor. As long as the executor is up and running state versions are cached and no Hadoop DFS is used (except for the initial load).</p>  <p>When requested for a StateStore, <code>StateStore</code> utility is given the version of a state store to look up. The version is either the &lt;&gt; (in Continuous Stream Processing) or the current batch ID (in Micro-Batch Stream Processing). <p><code>StateStore</code> utility requests <code>StateStoreProvider</code> utility to &lt;&gt; that creates the <code>StateStoreProvider</code> implementation (based on spark.sql.streaming.stateStore.providerClass internal configuration property) and requests it to &lt;&gt;. <p>The initialized <code>StateStoreProvider</code> is cached in loadedProviders internal lookup table (for a StateStoreId) for later lookups.</p> <p><code>StateStoreProvider</code> utility then requests the <code>StateStoreProvider</code> for the &lt;&gt;. (e.g. a HDFSBackedStateStore in case of HDFSBackedStateStoreProvider). <p>An instance of <code>StateStoreProvider</code> is requested to &lt;&gt; or &lt;&gt; (when &lt;&gt;) in &lt;&gt; that runs periodically every spark.sql.streaming.stateStore.maintenanceInterval configuration property.","text":"","location":"stateful-stream-processing/#versioned-state-statestores-and-statestoreproviders"},{"title":"IncrementalExecution \u2014 QueryExecution of Streaming Queries <p>Regardless of the query language (Dataset API or SQL), any structured query (incl. streaming queries) becomes a logical query plan.</p> <p>In Spark Structured Streaming it is IncrementalExecution that plans streaming queries for execution.</p> <p>While planning a streaming query for execution (aka query planning), <code>IncrementalExecution</code> uses the state preparation rule. The rule fills out the following physical operators with the execution-specific configuration (with StatefulOperatorStateInfo being the most important for stateful stream processing):</p> <ul> <li> <p>FlatMapGroupsWithStateExec</p> </li> <li> <p>StateStoreRestoreExec</p> </li> <li> <p>StateStoreSaveExec</p> </li> <li> <p>StreamingDeduplicateExec</p> </li> <li> <p>StreamingGlobalLimitExec</p> </li> <li> <p>StreamingSymmetricHashJoinExec</p> </li> </ul> <p>==== [[IncrementalExecution-shouldRunAnotherBatch]] Micro-Batch Stream Processing and Extra Non-Data Batch for StateStoreWriter Stateful Operators</p> <p>In Micro-Batch Stream Processing (with MicroBatchExecution engine), <code>IncrementalExecution</code> uses shouldRunAnotherBatch flag that allows StateStoreWriters stateful physical operators to indicate whether the last batch execution requires another non-data batch.</p> <p>The following <code>StateStoreWriters</code> redefine <code>shouldRunAnotherBatch</code> flag.</p> <p>[[StateStoreWriters-shouldRunAnotherBatch]] .StateStoreWriters and shouldRunAnotherBatch Flag [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | StateStoreWriter | shouldRunAnotherBatch Flag</p> <p>| FlatMapGroupsWithStateExec a| [[shouldRunAnotherBatch-FlatMapGroupsWithStateExec]] Based on GroupStateTimeout</p> <p>| &lt;&gt; a| [[shouldRunAnotherBatch-StateStoreSaveExec]] Based on &lt;&gt; <p>| &lt;&gt; a| [[shouldRunAnotherBatch-StreamingDeduplicateExec]] Based on &lt;&gt; <p>| &lt;&gt; a| [[shouldRunAnotherBatch-StreamingSymmetricHashJoinExec]] Based on &lt;&gt; <p>|===</p>","text":"","location":"stateful-stream-processing/#incrementalexecution-queryexecution-of-streaming-queries"},{"title":"StateStoreRDD <p>Right after query planning, a stateful streaming query (a single micro-batch actually) becomes an RDD with one or more StateStoreRDDs.</p> <p>You can find the <code>StateStoreRDDs</code> of a streaming query in the RDD lineage.</p> <pre><code>scala&gt; :type streamingQuery\norg.apache.spark.sql.streaming.StreamingQuery\n\nscala&gt; streamingQuery.explain\n== Physical Plan ==\n*(4) HashAggregate(keys=[window#13-T0ms, value#3L], functions=[count(1)])\n+- StateStoreSave [window#13-T0ms, value#3L], state info [ checkpoint = file:/tmp/checkpoint-counts/state, runId = 1dec2d81-f2d0-45b9-8f16-39ede66e13e7, opId = 0, ver = 1, numPartitions = 1], Append, 10000, 2\n   +- *(3) HashAggregate(keys=[window#13-T0ms, value#3L], functions=[merge_count(1)])\n      +- StateStoreRestore [window#13-T0ms, value#3L], state info [ checkpoint = file:/tmp/checkpoint-counts/state, runId = 1dec2d81-f2d0-45b9-8f16-39ede66e13e7, opId = 0, ver = 1, numPartitions = 1], 2\n         +- *(2) HashAggregate(keys=[window#13-T0ms, value#3L], functions=[merge_count(1)])\n            +- Exchange hashpartitioning(window#13-T0ms, value#3L, 1)\n               +- *(1) HashAggregate(keys=[window#13-T0ms, value#3L], functions=[partial_count(1)])\n                  +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#13-T0ms, value#3L]\n                     +- *(1) Filter isnotnull(time#2-T0ms)\n                        +- EventTimeWatermark time#2: timestamp, interval\n                           +- LocalTableScan &lt;empty&gt;, [time#2, value#3L]\n\nimport org.apache.spark.sql.execution.streaming.{StreamExecution, StreamingQueryWrapper}\nval se = streamingQuery.asInstanceOf[StreamingQueryWrapper].streamingQuery\n\nscala&gt; :type se\norg.apache.spark.sql.execution.streaming.StreamExecution\n\nscala&gt; :type se.lastExecution\norg.apache.spark.sql.execution.streaming.IncrementalExecution\n\nval rdd = se.lastExecution.toRdd\nscala&gt; rdd.toDebugString\nres3: String =\n(1) MapPartitionsRDD[39] at toRdd at &lt;console&gt;:40 []\n |  StateStoreRDD[38] at toRdd at &lt;console&gt;:40 [] // &lt;-- here\n |  MapPartitionsRDD[37] at toRdd at &lt;console&gt;:40 []\n |  StateStoreRDD[36] at toRdd at &lt;console&gt;:40 [] // &lt;-- here\n |  MapPartitionsRDD[35] at toRdd at &lt;console&gt;:40 []\n |  ShuffledRowRDD[17] at start at &lt;pastie&gt;:67 []\n +-(1) MapPartitionsRDD[16] at start at &lt;pastie&gt;:67 []\n    |  MapPartitionsRDD[15] at start at &lt;pastie&gt;:67 []\n    |  MapPartitionsRDD[14] at start at &lt;pastie&gt;:67 []\n    |  MapPartitionsRDD[13] at start at &lt;pastie&gt;:67 []\n    |  ParallelCollectionRDD[12] at start at &lt;pastie&gt;:67 []\n</code></pre>","text":"","location":"stateful-stream-processing/#statestorerdd"},{"title":"StateStoreCoordinator RPC Endpoint, StateStoreRDD and Preferred Locations <p>Since execution of a stateful streaming query happens on Spark executors whereas planning is on the driver, Spark Structured Streaming uses RPC environment for tracking locations of the state stores in use. That makes the tasks (of a structured query) to be scheduled where the state (of a partition) is.</p> <p>When planned for execution, the <code>StateStoreRDD</code> is first asked for the preferred locations of a partition (which happens on the driver) that are later used to compute it (on Spark executors).</p> <p>Spark Structured Streaming uses RPC environment to keep track of StateStores (their StateStoreProvider actually) for RDD planning.</p> <p>Every time StateStoreRDD is requested for the preferred locations of a partition, it communicates with the StateStoreCoordinator RPC endpoint that knows the locations of the required <code>StateStores</code> (per host and executor ID).</p> <p><code>StateStoreRDD</code> uses StateStoreProviderId with StateStoreId to uniquely identify the state store to use for (associate with) a stateful operator and a partition.</p>","text":"","location":"stateful-stream-processing/#statestorecoordinator-rpc-endpoint-statestorerdd-and-preferred-locations"},{"title":"State Management <p>The state in a stateful streaming query can be implicit or explicit.</p>","text":"","location":"stateful-stream-processing/#state-management"},{"title":"HDFSBackedStateStore","text":"<p><code>HDFSBackedStateStore</code> is a concrete StateStore that uses a Hadoop DFS-compatible file system for versioned state persistence.</p> <p><code>HDFSBackedStateStore</code> is &lt;&gt; exclusively when <code>HDFSBackedStateStoreProvider</code> is requested for the specified version of state (store) for update (when <code>StateStore</code> utility is requested to look up a StateStore by provider id). <p>[[id]] <code>HDFSBackedStateStore</code> uses the StateStoreId of the owning HDFSBackedStateStoreProvider.</p> <p>[[toString]] When requested for the textual representation, <code>HDFSBackedStateStore</code> gives HDFSStateStore[id=(op=[operatorId],part=[partitionId]),dir=[baseDir]].</p> <p>[[logging]] [TIP] ==== <code>HDFSBackedStateStore</code> is an internal class of HDFSBackedStateStoreProvider and uses its logger. ====</p> <p>=== [[creating-instance]] Creating HDFSBackedStateStore Instance</p> <p><code>HDFSBackedStateStore</code> takes the following to be created:</p> <ul> <li>[[version]] Version</li> <li>[[mapToUpdate]] State Map (<code>ConcurrentHashMap[UnsafeRow, UnsafeRow]</code>)</li> </ul> <p><code>HDFSBackedStateStore</code> initializes the &lt;&gt;. <p>=== [[state]] Internal State -- <code>state</code> Internal Property</p>","location":"stateful-stream-processing/HDFSBackedStateStore/"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStore/#source-scala"},{"title":"state: STATE","text":"<p><code>state</code> is the current state of <code>HDFSBackedStateStore</code> and can be in one of the three possible states: &lt;&gt;, &lt;&gt;, and &lt;&gt;. <p>State changes (to the internal &lt;&gt; registry) are allowed as long as <code>HDFSBackedStateStore</code> is in the default &lt;&gt; state. Right after a <code>HDFSBackedStateStore</code> transitions to either &lt;&gt; or &lt;&gt; state, no further state changes are allowed. <p>NOTE: Don't get confused with the term \"state\" as there are two states: the internal &lt;&gt; of <code>HDFSBackedStateStore</code> and the state of a streaming query (that <code>HDFSBackedStateStore</code> is responsible for). <p>[[states]] .Internal States [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| ABORTED a| [[ABORTED]] After &lt;&gt; <p>| COMMITTED a| [[COMMITTED]] After &lt;&gt; <p>&lt;&gt; flag indicates whether <code>HDFSBackedStateStore</code> is in this state or not. <p>| UPDATING a| [[UPDATING]] (default) Initial state after the <code>HDFSBackedStateStore</code> was &lt;&gt; <p>Allows for state changes (e.g. &lt;&gt;, &lt;&gt;, &lt;&gt;) and eventually &lt;&gt; or &lt;&gt; them <p>|===</p> <p>=== [[writeUpdateToDeltaFile]] <code>writeUpdateToDeltaFile</code> Internal Method</p>","location":"stateful-stream-processing/HDFSBackedStateStore/#state-state"},{"title":"[source, scala]","text":"<p>writeUpdateToDeltaFile(   output: DataOutputStream,   key: UnsafeRow,   value: UnsafeRow): Unit</p>  <p>CAUTION: FIXME</p> <p>=== [[put]] <code>put</code> Method</p>","location":"stateful-stream-processing/HDFSBackedStateStore/#source-scala_1"},{"title":"[source, scala]","text":"<p>put(   key: UnsafeRow,   value: UnsafeRow): Unit</p>  <p>NOTE: <code>put</code> is a part of StateStore.md#put[StateStore Contract] to...FIXME</p> <p><code>put</code> stores the copies of the key and value in &lt;&gt; internal registry followed by &lt;&gt; (using &lt;&gt;). <p><code>put</code> reports an <code>IllegalStateException</code> when <code>HDFSBackedStateStore</code> is not in &lt;&gt; state: <pre><code>Cannot put after already committed or aborted\n</code></pre> <p>=== [[commit]] Committing State Changes -- <code>commit</code> Method</p>","location":"stateful-stream-processing/HDFSBackedStateStore/#source-scala_2"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStore/#source-scala_3"},{"title":"commit(): Long","text":"<p><code>commit</code> is part of the StateStore abstraction.</p> <p><code>commit</code> requests the parent <code>HDFSBackedStateStoreProvider</code> to commit state changes (as a new version of state) (with the &lt;&gt;, the &lt;&gt; and the &lt;&gt;). <p><code>commit</code> transitions <code>HDFSBackedStateStore</code> to &lt;&gt; state. <p><code>commit</code> prints out the following INFO message to the logs:</p> <pre><code>Committed version [newVersion] for [this] to file [finalDeltaFile]\n</code></pre> <p><code>commit</code> returns a &lt;&gt;. <p><code>commit</code> throws an <code>IllegalStateException</code> when <code>HDFSBackedStateStore</code> is not in &lt;&gt; state: <pre><code>Cannot commit after already committed or aborted\n</code></pre> <p><code>commit</code> throws an <code>IllegalStateException</code> for any <code>NonFatal</code> exception:</p> <pre><code>Error committing version [newVersion] into [this]\n</code></pre> <p>=== [[abort]] Aborting State Changes -- <code>abort</code> Method</p>","location":"stateful-stream-processing/HDFSBackedStateStore/#commit-long"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStore/#source-scala_4"},{"title":"abort(): Unit","text":"<p><code>abort</code> is part of the StateStore abstraction.</p> <p><code>abort</code>...FIXME</p> <p>=== [[metrics]] Performance Metrics -- <code>metrics</code> Method</p>","location":"stateful-stream-processing/HDFSBackedStateStore/#abort-unit"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStore/#source-scala_5"},{"title":"metrics: StateStoreMetrics","text":"<p><code>metrics</code> is part of the StateStore abstraction.</p> <p><code>metrics</code> requests the performance metrics of the parent <code>HDFSBackedStateStoreProvider</code>.</p> <p>The performance metrics of the provider used are only the ones listed in supportedCustomMetrics.</p> <p>In the end, <code>metrics</code> returns a new StateStoreMetrics with the following:</p> <ul> <li> <p>Total number of keys as the size of &lt;&gt;  <li> <p>Memory used (in bytes) as the memoryUsedBytes metric (of the parent provider)</p> </li> <li> <p>StateStoreCustomMetrics as the supportedCustomMetrics and the metricStateOnCurrentVersionSizeBytes metric of the parent provider</p> </li>  <p>=== [[hasCommitted]] Are State Changes Committed? -- <code>hasCommitted</code> Method</p>","location":"stateful-stream-processing/HDFSBackedStateStore/#metrics-statestoremetrics"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStore/#source-scala_6"},{"title":"hasCommitted: Boolean","text":"<p><code>hasCommitted</code> is part of the StateStore abstraction.</p> <p><code>hasCommitted</code> returns <code>true</code> when <code>HDFSBackedStateStore</code> is in &lt;&gt; state and <code>false</code> otherwise.","location":"stateful-stream-processing/HDFSBackedStateStore/#hascommitted-boolean"},{"title":"Internal Properties","text":"<p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| compressedStream a| [[compressedStream]]</p>","location":"stateful-stream-processing/HDFSBackedStateStore/#internal-properties"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStore/#source-scala_7"},{"title":"compressedStream: DataOutputStream","text":"<p>The compressed https://docs.oracle.com/javase/8/docs/api/java/io/DataOutputStream.html[java.io.DataOutputStream] for the &lt;&gt; <p>| deltaFileStream a| [[deltaFileStream]]</p>","location":"stateful-stream-processing/HDFSBackedStateStore/#compressedstream-dataoutputstream"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStore/#source-scala_8"},{"title":"deltaFileStream: CheckpointFileManager.CancellableFSDataOutputStream","text":"<p>| finalDeltaFile a| [[finalDeltaFile]]</p>","location":"stateful-stream-processing/HDFSBackedStateStore/#deltafilestream-checkpointfilemanagercancellablefsdataoutputstream"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStore/#source-scala_9"},{"title":"finalDeltaFile: Path","text":"<p>The Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path] of the deltaFile for the version</p> <p>| newVersion a| [[newVersion]]</p>","location":"stateful-stream-processing/HDFSBackedStateStore/#finaldeltafile-path"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStore/#source-scala_10"},{"title":"newVersion: Long","text":"<p>Used exclusively when <code>HDFSBackedStateStore</code> is requested for the &lt;&gt;, to &lt;&gt; and &lt;&gt; <p>|===</p>","location":"stateful-stream-processing/HDFSBackedStateStore/#newversion-long"},{"title":"HDFSBackedStateStoreProvider","text":"<p><code>HDFSBackedStateStoreProvider</code> is a StateStoreProvider that uses a Hadoop DFS-compatible file system for versioned state checkpointing.</p> <p><code>HDFSBackedStateStoreProvider</code> is the default <code>StateStoreProvider</code> per the spark.sql.streaming.stateStore.providerClass internal configuration property.</p> <p><code>HDFSBackedStateStoreProvider</code> is &lt;&gt; and immediately requested to &lt;&gt; when <code>StateStoreProvider</code> utility is requested to &lt;&gt;. That is when <code>HDFSBackedStateStoreProvider</code> is given the &lt;&gt; that uniquely identifies the state store to use for a stateful operator and a partition. <p><code>HDFSStateStoreProvider</code> uses HDFSBackedStateStores to manage state (&lt;&gt;). <p><code>HDFSBackedStateStoreProvider</code> manages versioned state in delta and snapshot files (and uses a &lt;&gt; internally for faster access to state versions). <p>[[creating-instance]] <code>HDFSBackedStateStoreProvider</code> takes no arguments to be created.</p> <p>[[logging]] [TIP] ==== Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider=ALL\n</code></pre>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/"},{"title":"Refer to &lt;&gt;.","text":"","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#refer-to"},{"title":"Performance Metrics","text":"<p>[cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Name (in web UI) | Description</p> <p>| memoryUsedBytes a| [[memoryUsedBytes]] Estimated size of the &lt;&gt; internal registry <p>| count of cache hit on states cache in provider a| [[metricLoadedMapCacheHit]][[loadedMapCacheHitCount]] The number of times &lt;&gt; was successful and found (hit) the requested state version in the &lt;&gt; internal cache <p>| count of cache miss on states cache in provider a| [[metricLoadedMapCacheMiss]][[loadedMapCacheMissCount]] The number of times &lt;&gt; could not find (missed) the requested state version in the &lt;&gt; internal cache <p>| estimated size of state only on current version a| [[metricStateOnCurrentVersionSizeBytes]][[stateOnCurrentVersionSizeBytes]] Estimated size of the current state (of the HDFSBackedStateStore)</p> <p>|===</p> <p>=== [[baseDir]] State Checkpoint Base Directory -- <code>baseDir</code> Lazy Internal Property</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#performance-metrics"},{"title":"[source,scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#sourcescala"},{"title":"baseDir: Path","text":"<p><code>baseDir</code> is the base directory (as a Hadoop Path) for state checkpointing (for &lt;&gt; and &lt;&gt; state files). <p><code>baseDir</code> is initialized lazily since it is not yet known when <code>HDFSBackedStateStoreProvider</code> is &lt;&gt;. <p><code>baseDir</code> is initialized and created based on the &lt;&gt; of the &lt;&gt; when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;&gt;. <p>=== [[stateStoreId]][[stateStoreId_]] StateStoreId -- Unique Identifier of State Store</p> <p>As a &lt;&gt;, <code>HDFSBackedStateStoreProvider</code> is associated with a &lt;&gt; (which is a unique identifier of the state store for a stateful operator and a partition). <p><code>HDFSBackedStateStoreProvider</code> is given the &lt;&gt; at &lt;&gt; (as requested by the &lt;&gt; contract). <p>The &lt;&gt; is then used for the following: <ul> <li> <p><code>HDFSBackedStateStore</code> is requested for the id</p> </li> <li> <p><code>HDFSBackedStateStoreProvider</code> is requested for the &lt;&gt; and the &lt;&gt;   <p>=== [[toString]] Textual Representation -- <code>toString</code> Method</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#basedir-path"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala"},{"title":"toString: String","text":"<p>NOTE: <code>toString</code> is part of the ++https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object] contract for the string representation of the object.</p> <p><code>HDFSBackedStateStoreProvider</code> uses the &lt;&gt; and the &lt;&gt; for the textual representation: <pre><code>HDFSStateStoreProvider[id = (op=[operatorId],part=[partitionId]),dir = [baseDir]]\n</code></pre> <p>=== [[getStore]] Loading Specified Version of State (Store) For Update -- <code>getStore</code> Method</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#tostring-string"},{"title":"[source, scala]","text":"<p>getStore(   version: Long): StateStore</p>  <p><code>getStore</code> is part of the StateStoreProvider abstraction.</p> <p><code>getStore</code> creates a new empty state (<code>ConcurrentHashMap[UnsafeRow, UnsafeRow]</code>) and &lt;&gt; for versions greater than <code>0</code>. <p>In the end, <code>getStore</code> creates a new HDFSBackedStateStore for the specified version with the new state and prints out the following INFO message to the logs:</p> <pre><code>Retrieved version [version] of [this] for update\n</code></pre> <p><code>getStore</code> throws an <code>IllegalArgumentException</code> when the specified version is less than <code>0</code> (negative):</p> <pre><code>Version cannot be less than 0\n</code></pre> <p>=== [[deltaFile]] <code>deltaFile</code> Internal Method</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_1"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_2"},{"title":"deltaFile(version: Long): Path","text":"<p><code>deltaFile</code> simply returns the Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path] of the <code>[version].delta</code> file in the &lt;&gt;. <p><code>deltaFile</code> is used when:</p> <ul> <li> <p>HDFSBackedStateStore is created (and creates the &lt;&gt;)  <li> <p><code>HDFSBackedStateStoreProvider</code> is requested to updateFromDeltaFile</p> </li>  <p>=== [[snapshotFile]] <code>snapshotFile</code> Internal Method</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#deltafileversion-long-path"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_3"},{"title":"snapshotFile(version: Long): Path","text":"<p><code>snapshotFile</code> simply returns the Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path] of the <code>[version].snapshot</code> file in the &lt;&gt;. <p>NOTE: <code>snapshotFile</code> is used when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;&gt; or &lt;&gt;. <p>=== [[fetchFiles]] Listing All Delta And Snapshot Files In State Checkpoint Directory -- <code>fetchFiles</code> Internal Method</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#snapshotfileversion-long-path"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_4"},{"title":"fetchFiles(): Seq[StoreFile]","text":"<p><code>fetchFiles</code> requests the &lt;&gt; for all the files in the &lt;&gt;. <p>For every file, <code>fetchFiles</code> splits the name into two parts with <code>.</code> (dot) as a separator (files with more or less than two parts are simply ignored) and registers a new <code>StoreFile</code> for <code>snapshot</code> and <code>delta</code> files:</p> <ul> <li> <p>For <code>snapshot</code> files, <code>fetchFiles</code> creates a new <code>StoreFile</code> with <code>isSnapshot</code> flag on (<code>true</code>)</p> </li> <li> <p>For <code>delta</code> files, <code>fetchFiles</code> creates a new <code>StoreFile</code> with <code>isSnapshot</code> flag off (<code>false</code>)</p> </li> </ul> <p>NOTE: <code>delta</code> files are only registered if there was no <code>snapshot</code> file for the version.</p> <p><code>fetchFiles</code> prints out the following WARN message to the logs for any other files:</p> <pre><code>Could not identify file [path] for [this]\n</code></pre> <p>In the end, <code>fetchFiles</code> sorts the <code>StoreFiles</code> based on their version, prints out the following DEBUG message to the logs, and returns the files.</p> <pre><code>Current set of files for [this]: [storeFiles]\n</code></pre> <p>NOTE: <code>fetchFiles</code> is used when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;&gt; and &lt;&gt;. <p>=== [[init]] Initializing StateStoreProvider -- <code>init</code> Method</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#fetchfiles-seqstorefile"},{"title":"[source, scala]","text":"<p>init(   stateStoreId: StateStoreId,   keySchema: StructType,   valueSchema: StructType,   indexOrdinal: Option[Int],   storeConf: StateStoreConf,   hadoopConf: Configuration): Unit</p>  <p>NOTE: <code>init</code> is part of the &lt;&gt; to initialize itself. <p><code>init</code> records the values of the input arguments as the &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, and &lt;&gt; internal properties. <p><code>init</code> requests the given <code>StateStoreConf</code> for the spark.sql.streaming.maxBatchesToRetainInMemory configuration property (that is then recorded in the &lt;&gt; internal property). <p>In the end, <code>init</code> requests the &lt;&gt; to create the &lt;&gt; directory (with parent directories). <p>=== [[filesForVersion]] Finding Snapshot File and Delta Files For Version -- <code>filesForVersion</code> Internal Method</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_5"},{"title":"[source, scala]","text":"<p>filesForVersion(   allFiles: Seq[StoreFile],   version: Long): Seq[StoreFile]</p>  <p><code>filesForVersion</code> finds the latest snapshot version among the given <code>allFiles</code> files up to and including the given version (it may or may not be available).</p> <p>If a snapshot file was found (among the given file up to and including the given version), <code>filesForVersion</code> takes all delta files between the version of the snapshot file (exclusive) and the given version (inclusive) from the given <code>allFiles</code> files.</p> <p>NOTE: The number of delta files should be the given version minus the snapshot version.</p> <p>If a snapshot file was not found, <code>filesForVersion</code> takes all delta files up to the given version (inclusive) from the given <code>allFiles</code> files.</p> <p>In the end, <code>filesForVersion</code> returns a snapshot version (if available) and all delta files up to the given version (inclusive).</p> <p>NOTE: <code>filesForVersion</code> is used when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;&gt; and &lt;&gt;. <p>=== [[doMaintenance]] State Maintenance (Snapshotting and Cleaning Up) -- <code>doMaintenance</code> Method</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_6"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_7"},{"title":"doMaintenance(): Unit","text":"<p>NOTE: <code>doMaintenance</code> is part of the &lt;&gt; for optional state maintenance. <p><code>doMaintenance</code> simply does &lt;&gt; followed by &lt;&gt;. <p>In case of any non-fatal errors, <code>doMaintenance</code> simply prints out the following WARN message to the logs:</p> <pre><code>Error performing snapshot and cleaning up [this]\n</code></pre> <p>==== [[doSnapshot]] State Snapshoting (Rolling Up Delta Files into Snapshot File) -- <code>doSnapshot</code> Internal Method</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#domaintenance-unit"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_8"},{"title":"doSnapshot(): Unit","text":"<p><code>doSnapshot</code> &lt;&gt; (<code>files</code>) and prints out the following DEBUG message to the logs: <pre><code>fetchFiles() took [time] ms.\n</code></pre> <p><code>doSnapshot</code> returns immediately (and does nothing) when there are no delta and snapshot files.</p> <p><code>doSnapshot</code> takes the version of the latest file (<code>lastVersion</code>).</p> <p><code>doSnapshot</code> &lt;&gt; (among the files and for the last version). <p><code>doSnapshot</code> looks up the last version in the &lt;&gt;. <p>When the last version was found in the cache and the number of delta files is above spark.sql.streaming.stateStore.minDeltasForSnapshot internal threshold, <code>doSnapshot</code> &lt;&gt;. <p>In the end, <code>doSnapshot</code> prints out the following DEBUG message to the logs:</p> <pre><code>writeSnapshotFile() took [time] ms.\n</code></pre> <p>In case of non-fatal errors, <code>doSnapshot</code> simply prints out the following WARN message to the logs:</p> <pre><code>Error doing snapshots for [this]\n</code></pre> <p>NOTE: <code>doSnapshot</code> is used exclusively when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;&gt;. <p>==== [[cleanup]] Cleaning Up (Removing Old State Files) -- <code>cleanup</code> Internal Method</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#dosnapshot-unit"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_9"},{"title":"cleanup(): Unit","text":"<p><code>cleanup</code> &lt;&gt; (<code>files</code>) and prints out the following DEBUG message to the logs: <pre><code>fetchFiles() took [time] ms.\n</code></pre> <p><code>cleanup</code> returns immediately (and does nothing) when there are no delta and snapshot files.</p> <p><code>cleanup</code> takes the version of the latest state file (<code>lastVersion</code>) and decrements it by spark.sql.streaming.minBatchesToRetain configuration property that gives the earliest version to retain (and all older state files to be removed).</p> <p><code>cleanup</code> requests the &lt;&gt; to delete the path of every old state file. <p><code>cleanup</code> prints out the following DEBUG message to the logs:</p> <pre><code>deleting files took [time] ms.\n</code></pre> <p>In the end, <code>cleanup</code> prints out the following INFO message to the logs:</p> <pre><code>Deleted files older than [version] for [this]: [filesToDelete]\n</code></pre> <p>In case of a non-fatal exception, <code>cleanup</code> prints out the following WARN message to the logs:</p> <pre><code>Error cleaning up files for [this]\n</code></pre> <p>NOTE: <code>cleanup</code> is used exclusively when <code>HDFSBackedStateStoreProvider</code> is requested for &lt;&gt;. <p>=== [[close]] Closing State Store Provider -- <code>close</code> Method</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#cleanup-unit"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_10"},{"title":"close(): Unit","text":"<p>NOTE: <code>close</code> is part of the &lt;&gt; to close the state store provider. <p><code>close</code>...FIXME</p> <p>=== [[getMetricsForProvider]] <code>getMetricsForProvider</code> Method</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#close-unit"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_11"},{"title":"getMetricsForProvider(): Map[String, Long]","text":"<p><code>getMetricsForProvider</code> returns the following &lt;&gt;: <ul> <li> <p>&lt;&gt;  <li> <p>&lt;&gt;  <li> <p>&lt;&gt;   <p><code>getMetricsForProvider</code> is used when <code>HDFSBackedStateStore</code> is requested for performance metrics.</p> <p>=== [[supportedCustomMetrics]] Supported StateStoreCustomMetrics -- <code>supportedCustomMetrics</code> Method</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#getmetricsforprovider-mapstring-long"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_12"},{"title":"supportedCustomMetrics: Seq[StateStoreCustomMetric]","text":"<p>NOTE: <code>supportedCustomMetrics</code> is part of the &lt;&gt; for the &lt;&gt; of a state store provider. <p><code>supportedCustomMetrics</code> includes the following &lt;&gt;: <ul> <li> <p>&lt;&gt;  <li> <p>&lt;&gt;  <li> <p>&lt;&gt;   <p>=== [[commitUpdates]] Committing State Changes (As New Version of State) -- <code>commitUpdates</code> Internal Method</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#supportedcustommetrics-seqstatestorecustommetric"},{"title":"[source, scala]","text":"<p>commitUpdates(   newVersion: Long,   map: ConcurrentHashMap[UnsafeRow, UnsafeRow],   output: DataOutputStream): Unit</p>  <p><code>commitUpdates</code> &lt;&gt; (with the given <code>DataOutputStream</code>) followed by &lt;&gt; (with the given <code>newVersion</code> and the <code>map</code> state). <p><code>commitUpdates</code> is used when <code>HDFSBackedStateStore</code> is requested to commit state changes.</p> <p>=== [[loadMap]] Loading Specified Version of State (from Internal Cache or Snapshot and Delta Files) -- <code>loadMap</code> Internal Method</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_13"},{"title":"[source, scala]","text":"<p>loadMap(   version: Long): ConcurrentHashMap[UnsafeRow, UnsafeRow]</p>  <p><code>loadMap</code> firstly tries to find the state version in the &lt;&gt; internal cache and, if found, returns it immediately and increments the &lt;&gt; metric. <p>If the requested state version could not be found in the &lt;&gt; internal cache, <code>loadMap</code> prints out the following WARN message to the logs:","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_14"},{"title":"[options=\"wrap\"]","text":"","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#optionswrap"},{"title":"The state for version [version] doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.","text":"<p><code>loadMap</code> increments the &lt;&gt; metric. <p><code>loadMap</code> &lt;&gt; and, if found, &lt;&gt; and returns it. <p>If not found, <code>loadMap</code> tries to find the most recent state version by decrementing the requested version until one is found in the &lt;&gt; internal cache or &lt;&gt;. <p><code>loadMap</code> &lt;&gt; for all the remaining versions (from the snapshot version up to the requested one). <code>loadMap</code> &lt;&gt; (the closest snapshot and the remaining delta versions) and returns it. <p>In the end, <code>loadMap</code> prints out the following DEBUG message to the logs:</p> <pre><code>Loading state for [version] takes [elapsedMs] ms.\n</code></pre> <p>NOTE: <code>loadMap</code> is used exclusively when <code>HDFSBackedStateStoreProvider</code> is requested for the &lt;&gt;. <p>=== [[readSnapshotFile]] Loading State Snapshot File For Specified Version -- <code>readSnapshotFile</code> Internal Method</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#the-state-for-version-version-doesnt-exist-in-loadedmaps-reading-snapshot-file-and-delta-files-if-needednote-that-this-is-normal-for-the-first-batch-of-starting-query"},{"title":"[source, scala]","text":"<p>readSnapshotFile(   version: Long): Option[ConcurrentHashMap[UnsafeRow, UnsafeRow]]</p>  <p><code>readSnapshotFile</code> &lt;&gt; for the given <code>version</code>. <p><code>readSnapshotFile</code> requests the &lt;&gt; to open the snapshot file for reading and &lt;&gt; (<code>input</code>). <p><code>readSnapshotFile</code> reads the decompressed input stream until an EOF (that is marked as the integer <code>-1</code> in the stream) and inserts key and value rows in a state map (<code>ConcurrentHashMap[UnsafeRow, UnsafeRow]</code>):</p> <ul> <li> <p>First integer is the size of a key (buffer) followed by the key itself (of the size). <code>readSnapshotFile</code> creates an <code>UnsafeRow</code> for the key (with the number of fields as indicated by the number of fields of the &lt;&gt;).  <li> <p>Next integer is the size of a value (buffer) followed by the value itself (of the size). <code>readSnapshotFile</code> creates an <code>UnsafeRow</code> for the value (with the number of fields as indicated by the number of fields of the &lt;&gt;).   <p>In the end, <code>readSnapshotFile</code> prints out the following INFO message to the logs and returns the key-value map.</p> <pre><code>Read snapshot file for version [version] of [this] from [fileToRead]\n</code></pre> <p>In case of <code>FileNotFoundException</code> <code>readSnapshotFile</code> simply returns <code>None</code> (to indicate no snapshot state file was available and so no state for the version).</p> <p><code>readSnapshotFile</code> throws an <code>IOException</code> for the size of a key or a value below <code>0</code>:</p> <pre><code>Error reading snapshot file [fileToRead] of [this]: [key|value] size cannot be [keySize|valueSize]\n</code></pre> <p>NOTE: <code>readSnapshotFile</code> is used exclusively when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;&gt;. <p>=== [[updateFromDeltaFile]] Updating State with State Changes For Specified Version (per Delta File) -- <code>updateFromDeltaFile</code> Internal Method</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_15"},{"title":"[source, scala]","text":"<p>updateFromDeltaFile(   version: Long,   map: ConcurrentHashMap[UnsafeRow, UnsafeRow]): Unit</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_16"},{"title":"[NOTE] <p><code>updateFromDeltaFile</code> is very similar code-wise to &lt;&gt; with the two main differences: <ul> <li> <p><code>updateFromDeltaFile</code> is given the state map to update (while &lt;&gt; loads the state from a snapshot file)  <li> <p><code>updateFromDeltaFile</code> removes a key from the state map when the value (size) is <code>-1</code> (while &lt;&gt; throws an <code>IOException</code>)","text":"","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#note"},{"title":"The following description is almost an exact copy of &lt;&gt; just for completeness. <p><code>updateFromDeltaFile</code> &lt;&gt; for the requested <code>version</code>. <p><code>updateFromDeltaFile</code> requests the &lt;&gt; to open the delta file for reading and &lt;&gt; (<code>input</code>). <p><code>updateFromDeltaFile</code> reads the decompressed input stream until an EOF (that is marked as the integer <code>-1</code> in the stream) and inserts key and value rows in the given state map:</p> <ul> <li> <p>First integer is the size of a key (buffer) followed by the key itself (of the size). <code>updateFromDeltaFile</code> creates an <code>UnsafeRow</code> for the key (with the number of fields as indicated by the number of fields of the &lt;&gt;).  <li> <p>Next integer is the size of a value (buffer) followed by the value itself (of the size). <code>updateFromDeltaFile</code> creates an <code>UnsafeRow</code> for the value (with the number of fields as indicated by the number of fields of the &lt;&gt;) or removes the corresponding key from the state map (if the value size is <code>-1</code>)   <p>NOTE: <code>updateFromDeltaFile</code> removes the key-value entry from the state map if the value (size) is <code>-1</code>.</p> <p>In the end, <code>updateFromDeltaFile</code> prints out the following INFO message to the logs and returns the key-value map.</p> <pre><code>Read delta file for version [version] of [this] from [fileToRead]\n</code></pre> <p><code>updateFromDeltaFile</code> throws an <code>IllegalStateException</code> in case of <code>FileNotFoundException</code> while opening the delta file for the specified version:</p> <pre><code>Error reading delta file [fileToRead] of [this]: [fileToRead] does not exist\n</code></pre> <p>NOTE: <code>updateFromDeltaFile</code> is used exclusively when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;&gt;. <p>=== [[putStateIntoStateCacheMap]] Caching New Version of State -- <code>putStateIntoStateCacheMap</code> Internal Method</p>","text":"","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#the-following-description-is-almost-an-exact-copy-of-just-for-completeness"},{"title":"[source, scala]","text":"<p>putStateIntoStateCacheMap(   newVersion: Long,   map: ConcurrentHashMap[UnsafeRow, UnsafeRow]): Unit</p>  <p><code>putStateIntoStateCacheMap</code> registers state for a given version, i.e. adds the <code>map</code> state under the <code>newVersion</code> key in the &lt;&gt; internal registry. <p>With the &lt;&gt; threshold as <code>0</code> or below, <code>putStateIntoStateCacheMap</code> simply removes all entries from the &lt;&gt; internal registry and returns. <p><code>putStateIntoStateCacheMap</code> removes the oldest state version(s) in the &lt;&gt; internal registry until its size is at the &lt;&gt; threshold. <p>With the size of the &lt;&gt; internal registry is at the &lt;&gt; threshold, <code>putStateIntoStateCacheMap</code> does two more optimizations per <code>newVersion</code> <ul> <li> <p>It does not add the given state when the version of the oldest state is earlier (larger) than the given <code>newVersion</code></p> </li> <li> <p>It removes the oldest state when older (smaller) than the given <code>newVersion</code></p> </li> </ul> <p>NOTE: <code>putStateIntoStateCacheMap</code> is used when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;&gt; and &lt;&gt;. <p>=== [[writeSnapshotFile]] Writing Compressed Snapshot File for Specified Version -- <code>writeSnapshotFile</code> Internal Method</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_17"},{"title":"[source, scala]","text":"<p>writeSnapshotFile(   version: Long,   map: ConcurrentHashMap[UnsafeRow, UnsafeRow]): Unit</p>  <p><code>writeSnapshotFile</code> &lt;&gt; for the given version. <p><code>writeSnapshotFile</code> requests the &lt;&gt; to create the snapshot file (with overwriting enabled) and &lt;&gt;. <p>For every key-value <code>UnsafeRow</code> pair in the given map, <code>writeSnapshotFile</code> writes the size of the key followed by the key itself (as bytes). <code>writeSnapshotFile</code> then writes the size of the value followed by the value itself (as bytes).</p> <p>In the end, <code>writeSnapshotFile</code> prints out the following INFO message to the logs:</p> <pre><code>Written snapshot file for version [version] of [this] at [targetFile]\n</code></pre> <p>In case of any <code>Throwable</code> exception, <code>writeSnapshotFile</code> &lt;&gt; and re-throws the exception. <p>NOTE: <code>writeSnapshotFile</code> is used exclusively when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;&gt;. <p>=== [[compressStream]] <code>compressStream</code> Internal Method</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_18"},{"title":"[source, scala]","text":"<p>compressStream(   outputStream: DataOutputStream): DataOutputStream</p>  <p><code>compressStream</code> creates a new <code>LZ4CompressionCodec</code> (based on the &lt;&gt;) and requests it to create a <code>LZ4BlockOutputStream</code> with the given <code>DataOutputStream</code>. <p>In the end, <code>compressStream</code> creates a new <code>DataOutputStream</code> with the <code>LZ4BlockOutputStream</code>.</p> <p>NOTE: <code>compressStream</code> is used when...FIXME</p> <p>=== [[cancelDeltaFile]] <code>cancelDeltaFile</code> Internal Method</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_19"},{"title":"[source, scala]","text":"<p>cancelDeltaFile(   compressedStream: DataOutputStream,   rawStream: CancellableFSDataOutputStream): Unit</p>  <p><code>cancelDeltaFile</code>...FIXME</p> <p>NOTE: <code>cancelDeltaFile</code> is used when...FIXME</p> <p>=== [[finalizeDeltaFile]] <code>finalizeDeltaFile</code> Internal Method</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_20"},{"title":"[source, scala]","text":"<p>finalizeDeltaFile(   output: DataOutputStream): Unit</p>  <p><code>finalizeDeltaFile</code> simply writes <code>-1</code> to the given <code>DataOutputStream</code> (to indicate end of file) and closes it.</p> <p>NOTE: <code>finalizeDeltaFile</code> is used exclusively when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;&gt;. <p>=== [[loadedMaps]] Lookup Table (Cache) of States By Version -- <code>loadedMaps</code> Internal Method</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_21"},{"title":"[source, scala]","text":"<p>loadedMaps: TreeMap[   Long,                                    // version   ConcurrentHashMap[UnsafeRow, UnsafeRow]] // state (as keys and values)</p>  <p><code>loadedMaps</code> is a https://docs.oracle.com/javase/8/docs/api/java/util/TreeMap.html[java.util.TreeMap] of state versions sorted according to the reversed ordering of the versions (i.e. long numbers).</p> <p>A new entry (a version and the state updates) can only be added when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;&gt; (and only when the spark.sql.streaming.maxBatchesToRetainInMemory internal configuration is above <code>0</code>). <p><code>loadedMaps</code> is mainly used when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;&gt;. Positive hits (when a version could be found in the cache) is available as the &lt;&gt; performance metric while misses are counted in the &lt;&gt; performance metric. <p>NOTE: With no or missing versions in cache &lt;&gt; metric should be above <code>0</code> while &lt;&gt; always <code>0</code> (or smaller than the other metric). <p>The estimated size of <code>loadedMaps</code> is available as the &lt;&gt; performance metric. <p>The spark.sql.streaming.maxBatchesToRetainInMemory internal configuration is used as the threshold of the number of elements in <code>loadedMaps</code>. When <code>0</code> or negative, every &lt;&gt; removes all elements in (clears) <code>loadedMaps</code>. <p>NOTE: It is possible to change the configuration at restart of a structured query.</p> <p>The state deltas (the values) in <code>loadedMaps</code> are cleared (all entries removed) when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;&gt;. <p>Used when <code>HDFSBackedStateStoreProvider</code> is requested for the following:</p> <ul> <li> <p>&lt;&gt;  <li> <p>&lt;&gt;   <p>=== [[internal-properties]] Internal Properties</p> <p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| fm a| [[fm]] CheckpointFileManager for the &lt;&gt; (and the &lt;&gt;) <p>Used when:</p> <ul> <li> <p>Creating a new HDFSBackedStateStore (to create the CancellableFSDataOutputStream for the finalDeltaFile)</p> </li> <li> <p><code>HDFSBackedStateStoreProvider</code> is requested to &lt;&gt; (to create the &lt;&gt;), &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, and &lt;&gt;   <p>| hadoopConf a| [[hadoopConf]] Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration] of the &lt;&gt; <p>Given when <code>HDFSBackedStateStoreProvider</code> is requested to &lt;&gt; <p>| keySchema a| [[keySchema]]</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_22"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_23"},{"title":"keySchema: StructType","text":"<p>Schema of the state keys</p> <p>| valueSchema a| [[valueSchema]]</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#keyschema-structtype"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_24"},{"title":"valueSchema: StructType","text":"<p>Schema of the state values</p> <p>| numberOfVersionsToRetainInMemory a| [[numberOfVersionsToRetainInMemory]]</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#valueschema-structtype"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#source-scala_25"},{"title":"numberOfVersionsToRetainInMemory: Int","text":"<p><code>numberOfVersionsToRetainInMemory</code> is the maximum number of entries in the &lt;&gt; internal registry and is configured by the spark.sql.streaming.maxBatchesToRetainInMemory internal configuration. <p><code>numberOfVersionsToRetainInMemory</code> is a threshold when <code>HDFSBackedStateStoreProvider</code> removes the last key from the &lt;&gt; internal registry (per reverse ordering of state versions) when requested to &lt;&gt;. <p>| sparkConf a| [[sparkConf]] <code>SparkConf</code></p> <p>|===</p>","location":"stateful-stream-processing/HDFSBackedStateStoreProvider/#numberofversionstoretaininmemory-int"},{"title":"ReadStateStore","text":"<p><code>ReadStateStore</code> is...FIXME</p>","location":"stateful-stream-processing/ReadStateStore/"},{"title":"RocksDB","text":"","location":"stateful-stream-processing/RocksDB/"},{"title":"Creating Instance","text":"<p><code>RocksDB</code> takes the following to be created:</p> <ul> <li> DFS Root Directory <li> RocksDBConf <li> Local root directory <li> Hadoop <code>Configuration</code> <li> Logging ID  <p><code>RocksDB</code> is created when:</p> <ul> <li><code>RocksDBStateStoreProvider</code> is requested for the RocksDB</li> </ul>","location":"stateful-stream-processing/RocksDB/#creating-instance"},{"title":"RocksDBConf","text":"<p><code>RocksDBConf</code> is...FIXME</p>","location":"stateful-stream-processing/RocksDBConf/"},{"title":"RocksDBFileManager","text":"<p><code>RocksDBFileManager</code> is the file manager of RocksDB.</p>","location":"stateful-stream-processing/RocksDBFileManager/"},{"title":"Creating Instance","text":"<p><code>RocksDBFileManager</code> takes the following to be created:</p> <ul> <li> DFS Root Directory <li> Local Temporary Directory <li> Hadoop <code>Configuration</code> <li> Logging ID  <p><code>RocksDBFileManager</code> is created when:</p> <ul> <li><code>RocksDB</code> is created</li> </ul>","location":"stateful-stream-processing/RocksDBFileManager/#creating-instance"},{"title":"RocksDBStateStore","text":"<p><code>RocksDBStateStore</code> is a StateStore.</p>","location":"stateful-stream-processing/RocksDBStateStore/"},{"title":"RocksDBStateStoreProvider","text":"<p><code>RocksDBStateStoreProvider</code> is a StateStoreProvider.</p>","location":"stateful-stream-processing/RocksDBStateStoreProvider/"},{"title":"RocksDB <pre><code>rocksDB: RocksDB\n</code></pre>  Lazy Value <p><code>rocksDB</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p>  <p><code>rocksDB</code> requests the StateStoreId for storeCheckpointLocation.</p> <p><code>rocksDB</code> builds a store identifier (using the StateStoreId):</p> <pre><code>StateStoreId(opId=[operatorId],partId=[partitionId],name=[storeName])\n</code></pre> <p><code>rocksDB</code> creates a local root directory (in a temp directory for a directory with the store identifier).</p>  <p>spark.local.dir</p> <p>Use <code>spark.local.dir</code> Spark property to set up the local root directory.</p>  <p>In the end, <code>rocksDB</code> creates a RocksDB for the store identifier.</p>  <p><code>rocksDB</code> lazy value is initialized in init.</p>  <p><code>rocksDB</code> is used when:</p> <ul> <li><code>RocksDBStateStore</code> is requested to get the value for a key, put, remove, iterator, prefixScan, commit, abort, metrics</li> <li><code>RocksDBStateStoreProvider</code> is requested to getStore, doMaintenance, close, latestVersion</li> </ul>","text":"","location":"stateful-stream-processing/RocksDBStateStoreProvider/#rocksdb"},{"title":"Initialization <pre><code>init(\n  stateStoreId: StateStoreId,\n  keySchema: StructType,\n  valueSchema: StructType,\n  numColsPrefixKey: Int,\n  storeConf: StateStoreConf,\n  hadoopConf: Configuration): Unit\n</code></pre> <p><code>init</code> is part of the StateStoreProvider abstraction.</p>  <p><code>init</code> sets the internal registries (based on the given arguments):</p> <ul> <li>stateStoreId_</li> <li>keySchema</li> <li>valueSchema</li> <li>storeConf</li> <li>hadoopConf</li> <li>encoder</li> </ul> <p>In the end, <code>init</code> initializes the RocksDB (lazy value).</p>","text":"","location":"stateful-stream-processing/RocksDBStateStoreProvider/#initialization"},{"title":"StateSchemaCompatibilityChecker","text":"<p><code>StateSchemaCompatibilityChecker</code> is...FIXME</p>","location":"stateful-stream-processing/StateSchemaCompatibilityChecker/"},{"title":"StateStore","text":"<p><code>StateStore</code> is an extension of the ReadStateStore abstraction for versioned key-value stores for writing and reading state for Stateful Stream Processing (e.g., for persisting running aggregates in Streaming Aggregation).</p>  <p>Note</p> <p><code>StateStore</code> was introduced in [SPARK-13809][SQL] State store for streaming aggregations.</p> <p>Read the motivation and design in State Store for Streaming Aggregations.</p>","location":"stateful-stream-processing/StateStore/"},{"title":"Contract","text":"","location":"stateful-stream-processing/StateStore/#contract"},{"title":"Commit State Changes <pre><code>commit(): Long\n</code></pre> <p>Commits all updates (puts and removes) and returns a new version</p> <p>Used when:</p> <ul> <li><code>FlatMapGroupsWithStateExec</code> physical operator is requested to processDataWithPartition</li> <li><code>SessionWindowStateStoreSaveExec</code> physical operator is executed</li> <li><code>StreamingDeduplicateExec</code> physical operator is executed</li> <li><code>StreamingGlobalLimitExec</code> physical operator is executed</li> <li><code>StreamingAggregationStateManagerBaseImpl</code> is requested to commit</li> <li><code>StreamingSessionWindowStateManagerImplV1</code> is requested to <code>commit</code></li> <li><code>StateStoreHandler</code> is requested to commit</li> </ul>","text":"","location":"stateful-stream-processing/StateStore/#commit-state-changes"},{"title":"StateStoreMetrics <pre><code>metrics: StateStoreMetrics\n</code></pre> <p>StateStoreMetrics of this state store</p> <p>Used when:</p> <ul> <li><code>StateStoreWriter</code> physical operator is requested to setStoreMetrics</li> <li><code>StateStoreHandler</code> is requested for the metrics</li> </ul>","text":"","location":"stateful-stream-processing/StateStore/#statestoremetrics"},{"title":"Storing Value for Key <pre><code>put(\n  key: UnsafeRow,\n  value: UnsafeRow): Unit\n</code></pre> <p>Stores (puts) a new non-<code>null</code> value for a non-<code>null</code> key</p> <p>Used when:</p> <ul> <li><code>StreamingDeduplicateExec</code> physical operator is executed</li> <li><code>StreamingGlobalLimitExec</code> physical operator is executed</li> <li><code>StateManagerImplBase</code> is requested to putState</li> <li><code>StreamingAggregationStateManagerImplV2</code>(and <code>StreamingAggregationStateManagerImplV1</code>) is requested to put a row</li> <li><code>StreamingSessionWindowStateManagerImplV1</code> is requested to <code>putRows</code></li> <li><code>KeyToNumValuesStore</code> is requested to put the number of values of a key</li> <li><code>KeyWithIndexToValueStore</code> is requested to put a new value of a key</li> </ul>","text":"","location":"stateful-stream-processing/StateStore/#storing-value-for-key"},{"title":"Removing Key <pre><code>remove(\n  key: UnsafeRow): Unit\n</code></pre> <p>Removes a non-<code>null</code> key</p> <p>Used when:</p> <ul> <li><code>WatermarkSupport</code> physical operator is requested to removeKeysOlderThanWatermark</li> <li><code>StateManagerImplBase</code> is requested to removeState</li> <li><code>StreamingAggregationStateManagerBaseImpl</code> is requested to remove a key</li> <li><code>StreamingSessionWindowStateManagerImplV1</code> is requested to <code>removeByValueCondition</code> and <code>putRows</code></li> <li><code>KeyToNumValuesStore</code> is requested to remove a key</li> <li><code>KeyWithIndexToValueStore</code> is requested to remove a key</li> </ul>","text":"","location":"stateful-stream-processing/StateStore/#removing-key"},{"title":"Implementations","text":"<ul> <li>HDFSBackedStateStore</li> <li>RocksDBStateStore</li> </ul>","location":"stateful-stream-processing/StateStore/#implementations"},{"title":"Review Me","text":"<p><code>StateStore</code> supports incremental checkpointing in which only the key-value \"Row\" pairs that changed are &lt;&gt; or &lt;&gt; (without touching other key-value pairs). <p><code>StateStore</code> is identified with the &lt;&gt; (among other properties for identification). <p>[[logging]] [TIP] ==== Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.streaming.state.StateStore$</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.streaming.state.StateStore$=ALL\n</code></pre>","location":"stateful-stream-processing/StateStore/#review-me"},{"title":"Refer to &lt;&gt;. <p>=== [[coordinatorRef]] Creating (and Caching) RPC Endpoint Reference to StateStoreCoordinator for Executors</p>","text":"","location":"stateful-stream-processing/StateStore/#refer-to"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/StateStore/#source-scala"},{"title":"coordinatorRef: Option[StateStoreCoordinatorRef]","text":"<p><code>coordinatorRef</code> requests the <code>SparkEnv</code> helper object for the current <code>SparkEnv</code>.</p> <p>If the <code>SparkEnv</code> is available and the &lt;&lt;_coordRef, _coordRef&gt;&gt; is not assigned yet, <code>coordinatorRef</code> prints out the following DEBUG message to the logs followed by requesting the <code>StateStoreCoordinatorRef</code> for the StateStoreCoordinator endpoint.</p> <pre><code>Getting StateStoreCoordinatorRef\n</code></pre> <p>If the <code>SparkEnv</code> is available, <code>coordinatorRef</code> prints out the following INFO message to the logs:</p> <pre><code>Retrieved reference to StateStoreCoordinator: [_coordRef]\n</code></pre> <p>NOTE: <code>coordinatorRef</code> is used when <code>StateStore</code> helper object is requested to &lt;&gt; (when <code>StateStore</code> object helper is requested to &lt;&gt;) and &lt;&gt; (when <code>StateStore</code> object helper is requested to &lt;&gt;). <p>=== [[reportActiveStoreInstance]] Announcing New StateStoreProvider</p>","location":"stateful-stream-processing/StateStore/#coordinatorref-optionstatestorecoordinatorref"},{"title":"[source, scala]","text":"<p>reportActiveStoreInstance(   storeProviderId: StateStoreProviderId): Unit</p>  <p><code>reportActiveStoreInstance</code> takes the current host and <code>executorId</code> (from the <code>BlockManager</code> on the Spark executor) and requests the &lt;&gt; to reportActiveInstance. <p>NOTE: <code>reportActiveStoreInstance</code> uses <code>SparkEnv</code> to access the <code>BlockManager</code>.</p> <p>In the end, <code>reportActiveStoreInstance</code> prints out the following INFO message to the logs:</p> <pre><code>Reported that the loaded instance [storeProviderId] is active\n</code></pre> <p>NOTE: <code>reportActiveStoreInstance</code> is used exclusively when <code>StateStore</code> utility is requested to &lt;&gt;. <p>=== [[MaintenanceTask]] <code>MaintenanceTask</code> Daemon Thread</p> <p><code>MaintenanceTask</code> is a daemon thread that &lt;&gt;. <p>When an error occurs, <code>MaintenanceTask</code> clears &lt;&gt; internal registry. <p><code>MaintenanceTask</code> is scheduled on state-store-maintenance-task thread pool that runs periodically every spark.sql.streaming.stateStore.maintenanceInterval.</p>","location":"stateful-stream-processing/StateStore/#source-scala_1"},{"title":"Looking Up StateStore by Provider ID <pre><code>get(\n  storeProviderId: StateStoreProviderId,\n  keySchema: StructType,\n  valueSchema: StructType,\n  indexOrdinal: Option[Int],\n  version: Long,\n  storeConf: StateStoreConf,\n  hadoopConf: Configuration): StateStore\n</code></pre> <p><code>get</code> finds <code>StateStore</code> for the specified StateStoreProviderId and version.</p> <p>NOTE: The version is either the &lt;&gt; (in Continuous Stream Processing) or the current batch ID (in Micro-Batch Stream Processing). <p>Internally, <code>get</code> looks up the &lt;&gt; (by <code>storeProviderId</code>) in the &lt;&gt; internal cache. If unavailable, <code>get</code> uses the <code>StateStoreProvider</code> utility to &lt;&gt;. <p><code>get</code> will also &lt;&gt; (unless already started) and &lt;&gt;. <p>In the end, <code>get</code> requests the <code>StateStoreProvider</code> to &lt;&gt;. <p><code>get</code> is used when:</p> <ul> <li> <p><code>StateStoreRDD</code> is requested to compute a partition</p> </li> <li> <p><code>StateStoreHandler</code> (of SymmetricHashJoinStateManager) is requested to &lt;&gt;   <p>==== [[startMaintenanceIfNeeded]] Starting Periodic Maintenance Task (Unless Already Started) -- <code>startMaintenanceIfNeeded</code> Internal Object Method</p>","text":"","location":"stateful-stream-processing/StateStore/#looking-up-statestore-by-provider-id"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/StateStore/#source-scala_2"},{"title":"startMaintenanceIfNeeded(): Unit <p><code>startMaintenanceIfNeeded</code> schedules &lt;&gt; to start after and every spark.sql.streaming.stateStore.maintenanceInterval (defaults to <code>60s</code>). <p>NOTE: <code>startMaintenanceIfNeeded</code> does nothing when the maintenance task has already been started and is still running.</p> <p>NOTE: <code>startMaintenanceIfNeeded</code> is used exclusively when <code>StateStore</code> is requested to &lt;&gt;. <p>==== [[doMaintenance]] Doing State Maintenance of Registered State Store Providers -- <code>doMaintenance</code> Internal Object Method</p>","text":"","location":"stateful-stream-processing/StateStore/#startmaintenanceifneeded-unit"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/StateStore/#source-scala_3"},{"title":"doMaintenance(): Unit <p>Internally, <code>doMaintenance</code> prints the following DEBUG message to the logs:</p> <pre><code>Doing maintenance\n</code></pre> <p><code>doMaintenance</code> then requests every StateStoreProvider.md[StateStoreProvider] (registered in &lt;&gt;) to StateStoreProvider.md#doMaintenance[do its own internal maintenance] (only when a <code>StateStoreProvider</code> &lt;&gt;). <p>When a <code>StateStoreProvider</code> is &lt;&gt;, <code>doMaintenance</code> &lt;&gt; and prints the following INFO message to the logs: <pre><code>Unloaded [provider]\n</code></pre> <p>NOTE: <code>doMaintenance</code> is used exclusively in &lt;&gt;.","text":"","location":"stateful-stream-processing/StateStore/#domaintenance-unit"},{"title":"StateStoreConf","text":"","location":"stateful-stream-processing/StateStoreConf/"},{"title":"minDeltasForSnapshot <p>spark.sql.streaming.stateStore.minDeltasForSnapshot</p>","text":"","location":"stateful-stream-processing/StateStoreConf/#mindeltasforsnapshot"},{"title":"maxVersionsToRetainInMemory <p>spark.sql.streaming.maxBatchesToRetainInMemory</p>","text":"","location":"stateful-stream-processing/StateStoreConf/#maxversionstoretaininmemory"},{"title":"minVersionsToRetain <p>spark.sql.streaming.minBatchesToRetain</p> <p>Used when <code>HDFSBackedStateStoreProvider</code> is requested for cleanup.</p>","text":"","location":"stateful-stream-processing/StateStoreConf/#minversionstoretain"},{"title":"providerClass <p>spark.sql.streaming.stateStore.providerClass</p> <p>Used when <code>StateStoreProvider</code> helper object is requested to create and initialize the StateStoreProvider.</p>","text":"","location":"stateful-stream-processing/StateStoreConf/#providerclass"},{"title":"StateStoreCoordinator RPC Endpoint","text":"<p><code>StateStoreCoordinator</code> keeps track of StateStores on Spark executors (per host and executor ID).</p> <p><code>StateStoreCoordinator</code> is used by <code>StateStoreRDD</code> when requested to get the location preferences of partitions (based on the location of the stores).</p> <p><code>StateStoreCoordinator</code> is a <code>ThreadSafeRpcEndpoint</code> RPC endpoint that manipulates &lt;&gt; registry through &lt;&gt;. <p>[[messages]] .StateStoreCoordinator RPC Endpoint's Messages and Message Handlers [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Message | Message Handler</p> <p>| DeactivateInstances a| [[DeactivateInstances]] Removes &lt;&gt; of a streaming query (given <code>runId</code>)  <p>Internally, <code>StateStoreCoordinator</code> finds the <code>StateStoreProviderIds</code> of the streaming query per <code>queryRunId</code> and the given <code>runId</code> and removes them from the &lt;&gt; internal registry. <p><code>StateStoreCoordinator</code> prints out the following DEBUG message to the logs:</p> <pre><code>Deactivating instances related to checkpoint location [runId]: [storeIdsToRemove]\n</code></pre> <p>| GetLocation a| [[GetLocation]] Gives the location of &lt;&gt; (from &lt;&gt;) with the host and an executor id on that host. <p>You should see the following DEBUG message in the logs:</p> <pre><code>Got location of the state store [id]: [executorId]\n</code></pre> <p>| ReportActiveInstance a| [[ReportActiveInstance]] One-way asynchronous (fire-and-forget) message to register a new &lt;&gt; on an executor (given <code>host</code> and <code>executorId</code>). <p>Sent out exclusively when StateStoreCoordinatorRef RPC endpoint reference is requested to reportActiveInstance (when <code>StateStore</code> utility is requested to look up the StateStore by provider ID when the <code>StateStore</code> and a corresponding <code>StateStoreProvider</code> were just created and initialized).</p>  <p>Internally, <code>StateStoreCoordinator</code> prints out the following DEBUG message to the logs:</p> <pre><code>Reported state store [id] is active at [executorId]\n</code></pre> <p>In the end, <code>StateStoreCoordinator</code> adds the <code>StateStoreProviderId</code> to the &lt;&gt; internal registry. <p>| StopCoordinator a| [[StopCoordinator]] Stops <code>StateStoreCoordinator</code> RPC Endpoint</p> <p>You should see the following DEBUG message in the logs:</p> <pre><code>StateStoreCoordinator stopped\n</code></pre> <p>| VerifyIfInstanceActive a| [[VerifyIfInstanceActive]] Verifies if a given &lt;&gt; is registered (in &lt;&gt;) on <code>executorId</code> <p>You should see the following DEBUG message in the logs:</p> <p><pre><code>Verified that state store [id] is active: [response]\n</code></pre> |===</p> <p>[[logging]] [TIP] ==== Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.streaming.state.StateStoreCoordinator</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.streaming.state.StateStoreCoordinator=ALL\n</code></pre>","location":"stateful-stream-processing/StateStoreCoordinator/"},{"title":"Refer to &lt;&gt;. <p>=== [[instances]] <code>instances</code> Internal Registry</p>","text":"","location":"stateful-stream-processing/StateStoreCoordinator/#refer-to"},{"title":"[source,scala]","text":"","location":"stateful-stream-processing/StateStoreCoordinator/#sourcescala"},{"title":"instances: HashMap[StateStoreProviderId, ExecutorCacheTaskLocation]","text":"<p><code>instances</code> is an internal registry of &lt;&gt; by their &lt;&gt; and <code>ExecutorCacheTaskLocations</code> (with a <code>host</code> and a <code>executorId</code>). <ul> <li> <p>A new <code>StateStoreProviderId</code> added when <code>StateStoreCoordinator</code> is requested to &lt;&gt;  <li> <p>All <code>StateStoreProviderIds</code> of a streaming query are removed  when <code>StateStoreCoordinator</code> is requested to &lt;&gt;","location":"stateful-stream-processing/StateStoreCoordinator/#instances-hashmapstatestoreproviderid-executorcachetasklocation"},{"title":"StateStoreCoordinatorRef","text":"<p><code>StateStoreCoordinatorRef</code> is used to (let the tasks on Spark executors to) send &lt;&gt; to the &lt;&gt; (that lives on the driver). <p>[[creating-instance]] [[rpcEndpointRef]] <code>StateStoreCoordinatorRef</code> is given the <code>RpcEndpointRef</code> to the StateStoreCoordinator RPC endpoint when created.</p> <p><code>StateStoreCoordinatorRef</code> is &lt;&gt; through <code>StateStoreCoordinatorRef</code> helper object when requested to create one for the &lt;&gt; (when StreamingQueryManager is created) or an &lt;&gt; (when <code>StateStore</code> helper object is requested for the RPC endpoint reference to StateStoreCoordinator for Executors). <p>[[messages]] .StateStoreCoordinatorRef's Methods and Underlying RPC Messages [width=\"100%\",cols=\"1m,3\",options=\"header\"] |=== | Method | Description</p> <p>| deactivateInstances a| [[deactivateInstances]]</p>","location":"stateful-stream-processing/StateStoreCoordinatorRef/"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/StateStoreCoordinatorRef/#source-scala"},{"title":"deactivateInstances(runId: UUID): Unit","text":"<p>Requests the RpcEndpointRef to send a DeactivateInstances synchronous message with the given <code>runId</code> and waits for a <code>true</code> / <code>false</code> response</p> <p>Used exclusively when <code>StreamingQueryManager</code> is requested to handle termination of a streaming query (when <code>StreamExecution</code> is requested to run a streaming query and the query has finished (running streaming batches)).</p> <p>| getLocation a| [[getLocation]]</p>","location":"stateful-stream-processing/StateStoreCoordinatorRef/#deactivateinstancesrunid-uuid-unit"},{"title":"[source, scala]","text":"<p>getLocation(   stateStoreProviderId: StateStoreProviderId): Option[String]</p>  <p>Requests the RpcEndpointRef to send a GetLocation synchronous message with the given StateStoreProviderId and waits for the location</p> <p>Used when:</p> <ul> <li> <p><code>StateStoreAwareZipPartitionsRDD</code> is requested for the preferred locations of a partition (when StreamingSymmetricHashJoinExec physical operator is executed</p> </li> <li> <p><code>StateStoreRDD</code> is requested for preferred locations for a task for a partition</p> </li> </ul> <p>| reportActiveInstance a| [[reportActiveInstance]]</p>","location":"stateful-stream-processing/StateStoreCoordinatorRef/#source-scala_1"},{"title":"[source, scala]","text":"<p>reportActiveInstance(   stateStoreProviderId: StateStoreProviderId,   host: String,   executorId: String): Unit</p>  <p>Requests the RpcEndpointRef to send a ReportActiveInstance one-way asynchronous (fire-and-forget) message with the given StateStoreProviderId, <code>host</code> and <code>executorId</code></p> <p>Used when <code>StateStore</code> utility is requested for reportActiveStoreInstance (when <code>StateStore</code> utility is requested to look up the StateStore by StateStoreProviderId)</p> <p>| stop a| [[stop]]</p>","location":"stateful-stream-processing/StateStoreCoordinatorRef/#source-scala_2"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/StateStoreCoordinatorRef/#source-scala_3"},{"title":"stop(): Unit","text":"<p>Requests the RpcEndpointRef to send a StopCoordinator synchronous message</p> <p>Used exclusively for unit testing</p> <p>| verifyIfInstanceActive a| [[verifyIfInstanceActive]]</p>","location":"stateful-stream-processing/StateStoreCoordinatorRef/#stop-unit"},{"title":"[source, scala]","text":"<p>verifyIfInstanceActive(   stateStoreProviderId: StateStoreProviderId,   executorId: String): Boolean</p>  <p>Requests the RpcEndpointRef to send a VerifyIfInstanceActive synchronous message with the given StateStoreProviderId and <code>executorId</code>, and waits for a <code>true</code> / <code>false</code> response</p> <p>Used when <code>StateStore</code> utility is requested for verifyIfStoreInstanceActive (when requested to doMaintenance from a running MaintenanceTask daemon thread)</p> <p>|===</p>","location":"stateful-stream-processing/StateStoreCoordinatorRef/#source-scala_4"},{"title":"Creating StateStoreCoordinatorRef to StateStoreCoordinator RPC Endpoint for Driver <pre><code>forDriver(\n  env: SparkEnv): StateStoreCoordinatorRef\n</code></pre> <p><code>forDriver</code>...FIXME</p> <p><code>forDriver</code> is used when <code>StreamingQueryManager</code> is created.</p>","text":"","location":"stateful-stream-processing/StateStoreCoordinatorRef/#creating-statestorecoordinatorref-to-statestorecoordinator-rpc-endpoint-for-driver"},{"title":"Creating StateStoreCoordinatorRef to StateStoreCoordinator RPC Endpoint for Executor <pre><code>forExecutor(\n  env: SparkEnv): StateStoreCoordinatorRef\n</code></pre> <p><code>forExecutor</code>...FIXME</p> <p><code>forExecutor</code> is used when <code>StateStore</code> utility is requested for the RPC endpoint reference to StateStoreCoordinator for Executors.</p>","text":"","location":"stateful-stream-processing/StateStoreCoordinatorRef/#creating-statestorecoordinatorref-to-statestorecoordinator-rpc-endpoint-for-executor"},{"title":"StateStoreCustomMetric","text":"<p><code>StateStoreCustomMetric</code> is the &lt;&gt; of &lt;&gt; that a state store may wish to expose (as StateStoreMetrics or &lt;&gt;). <p><code>StateStoreCustomMetric</code> is used when:</p> <ul> <li> <p><code>StateStoreProvider</code> is requested for the &lt;&gt;  <li> <p><code>StateStoreMetrics</code> is created</p> </li>  <p>[[contract]] .StateStoreCustomMetric Contract [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| desc a| [[desc]]</p>","location":"stateful-stream-processing/StateStoreCustomMetric/"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/StateStoreCustomMetric/#source-scala"},{"title":"desc: String","text":"<p>Description of the custom metrics</p> <p>| name a| [[name]]</p>","location":"stateful-stream-processing/StateStoreCustomMetric/#desc-string"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/StateStoreCustomMetric/#source-scala_1"},{"title":"name: String","text":"<p>Name of the custom metrics</p> <p>|===</p> <p>[[implementations]] .StateStoreCustomMetrics [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | StateStoreCustomMetric | Description</p> <p>| StateStoreCustomSizeMetric | [[StateStoreCustomSizeMetric]]</p> <p>| StateStoreCustomSumMetric | [[StateStoreCustomSumMetric]]</p> <p>| StateStoreCustomTimingMetric | [[StateStoreCustomTimingMetric]] |===</p>","location":"stateful-stream-processing/StateStoreCustomMetric/#name-string"},{"title":"StateStoreId","text":"<p>[[creating-instance]] <code>StateStoreId</code> is a unique identifier of a StateStore with the following attributes:</p> <ul> <li>[[checkpointRootLocation]] Checkpoint Root Location - the root directory for state checkpointing</li> <li>[[operatorId]] Operator ID - a unique ID of the stateful operator</li> <li>[[partitionId]] Partition ID - the index of the partition</li> <li>[[storeName]] Store Name - the name of the state store (default: &lt;&gt;)  <p><code>StateStoreId</code> is &lt;&gt; when: <ul> <li> <p><code>StateStoreRDD</code> is requested for the preferred locations of a partition (executed on the driver) and to compute it (later on an executor)</p> </li> <li> <p><code>StateStoreProviderId</code> helper object is requested to create a &lt;&gt; (with a &lt;&gt; and the run ID of a streaming query) that is then used for the preferred locations of a partition of a <code>StateStoreAwareZipPartitionsRDD</code> (executed on the driver) and to...FIXME   <p>[[DEFAULT_STORE_NAME]] The name of the default state store (for reading state store data that was generated before store names were used, i.e. in Spark 2.2 and earlier) is default.</p> <p>=== [[storeCheckpointLocation]] State Checkpoint Base Directory of Stateful Operator -- <code>storeCheckpointLocation</code> Method</p>","location":"stateful-stream-processing/StateStoreId/"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/StateStoreId/#source-scala"},{"title":"storeCheckpointLocation(): Path","text":"<p><code>storeCheckpointLocation</code> is Hadoop DFS's https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path] of the checkpoint location (for the stateful operator by &lt;&gt;, the partition by the &lt;&gt; in the &lt;&gt;). <p>If the &lt;&gt; is used (for Spark 2.2 and earlier), the &lt;&gt; is not included in the path. <p><code>storeCheckpointLocation</code> is used when <code>HDFSBackedStateStoreProvider</code> is requested for the state checkpoint base directory.</p>","location":"stateful-stream-processing/StateStoreId/#storecheckpointlocation-path"},{"title":"StateStoreMetrics","text":"<p><code>StateStoreMetrics</code> holds the performance metrics of StateStores and SymmetricHashJoinStateManager.</p>","location":"stateful-stream-processing/StateStoreMetrics/"},{"title":"Creating Instance","text":"<p><code>StateStoreMetrics</code> takes the following to be created:</p> <ul> <li> Number of Keys <li> Memory used (in bytes) <li>Custom Metrics</li>  <p><code>StateStoreMetrics</code> is created when:</p> <ul> <li><code>HDFSBackedStateStore</code> is requested for metrics</li> <li><code>RocksDBStateStore</code> is requested for metrics</li> <li><code>StateStoreMetrics</code> is requested to combine metrics</li> <li><code>SymmetricHashJoinStateManager</code> is requested for metrics</li> </ul>","location":"stateful-stream-processing/StateStoreMetrics/#creating-instance"},{"title":"Custom Metrics <p><code>StateStoreMetrics</code> is given StateStoreCustomMetrics and their current values when created.</p>","text":"","location":"stateful-stream-processing/StateStoreMetrics/#custom-metrics"},{"title":"StateStoreOps","text":"<p>[[dataRDD]] <code>StateStoreOps</code> is a Scala implicit class of a data RDD (of type <code>RDD[T]</code>) to create a StateStoreRDD for the following physical operators:</p> <ul> <li> <p>FlatMapGroupsWithStateExec</p> </li> <li> <p>StateStoreRestoreExec</p> </li> <li> <p>StateStoreSaveExec</p> </li> <li> <p>StreamingDeduplicateExec</p> </li> </ul>  <p>Note</p> <p>Implicit Classes are a language feature in Scala for implicit conversions with extension methods for existing types.</p>","location":"stateful-stream-processing/StateStoreOps/"},{"title":"Creating StateStoreRDD (with storeUpdateFunction Aborting StateStore When Task Fails) <pre><code>mapPartitionsWithStateStore[U](\n  stateInfo: StatefulOperatorStateInfo,\n  keySchema: StructType,\n  valueSchema: StructType,\n  indexOrdinal: Option[Int],\n  sessionState: SessionState,\n  storeCoordinator: Option[StateStoreCoordinatorRef])(\n  storeUpdateFunction: (StateStore, Iterator[T]) =&gt; Iterator[U]): StateStoreRDD[T, U]\n</code></pre> <p>Internally, <code>mapPartitionsWithStateStore</code> requests <code>SparkContext</code> to clean <code>storeUpdateFunction</code> function.</p> <p>NOTE: <code>mapPartitionsWithStateStore</code> uses the &lt;&gt; to access the current <code>SparkContext</code>. <p>NOTE: Function Cleaning is to clean a closure from unreferenced variables before it is serialized and sent to tasks. <code>SparkContext</code> reports a <code>SparkException</code> when the closure is not serializable.</p> <p><code>mapPartitionsWithStateStore</code> then creates a (wrapper) function to abort the <code>StateStore</code> if state updates had not been committed before a task finished (which is to make sure that the <code>StateStore</code> has been committed or aborted in the end to follow the contract of <code>StateStore</code>).</p> <p>NOTE: <code>mapPartitionsWithStateStore</code> uses <code>TaskCompletionListener</code> to be notified when a task has finished.</p> <p>In the end, <code>mapPartitionsWithStateStore</code> creates a StateStoreRDD (with the wrapper function, <code>SessionState</code> and StateStoreCoordinatorRef).</p> <p><code>mapPartitionsWithStateStore</code> is used when the following physical operators are executed:</p> <ul> <li>FlatMapGroupsWithStateExec</li> <li>StateStoreRestoreExec</li> <li>StateStoreSaveExec</li> <li>StreamingDeduplicateExec</li> <li>StreamingGlobalLimitExec</li> </ul>","text":"","location":"stateful-stream-processing/StateStoreOps/#creating-statestorerdd-with-storeupdatefunction-aborting-statestore-when-task-fails"},{"title":"StateStoreProvider","text":"<p><code>StateStoreProvider</code> is an abstraction of StateStore providers that manage state stores (state data) in Stateful Stream Processing (e.g., for persisting running aggregates in Streaming Aggregation).</p> <p><code>StateStoreProvider</code> utility uses spark.sql.streaming.stateStore.providerClass configuration property to choose the default implementation.</p>","location":"stateful-stream-processing/StateStoreProvider/"},{"title":"Contract","text":"","location":"stateful-stream-processing/StateStoreProvider/#contract"},{"title":"Initialization <pre><code>init(\n  stateStoreId: StateStoreId,\n  keySchema: StructType,\n  valueSchema: StructType,\n  numColsPrefixKey: Int,\n  storeConfs: StateStoreConf,\n  hadoopConf: Configuration): Unit\n</code></pre> <p>Initializes a StateStoreProvider for the given StateStoreId</p> <p>Used when:</p> <ul> <li><code>StateStoreProvider</code> utility is used to create and initialize a StateStoreProvider</li> </ul>","text":"","location":"stateful-stream-processing/StateStoreProvider/#initialization"},{"title":"Implementations","text":"<ul> <li>HDFSBackedStateStoreProvider</li> <li>RocksDBStateStoreProvider</li> </ul>","location":"stateful-stream-processing/StateStoreProvider/#implementations"},{"title":"Creating and Initializing StateStoreProvider <pre><code>createAndInit(\n  providerId: StateStoreProviderId,\n  keySchema: StructType,\n  valueSchema: StructType,\n  numColsPrefixKey: Int,\n  storeConf: StateStoreConf,\n  hadoopConf: Configuration): StateStoreProvider\n</code></pre> <p><code>createAndInit</code> creates a StateStoreProvider based on spark.sql.streaming.stateStore.providerClass.</p> <p>In the end, <code>createAndInit</code> requests the <code>StateStoreProvider</code> to initialize.</p>  <p><code>createAndInit</code> is used when:</p> <ul> <li><code>StateStore</code> utility is used to look up a StateStore by StateStoreProviderId</li> </ul>","text":"","location":"stateful-stream-processing/StateStoreProvider/#creating-and-initializing-statestoreprovider"},{"title":"StateStoreProviderId","text":"<p>[[creating-instance]] <code>StateStoreProviderId</code> is a unique identifier of a StateStoreProvider with the following properties:</p> <ul> <li>[[storeId]] StateStoreId</li> <li>[[queryRunId]] Run ID of a streaming query (java.util.UUID)</li> </ul> <p>In other words, <code>StateStoreProviderId</code> is a &lt;&gt; with the &lt;&gt; that is different every restart. <p><code>StateStoreProviderId</code> is used by the following execution components:</p> <ul> <li> <p><code>StateStoreCoordinator</code> to track the executors of state store providers (on the driver)</p> </li> <li> <p><code>StateStore</code> object to manage state store providers (on executors)</p> </li> </ul> <p><code>StateStoreProviderId</code> is &lt;&gt; (directly or using &lt;&gt; factory method) when: <ul> <li> <p><code>StateStoreRDD</code> is requested for the placement preferences of a partition and to compute a partition</p> </li> <li> <p><code>StateStoreAwareZipPartitionsRDD</code> is requested for the preferred locations of a partition</p> </li> <li> <p><code>StateStoreHandler</code> is requested to look up a state store</p> </li> </ul>","location":"stateful-stream-processing/StateStoreProviderId/"},{"title":"Creating StateStoreProviderId <pre><code>apply(\n  stateInfo: StatefulOperatorStateInfo,\n  partitionIndex: Int,\n  storeName: String): StateStoreProviderId\n</code></pre> <p><code>apply</code> simply creates a &lt;&gt; for the StatefulOperatorStateInfo, the partition and the store name. <p>Internally, <code>apply</code> requests the <code>StatefulOperatorStateInfo</code> for the checkpoint directory (checkpointLocation) and the stateful operator ID and creates a new StateStoreId (with the <code>partitionIndex</code> and <code>storeName</code>).</p> <p>In the end, <code>apply</code> requests the <code>StatefulOperatorStateInfo</code> for the run ID of a streaming query and creates a &lt;&gt; (together with the run ID). <p><code>apply</code> is used when:</p> <ul> <li> <p><code>StateStoreAwareZipPartitionsRDD</code> is requested for the preferred locations of a partition</p> </li> <li> <p><code>StateStoreHandler</code> is requested to look up a state store</p> </li> </ul>","text":"","location":"stateful-stream-processing/StateStoreProviderId/#creating-statestoreproviderid"},{"title":"StateStoreRDD","text":"<p><code>StateStoreRDD</code> is an <code>RDD</code> for &lt;&gt; with StateStore (and data from partitions of the &lt;&gt;). <p><code>StateStoreRDD</code> is &lt;&gt; for the following stateful physical operators (using StateStoreOps.mapPartitionsWithStateStore): <ul> <li>FlatMapGroupsWithStateExec</li> <li>StateStoreRestoreExec</li> <li>StateStoreSaveExec</li> <li>StreamingDeduplicateExec</li> <li>StreamingGlobalLimitExec</li> </ul> <p></p> <p><code>StateStoreRDD</code> uses <code>StateStoreCoordinator</code> for the &lt;&gt; for job scheduling. <p></p> <p>[[getPartitions]] <code>getPartitions</code> is exactly the partitions of the &lt;&gt;.","location":"stateful-stream-processing/StateStoreRDD/"},{"title":"Computing Partition <pre><code>compute(\n  partition: Partition,\n  ctxt: TaskContext): Iterator[U]\n</code></pre> <p><code>compute</code> is part of the <code>RDD</code> abstraction.</p> <p><code>compute</code> computes &lt;&gt; passing the result on to &lt;&gt; (with a configured StateStore). <p>Internally, (and similarly to &lt;&gt;) <code>compute</code> creates a &lt;&gt; with <code>StateStoreId</code> (using &lt;&gt;, &lt;&gt; and the index of the input <code>partition</code>) and &lt;&gt;. <p><code>compute</code> then requests <code>StateStore</code> for the store for the StateStoreProviderId.</p> <p>In the end, <code>compute</code> computes &lt;&gt; (using the input <code>partition</code> and <code>ctxt</code>) followed by executing &lt;&gt; (with the store and the result). <p>=== [[getPreferredLocations]] Placement Preferences of Partition (Preferred Locations) -- <code>getPreferredLocations</code> Method</p>","text":"","location":"stateful-stream-processing/StateStoreRDD/#computing-partition"},{"title":"[source, scala]","text":"","location":"stateful-stream-processing/StateStoreRDD/#source-scala"},{"title":"getPreferredLocations(partition: Partition): Seq[String] <p>NOTE: <code>getPreferredLocations</code> is a part of the RDD Contract to specify placement preferences (aka preferred task locations), i.e. where tasks should be executed to be as close to the data as possible.</p> <p><code>getPreferredLocations</code> creates a &lt;&gt; with <code>StateStoreId</code> (using &lt;&gt;, &lt;&gt; and the index of the input <code>partition</code>) and &lt;&gt;. <p>NOTE: &lt;&gt; and &lt;&gt; are shared across different partitions and so the only difference in &lt;&gt; is the partition index. <p>In the end, <code>getPreferredLocations</code> requests &lt;&gt; for the location of the state store for the StateStoreProviderId.","text":"","location":"stateful-stream-processing/StateStoreRDD/#getpreferredlocationspartition-partition-seqstring"},{"title":"Creating Instance <p><code>StateStoreRDD</code> takes the following to be created:</p> <ul> <li>[[dataRDD]] Data RDD (<code>RDD[T]</code> to update the aggregates in a state store)</li> <li>[[storeUpdateFunction]] Store update function (<code>(StateStore, Iterator[T]) =&gt; Iterator[U]</code> where <code>T</code> is the type of rows in the &lt;&gt;) <li>[[checkpointLocation]] Checkpoint directory</li> <li>[[queryRunId]] Run ID of the streaming query</li> <li>[[operatorId]] Operator ID</li> <li>[[storeVersion]] Version of the store</li> <li>[[keySchema]] Key schema - schema of the keys</li> <li>[[valueSchema]] Value schema - schema of the values</li> <li>[[indexOrdinal]] Index</li> <li>[[sessionState]] <code>SessionState</code></li> <li>[[storeCoordinator]] Optional StateStoreCoordinatorRef</li>  <p>=== [[internal-properties]] Internal Properties</p> <p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| hadoopConfBroadcast | [[hadoopConfBroadcast]]</p> <p>| storeConf | [[storeConf]] Configuration parameters (as <code>StateStoreConf</code>) using the current <code>SQLConf</code> (from <code>SessionState</code>) |===</p>","text":"","location":"stateful-stream-processing/StateStoreRDD/#creating-instance"},{"title":"StatefulOperatorPartitioning","text":"","location":"stateful-stream-processing/StatefulOperatorPartitioning/"},{"title":"getCompatibleDistribution <pre><code>getCompatibleDistribution(\n  expressions: Seq[Expression],\n  numPartitions: Int,\n  conf: SQLConf): Distribution\ngetCompatibleDistribution(\n  expressions: Seq[Expression],\n  stateInfo: StatefulOperatorStateInfo,\n  conf: SQLConf): Distribution\n</code></pre> <p><code>getCompatibleDistribution</code> returns the following <code>Distribution</code>s (Spark SQL) based on spark.sql.streaming.statefulOperator.useStrictDistribution configuration property:</p> <ul> <li><code>StatefulOpClusteredDistribution</code> when enabled</li> <li><code>ClusteredDistribution</code> (Spark SQL) otherwise</li> </ul>  <p><code>getCompatibleDistribution</code> is used when:</p> <ul> <li><code>BaseAggregateExec</code> (Spark SQL) is requested for the required child output distribution (of a streaming query)</li> <li><code>UpdatingSessionsExec</code> is requested for the required child output distribution (of a streaming query)</li> <li><code>FlatMapGroupsWithStateExec</code> is requested for the required child output distribution</li> <li><code>StateStoreRestoreExec</code> is requested for the required child output distribution</li> <li><code>StateStoreSaveExec</code> is requested for the required child output distribution</li> <li><code>SessionWindowStateStoreRestoreExec</code> is requested for the required child output distribution</li> <li><code>SessionWindowStateStoreSaveExec</code> is requested for the required child output distribution</li> <li><code>StreamingDeduplicateExec</code> is requested for the required child output distribution</li> </ul>","text":"","location":"stateful-stream-processing/StatefulOperatorPartitioning/#getcompatibledistribution"},{"title":"StatefulOperatorStateInfo","text":"<p><code>StatefulOperatorStateInfo</code> identifies the state store for a given stateful physical operator:</p> <ul> <li>[[checkpointLocation]] Checkpoint directory (<code>checkpointLocation</code>)</li> <li>[[queryRunId]] &lt;&gt; of a streaming query (<code>queryRunId</code>) <li>[[operatorId]] Stateful operator ID (<code>operatorId</code>)</li> <li>[[storeVersion]] &lt;&gt; (<code>storeVersion</code>) <li>[[numPartitions]] Number of partitions</li>  <p><code>StatefulOperatorStateInfo</code> is &lt;&gt; exclusively when <code>IncrementalExecution</code> is requested for nextStatefulOperationStateInfo. <p>[[toString]] When requested for a textual representation (<code>toString</code>), <code>StatefulOperatorStateInfo</code> returns the following:</p> <pre><code>state info [ checkpoint = [checkpointLocation], runId = [queryRunId], opId = [operatorId], ver = [storeVersion], numPartitions = [numPartitions]]\n</code></pre>","location":"stateful-stream-processing/StatefulOperatorStateInfo/"},{"title":"State Version and Batch ID <p>When created (when <code>IncrementalExecution</code> is requested for the next StatefulOperatorStateInfo), a <code>StatefulOperatorStateInfo</code> is given a state version.</p> <p>The state version is exactly the batch ID of the IncrementalExecution.</p>","text":"","location":"stateful-stream-processing/StatefulOperatorStateInfo/#state-version-and-batch-id"},{"title":"Streaming Aggregation","text":"<p>In Spark Structured Streaming, Streaming Aggregation is an operation in a streaming query that was described (build) using the following high-level streaming operators:</p> <ul> <li> <p>Dataset.groupBy, <code>Dataset.rollup</code>, <code>Dataset.cube</code> (that all create a <code>RelationalGroupedDataset</code> (Spark SQL))</p> </li> <li> <p>Dataset.groupByKey (that creates a <code>KeyValueGroupedDataset</code> (Spark SQL))</p> </li> <li> <p>SQL's <code>GROUP BY</code> clause (including <code>WITH CUBE</code> and <code>WITH ROLLUP</code> (Spark SQL))</p> </li> </ul> <p>Streaming aggregation is part of Stateful Stream Processing.</p>","location":"streaming-aggregation/"},{"title":"IncrementalExecution <p>Under the covers, these high-level operators create logical query plans with <code>Aggregate</code> (Spark SQL) logical operators.</p> <p>Spark Structured Streaming uses IncrementalExecution for planning streaming queries for execution.</p> <p>At query planning, <code>IncrementalExecution</code> uses the StatefulAggregationStrategy execution planning strategy for planning streaming aggregations (<code>Aggregate</code> unary logical operators) as pairs of StateStoreRestoreExec and StateStoreSaveExec physical operators.</p>","text":"","location":"streaming-aggregation/#incrementalexecution"},{"title":"Demo <pre><code>// input data from a data source\n// it's rate data source\n// but that does not really matter\n// We need a streaming Dataset\nval input = spark\n  .readStream\n  .format(\"rate\")\n  .load\n\n// Streaming aggregation with groupBy\nval counts = input\n  .groupBy($\"value\" % 2)\n  .count\n\ncounts.explain(extended = true)\n/**\n== Parsed Logical Plan ==\n'Aggregate [('value % 2)], [('value % 2) AS (value % 2)#23, count(1) AS count#22L]\n+- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@7879348, rate, [timestamp#15, value#16L]\n\n== Analyzed Logical Plan ==\n(value % 2): bigint, count: bigint\nAggregate [(value#16L % cast(2 as bigint))], [(value#16L % cast(2 as bigint)) AS (value % 2)#23L, count(1) AS count#22L]\n+- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@7879348, rate, [timestamp#15, value#16L]\n\n== Optimized Logical Plan ==\nAggregate [(value#16L % 2)], [(value#16L % 2) AS (value % 2)#23L, count(1) AS count#22L]\n+- Project [value#16L]\n   +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@7879348, rate, [timestamp#15, value#16L]\n\n== Physical Plan ==\n*(4) HashAggregate(keys=[(value#16L % 2)#27L], functions=[count(1)], output=[(value % 2)#23L, count#22L])\n+- StateStoreSave [(value#16L % 2)#27L], state info [ checkpoint = &lt;unknown&gt;, runId = 8c0ae2be-5eaa-4038-bc29-a176abfaf885, opId = 0, ver = 0, numPartitions = 200], Append, 0, 2\n   +- *(3) HashAggregate(keys=[(value#16L % 2)#27L], functions=[merge_count(1)], output=[(value#16L % 2)#27L, count#29L])\n      +- StateStoreRestore [(value#16L % 2)#27L], state info [ checkpoint = &lt;unknown&gt;, runId = 8c0ae2be-5eaa-4038-bc29-a176abfaf885, opId = 0, ver = 0, numPartitions = 200], 2\n         +- *(2) HashAggregate(keys=[(value#16L % 2)#27L], functions=[merge_count(1)], output=[(value#16L % 2)#27L, count#29L])\n            +- Exchange hashpartitioning((value#16L % 2)#27L, 200)\n               +- *(1) HashAggregate(keys=[(value#16L % 2) AS (value#16L % 2)#27L], functions=[partial_count(1)], output=[(value#16L % 2)#27L, count#29L])\n                  +- *(1) Project [value#16L]\n                     +- StreamingRelation rate, [timestamp#15, value#16L]\n*/\n</code></pre>","text":"","location":"streaming-aggregation/#demo"},{"title":"More Demos <p>Learn more in the following demos:</p> <ul> <li> <p>Streaming Watermark with Aggregation in Append Output Mode</p> </li> <li> <p>Streaming Query for Running Counts (Socket Source and Complete Output Mode)</p> </li> <li> <p>Streaming Aggregation with Kafka Data Source</p> </li> <li> <p>groupByKey Streaming Aggregation in Update Mode</p> </li> </ul>","text":"","location":"streaming-aggregation/#more-demos"},{"title":"StreamingAggregationStateManager","text":"<p><code>StreamingAggregationStateManager</code> is an abstraction of state managers for the physical operators used in Streaming Aggregation (e.g., StateStoreSaveExec and StateStoreRestoreExec).</p>","location":"streaming-aggregation/StreamingAggregationStateManager/"},{"title":"Contract","text":"","location":"streaming-aggregation/StreamingAggregationStateManager/#contract"},{"title":"Committing State Changes <pre><code>commit(\n  store: StateStore): Long\n</code></pre> <p>Commits all updates (changes) to the given StateStore and returns the new version</p> <p>See StreamingAggregationStateManagerBaseImpl</p> <p>Used when:</p> <ul> <li><code>StateStoreSaveExec</code> physical operator is executed</li> </ul>","text":"","location":"streaming-aggregation/StreamingAggregationStateManager/#committing-state-changes"},{"title":"Retrieving Value for Key (from ReadStateStore) <pre><code>get(\n  store: ReadStateStore,\n  key: UnsafeRow): UnsafeRow\n</code></pre> <p>Retrieves (gets) the current value for a given non-<code>null</code> key from ReadStateStore</p> <p>See StreamingAggregationStateManagerImplV2</p> <p>Used when:</p> <ul> <li><code>StateStoreRestoreExec</code> physical operator is executed</li> </ul>","text":"","location":"streaming-aggregation/StreamingAggregationStateManager/#retrieving-value-for-key-from-readstatestore"},{"title":"Extracting Key <pre><code>getKey(\n  row: UnsafeRow): UnsafeRow\n</code></pre> <p>Extracts the columns of a key from the given <code>row</code></p> <p>See StreamingAggregationStateManagerBaseImpl</p> <p>Used when:</p> <ul> <li><code>StateStoreRestoreExec</code> physical operator is executed</li> </ul>","text":"","location":"streaming-aggregation/StreamingAggregationStateManager/#extracting-key"},{"title":"Storing New Value for Key <pre><code>put(\n  store: StateStore,\n  row: UnsafeRow): Unit\n</code></pre> <p>Stores (puts) a new value for a non-<code>null</code> key to the StateStore. The key and the value are part of the given <code>row</code>. The key is extracted using getKey.</p> <p>See StreamingAggregationStateManagerImplV2</p> <p>Used when:</p> <ul> <li><code>StateStoreSaveExec</code> physical operator is executed</li> </ul>","text":"","location":"streaming-aggregation/StreamingAggregationStateManager/#storing-new-value-for-key"},{"title":"Removing Key <pre><code>remove(\n  store: StateStore,\n  key: UnsafeRow): Unit\n</code></pre> <p>Removes a non-<code>null</code> key from the StateStore</p> <p>See StreamingAggregationStateManagerBaseImpl</p> <p>Used when:</p> <ul> <li><code>WatermarkSupport</code> physical operator is requested to removeKeysOlderThanWatermark</li> <li><code>StateStoreSaveExec</code> physical operator is executed</li> </ul>","text":"","location":"streaming-aggregation/StreamingAggregationStateManager/#removing-key"},{"title":"Implementations","text":"<ul> <li>StreamingAggregationStateManagerBaseImpl</li> </ul>  Sealed Trait <p><code>StreamingAggregationStateManager</code> is a Scala sealed trait which means that all of the implementations are in the same compilation unit (a single file).</p> <p>Learn more in the Scala Language Specification.</p>","location":"streaming-aggregation/StreamingAggregationStateManager/#implementations"},{"title":"Creating StreamingAggregationStateManager <pre><code>createStateManager(\n  keyExpressions: Seq[Attribute],\n  inputRowAttributes: Seq[Attribute],\n  stateFormatVersion: Int): StreamingAggregationStateManager\n</code></pre> <p><code>createStateManager</code> creates a <code>StreamingAggregationStateManager</code> based on the given <code>stateFormatVersion</code> (based on spark.sql.streaming.aggregation.stateFormatVersion).</p>    stateFormatVersion StreamingAggregationStateManager     1 <code>StreamingAggregationStateManagerImplV1</code>   2 StreamingAggregationStateManagerImplV2     <p><code>createStateManager</code> is used when:</p> <ul> <li><code>StateStoreRestoreExec</code> physical operator is created</li> <li><code>StateStoreSaveExec</code> physical operator is created</li> </ul>","text":"","location":"streaming-aggregation/StreamingAggregationStateManager/#creating-streamingaggregationstatemanager"},{"title":"StreamingAggregationStateManagerBaseImpl","text":"<p><code>StreamingAggregationStateManagerBaseImpl</code> is a base implementation of the StreamingAggregationStateManager abstraction for state managers for streaming aggregations.</p>","location":"streaming-aggregation/StreamingAggregationStateManagerBaseImpl/"},{"title":"Implementations","text":"<ul> <li><code>StreamingAggregationStateManagerImplV1</code> (legacy)</li> <li>StreamingAggregationStateManagerImplV2</li> </ul>","location":"streaming-aggregation/StreamingAggregationStateManagerBaseImpl/#implementations"},{"title":"Creating Instance","text":"<p><code>StreamingAggregationStateManagerBaseImpl</code> takes the following to be created:</p> <ul> <li> Key <code>Attribute</code>s (Spark SQL) <li> Input Row <code>Attribute</code>s (Spark SQL)   <p>Abstract Class</p> <p><code>StreamingAggregationStateManagerBaseImpl</code> is an abstract class and cannot be created directly. It is created indirectly for the concrete StreamingAggregationStateManagerBaseImpls.</p>","location":"streaming-aggregation/StreamingAggregationStateManagerBaseImpl/#creating-instance"},{"title":"Committing State Changes <pre><code>commit(\n  store: StateStore): Long\n</code></pre> <p><code>commit</code> is part of the StreamingAggregationStateManager abstraction.</p>  <p><code>commit</code> requests the given StateStore to commit state changes.</p>","text":"","location":"streaming-aggregation/StreamingAggregationStateManagerBaseImpl/#committing-state-changes"},{"title":"Extracting Key <pre><code>getKey(\n  row: UnsafeRow): UnsafeRow\n</code></pre> <p><code>getKey</code> is part of the StreamingAggregationStateManager abstraction.</p>  <p><code>getKey</code> uses the keyProjector to extract a key from the given <code>row</code>.</p>","text":"","location":"streaming-aggregation/StreamingAggregationStateManagerBaseImpl/#extracting-key"},{"title":"Removing Key <pre><code>remove(\n  store: StateStore,\n  key: UnsafeRow): Unit\n</code></pre> <p><code>remove</code> is part of the StreamingAggregationStateManager abstraction.</p>  <p><code>remove</code> requests the given StateStore to remove the given <code>key</code>.</p>","text":"","location":"streaming-aggregation/StreamingAggregationStateManagerBaseImpl/#removing-key"},{"title":"StreamingAggregationStateManagerImplV2","text":"<p><code>StreamingAggregationStateManagerImplV2</code> is the default state manager for streaming aggregations (used when spark.sql.streaming.aggregation.stateFormatVersion configuration property is <code>2</code>).</p>","location":"streaming-aggregation/StreamingAggregationStateManagerImplV2/"},{"title":"Creating Instance","text":"<p><code>StreamingAggregationStateManagerImplV2</code> (like the parent StreamingAggregationStateManagerBaseImpl) takes the following to be created:</p> <ul> <li>[[keyExpressions]] Catalyst expressions for the keys (<code>Seq[Attribute]</code>)</li> <li>[[inputRowAttributes]] Catalyst expressions for the input rows (<code>Seq[Attribute]</code>)</li> </ul> <p><code>StreamingAggregationStateManagerImplV2</code> is created when <code>StreamingAggregationStateManager</code> is requested for a new StreamingAggregationStateManager.</p>","location":"streaming-aggregation/StreamingAggregationStateManagerImplV2/#creating-instance"},{"title":"Storing Row in State Store <pre><code>put(\n  store: StateStore,\n  row: UnsafeRow): Unit\n</code></pre> <p><code>put</code> is part of the StreamingAggregationStateManager abstraction.</p> <p><code>put</code>...FIXME</p>","text":"","location":"streaming-aggregation/StreamingAggregationStateManagerImplV2/#storing-row-in-state-store"},{"title":"Getting Saved State for Non-Null Key from State Store <pre><code>get(\n  store: StateStore,\n  key: UnsafeRow): UnsafeRow\n</code></pre> <p><code>get</code> is part of the StreamingAggregationStateManager abstraction.</p> <p><code>get</code> requests the given StateStore for the current state value for the given key.</p> <p><code>get</code> returns <code>null</code> if the key could not be found in the state store. Otherwise, <code>get</code> restoreOriginalRow (for the key and the saved state).</p> <p>=== [[restoreOriginalRow]] <code>restoreOriginalRow</code> Internal Method</p>","text":"","location":"streaming-aggregation/StreamingAggregationStateManagerImplV2/#getting-saved-state-for-non-null-key-from-state-store"},{"title":"[source, scala] <p>restoreOriginalRow(key: UnsafeRow, value: UnsafeRow): UnsafeRow restoreOriginalRow(rowPair: UnsafeRowPair): UnsafeRow</p>  <p><code>restoreOriginalRow</code>...FIXME</p> <p>NOTE: <code>restoreOriginalRow</code> is used when <code>StreamingAggregationStateManagerImplV2</code> is requested to &lt;&gt;, &lt;&gt; and &lt;&gt;.","text":"","location":"streaming-aggregation/StreamingAggregationStateManagerImplV2/#source-scala"},{"title":"getStateValueSchema <pre><code>getStateValueSchema: StructType\n</code></pre> <p><code>getStateValueSchema</code> is part of the StreamingAggregationStateManager abstraction.</p> <p><code>getStateValueSchema</code> simply requests the valueExpressions for the schema.</p>","text":"","location":"streaming-aggregation/StreamingAggregationStateManagerImplV2/#getstatevalueschema"},{"title":"iterator <pre><code>iterator(\n  store: StateStore): Iterator[UnsafeRowPair]\n</code></pre> <p><code>iterator</code> is part of the StreamingAggregationStateManager abstraction.</p> <p><code>iterator</code> simply requests the input state store for the iterator that is mapped to an iterator of <code>UnsafeRowPairs</code> with the key (of the input <code>UnsafeRowPair</code>) and the value as a restored original row.</p>  <p>Note</p> <p>scala.collection.Iterator is a data structure that allows to iterate over a sequence of elements that are usually fetched lazily (i.e. no elements are fetched from the underlying store until processed).</p>","text":"","location":"streaming-aggregation/StreamingAggregationStateManagerImplV2/#iterator"},{"title":"values <pre><code>values(\n  store: StateStore): Iterator[UnsafeRow]\n</code></pre> <p><code>values</code> is part of the StreamingAggregationStateManager abstraction.</p> <p><code>values</code>...FIXME</p>","text":"","location":"streaming-aggregation/StreamingAggregationStateManagerImplV2/#values"},{"title":"Internal Properties <p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| joiner | [[joiner]]</p> <p>| keyValueJoinedExpressions a| [[keyValueJoinedExpressions]]</p> <p>| needToProjectToRestoreValue a| [[needToProjectToRestoreValue]]</p> <p>| restoreValueProjector a| [[restoreValueProjector]]</p> <p>| valueExpressions a| [[valueExpressions]]</p> <p>| valueProjector a| [[valueProjector]] |===</p>","text":"","location":"streaming-aggregation/StreamingAggregationStateManagerImplV2/#internal-properties"},{"title":"Index","text":"<p>== Streaming Deduplication</p> <p>Streaming Deduplication is...FIXME</p>","location":"streaming-deduplication/"},{"title":"Streaming Join","text":"<p>In Spark Structured Streaming, a streaming join is an operator in a streaming query that is described (built) using the high-level streaming operators:</p> <ul> <li> <p>Dataset.crossJoin</p> </li> <li> <p>Dataset.join</p> </li> <li> <p>Dataset.joinWith</p> </li> <li> <p>SQL's <code>JOIN</code> clause</p> </li> </ul> <p>Streaming joins can be stateless or stateful:</p> <ul> <li> <p>Joins of a streaming query and a batch query (stream-static joins) are stateless and no state management is required</p> </li> <li> <p>Joins of two streaming queries (stream-stream joins) are stateful and require streaming state (with an optional join state watermark for state removal).</p> </li> </ul>","location":"streaming-join/"},{"title":"Stream-Stream Joins","text":"<p>Spark Structured Streaming supports stream-stream joins with the following:</p> <ul> <li> <p>Equality predicate (equi-joins that use only equality comparisons in the join predicate)</p> </li> <li> <p><code>Inner</code>, <code>LeftOuter</code>, and <code>RightOuter</code> &lt;&gt;   <p>Stream-stream equi-joins are planned as &lt;&gt; physical operators of two <code>ShuffleExchangeExec</code> physical operators (per &lt;&gt;).","location":"streaming-join/#stream-stream-joins"},{"title":"Join State Watermark","text":"<p>Stream-stream joins may optionally define Join State Watermark for state removal (cf. &lt;&gt;). <p>A join state watermark can be specified on the following:</p> <p>. &lt;&gt; (key state) <p>. &lt;&gt; (value state) <p>A join state watermark can be specified on key state, value state or both.</p>","location":"streaming-join/#join-state-watermark"},{"title":"IncrementalExecution","text":"<p>Under the covers, the &lt;&gt; create a logical query plan with one or more <code>Join</code> logical operators. <p>Spark Structured Streaming uses IncrementalExecution for planning streaming queries for execution.</p> <p>At query planning, <code>IncrementalExecution</code> uses the StreamingJoinStrategy execution planning strategy for planning stream-stream joins as StreamingSymmetricHashJoinExec physical operators.</p>","location":"streaming-join/#incrementalexecution"},{"title":"Demos","text":"<ul> <li>StreamStreamJoinApp</li> </ul>","location":"streaming-join/#demos"},{"title":"Further Reading Or Watching","text":"<ul> <li> <p>Stream-stream Joins in the official documentation of Apache Spark for Structured Streaming</p> </li> <li> <p>Introducing Stream-Stream Joins in Apache Spark 2.3 by Databricks</p> </li> <li> <p>(video) Deep Dive into Stateful Stream Processing in Structured Streaming by Tathagata Das</p> </li> </ul>","location":"streaming-join/#further-reading-or-watching"},{"title":"JoinStateWatermarkPredicate","text":"<p><code>JoinStateWatermarkPredicate</code> is the &lt;&gt; of &lt;&gt; that are described by a &lt;&gt; and &lt;&gt;. <p><code>JoinStateWatermarkPredicate</code> is created using StreamingSymmetricHashJoinHelper utility (for planning a StreamingSymmetricHashJoinExec physical operator for execution with execution-specific configuration)</p> <p><code>JoinStateWatermarkPredicate</code> is used to create a OneSideHashJoiner (and JoinStateWatermarkPredicates).</p> <p>[[contract]] .JoinStateWatermarkPredicate Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| desc a| [[desc]]</p>","location":"streaming-join/JoinStateWatermarkPredicate/"},{"title":"[source, scala]","text":"","location":"streaming-join/JoinStateWatermarkPredicate/#source-scala"},{"title":"desc: String","text":"<p>Used exclusively for the &lt;&gt; <p>| expr a| [[expr]]</p>","location":"streaming-join/JoinStateWatermarkPredicate/#desc-string"},{"title":"[source, scala]","text":"","location":"streaming-join/JoinStateWatermarkPredicate/#source-scala_1"},{"title":"expr: Expression","text":"<p>A Catalyst <code>Expression</code></p> <p>Used for the &lt;&gt; and a JoinStateWatermarkPredicates (for StreamingSymmetricHashJoinExec physical operator) <p>|===</p> <p>[[implementations]] .JoinStateWatermarkPredicates [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | JoinStateWatermarkPredicate | Description</p> <p>| JoinStateKeyWatermarkPredicate a| [[JoinStateKeyWatermarkPredicate]] Watermark predicate on state keys (i.e. when the streaming watermark is defined either on the left or right join keys)</p> <p>Created when <code>StreamingSymmetricHashJoinHelper</code> utility is requested for a JoinStateWatermarkPredicates for the left and right side of a stream-stream join (when <code>IncrementalExecution</code> is requested to optimize a query plan with a StreamingSymmetricHashJoinExec physical operator)</p> <p>Used when <code>OneSideHashJoiner</code> is requested for the stateKeyWatermarkPredicateFunc and then to remove an old state</p> <p>| JoinStateValueWatermarkPredicate | [[JoinStateValueWatermarkPredicate]] Watermark predicate on state values</p> <p>|===</p> <p>NOTE: <code>JoinStateWatermarkPredicate</code> is a Scala sealed trait which means that all the &lt;&gt; are in the same compilation unit (a single file). <p>=== [[toString]] Textual Representation -- <code>toString</code> Method</p>","location":"streaming-join/JoinStateWatermarkPredicate/#expr-expression"},{"title":"[source, scala]","text":"","location":"streaming-join/JoinStateWatermarkPredicate/#source-scala_2"},{"title":"toString: String","text":"<p>NOTE: <code>toString</code> is part of the ++https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object] contract for the string representation of the object.</p> <p><code>toString</code> uses the &lt;&gt; and &lt;&gt; for the string representation: <pre><code>[desc]: [expr]\n</code></pre>","location":"streaming-join/JoinStateWatermarkPredicate/#tostring-string"},{"title":"JoinStateWatermarkPredicates","text":"<p>[[creating-instance]] <code>JoinStateWatermarkPredicates</code> contains watermark predicates for state removal of the children of a StreamingSymmetricHashJoinExec physical operator:</p> <ul> <li> <p>[[left]] &lt;&gt; for the left-hand side of a join (default: <code>None</code>)  <li> <p>[[right]] &lt;&gt; for the right-hand side of a join (default: <code>None</code>)   <p><code>JoinStateWatermarkPredicates</code> is &lt;&gt; for the following: <ul> <li> <p>&lt;&gt; physical operator is created (with the optional properties undefined, including &lt;&gt;)  <li> <p><code>StreamingSymmetricHashJoinHelper</code> utility is requested for one (for <code>IncrementalExecution</code> for the state preparation rule to optimize and specify the execution-specific configuration for a query plan with StreamingSymmetricHashJoinExec physical operators)</p> </li>  <p>=== [[toString]] Textual Representation -- <code>toString</code> Method</p>","location":"streaming-join/JoinStateWatermarkPredicates/"},{"title":"[source, scala]","text":"","location":"streaming-join/JoinStateWatermarkPredicates/#source-scala"},{"title":"toString: String","text":"<p>NOTE: <code>toString</code> is part of the ++https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object] contract for the string representation of the object.</p> <p><code>toString</code> uses the &lt;&gt; and &lt;&gt; predicates for the string representation: <pre><code>state cleanup [ left [left], right [right] ]\n</code></pre>","location":"streaming-join/JoinStateWatermarkPredicates/#tostring-string"},{"title":"KeyToNumValuesStore","text":"<p><code>KeyToNumValuesStore</code> is a StateStoreHandler (of KeyToNumValuesType) for SymmetricHashJoinStateManager to manage a &lt;&gt;. <p></p> <p>[[stateStore]] As a StateStoreHandler, <code>KeyToNumValuesStore</code> manages a state store (that is loaded) with the join keys (per key schema) and their count (per &lt;&gt;). <p>[[longValueSchema]] <code>KeyToNumValuesStore</code> uses the schema for values in the &lt;&gt; with one field <code>value</code> (of type <code>long</code>) that is the number of value rows (count). <p>[[logging]] [TIP] ==== Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyToNumValuesStore</code> to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyToNumValuesStore=ALL\n</code></pre>","location":"streaming-join/KeyToNumValuesStore/"},{"title":"Refer to &lt;&gt;. <p>=== [[get]] Looking Up Number Of Value Rows For Given Key (Value Count) -- <code>get</code> Method</p>","text":"","location":"streaming-join/KeyToNumValuesStore/#refer-to"},{"title":"[source, scala]","text":"","location":"streaming-join/KeyToNumValuesStore/#source-scala"},{"title":"get(key: UnsafeRow): Long","text":"<p><code>get</code> requests the &lt;&gt; for the value for the given key and returns the long value at <code>0</code>th position (of the row found) or <code>0</code>. <p><code>get</code> is used when <code>SymmetricHashJoinStateManager</code> is requested for the values for a given key and append a new value to a given key.</p> <p>=== [[put]] Storing Key Count For Given Key -- <code>put</code> Method</p>","location":"streaming-join/KeyToNumValuesStore/#getkey-unsaferow-long"},{"title":"[source, scala]","text":"<p>put(   key: UnsafeRow,   numValues: Long): Unit</p>  <p><code>put</code> stores the <code>numValues</code> at the <code>0</code>th position (of the internal unsafe row) and requests the &lt;&gt; to store it with the given key. <p><code>put</code> requires that the <code>numValues</code> count is greater than <code>0</code> (or throws an <code>IllegalArgumentException</code>).</p> <p><code>put</code> is used when <code>SymmetricHashJoinStateManager</code> is requested for the append a new value to a given key and updateNumValueForCurrentKey.</p> <p>=== [[iterator]] All State Keys and Values -- <code>iterator</code> Method</p>","location":"streaming-join/KeyToNumValuesStore/#source-scala_1"},{"title":"[source, scala]","text":"","location":"streaming-join/KeyToNumValuesStore/#source-scala_2"},{"title":"iterator: Iterator[KeyAndNumValues]","text":"<p><code>iterator</code> simply requests the &lt;&gt; for all state keys and values. <p><code>iterator</code> is used when <code>SymmetricHashJoinStateManager</code> is requested to removeByKeyCondition and removeByValueCondition.</p> <p>=== [[remove]] Removing State Key -- <code>remove</code> Method</p>","location":"streaming-join/KeyToNumValuesStore/#iterator-iteratorkeyandnumvalues"},{"title":"[source, scala]","text":"","location":"streaming-join/KeyToNumValuesStore/#source-scala_3"},{"title":"remove(key: UnsafeRow): Unit","text":"<p><code>remove</code> simply requests the &lt;&gt; to remove the given key. <p><code>remove</code> is used when...FIXME</p>","location":"streaming-join/KeyToNumValuesStore/#removekey-unsaferow-unit"},{"title":"KeyWithIndexToValueStore","text":"<p><code>KeyWithIndexToValueStore</code> is a StateStoreHandler (of KeyWithIndexToValueType) for SymmetricHashJoinStateManager to manage a &lt;&gt;. <p>.KeyToNumValuesStore, KeyWithIndexToValueStore and Stream-Stream Join image::images/KeyToNumValuesStore-KeyWithIndexToValueStore.png[align=\"center\"]</p> <p>[[stateStore]] As a StateStoreHandler, <code>KeyWithIndexToValueStore</code> manages a state store (that is loaded) for keys and values per the &lt;&gt; and input values schemas, respectively. <p>[[keyWithIndexSchema]] <code>KeyWithIndexToValueStore</code> uses a schema (for the &lt;&gt;) that is the key schema (of the parent <code>SymmetricHashJoinStateManager</code>) with an extra field <code>index</code> of type <code>long</code>. <p>[[logging]] [TIP] ==== Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyWithIndexToValueStore</code> to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyWithIndexToValueStore=ALL\n</code></pre>","location":"streaming-join/KeyWithIndexToValueStore/"},{"title":"Refer to &lt;&gt;. <p>=== [[get]] Looking Up State Row For Given Key and Index -- <code>get</code> Method</p>","text":"","location":"streaming-join/KeyWithIndexToValueStore/#refer-to"},{"title":"[source, scala]","text":"<p>get(   key: UnsafeRow,   valueIndex: Long): UnsafeRow</p>  <p><code>get</code> simply requests the internal state store to look up the value for the given &lt;&gt;. <p><code>get</code> is used when <code>SymmetricHashJoinStateManager</code> is requested to removeByValueCondition.</p> <p>=== [[getAll]] Retrieving (Given Number of) Values for Key -- <code>getAll</code> Method</p>","location":"streaming-join/KeyWithIndexToValueStore/#source-scala"},{"title":"[source, scala]","text":"<p>getAll(   key: UnsafeRow,   numValues: Long): Iterator[KeyWithIndexAndValue]</p>  <p><code>getAll</code>...FIXME</p> <p><code>getAll</code> is used when <code>SymmetricHashJoinStateManager</code> is requested to get values for a given key and removeByKeyCondition.</p> <p>=== [[put]] Storing State Row For Given Key and Index -- <code>put</code> Method</p>","location":"streaming-join/KeyWithIndexToValueStore/#source-scala_1"},{"title":"[source, scala]","text":"<p>put(   key: UnsafeRow,   valueIndex: Long,   value: UnsafeRow): Unit</p>  <p><code>put</code>...FIXME</p> <p><code>put</code> is used when <code>SymmetricHashJoinStateManager</code> is requested to append a new value to a given key and removeByKeyCondition.</p> <p>=== [[remove]] <code>remove</code> Method</p>","location":"streaming-join/KeyWithIndexToValueStore/#source-scala_2"},{"title":"[source, scala]","text":"<p>remove(   key: UnsafeRow,   valueIndex: Long): Unit</p>  <p><code>remove</code>...FIXME</p> <p><code>remove</code> is used when <code>SymmetricHashJoinStateManager</code> is requested to removeByKeyCondition and removeByValueCondition.</p> <p>=== [[keyWithIndexRow]] <code>keyWithIndexRow</code> Internal Method</p>","location":"streaming-join/KeyWithIndexToValueStore/#source-scala_3"},{"title":"[source, scala]","text":"<p>keyWithIndexRow(   key: UnsafeRow,   valueIndex: Long): UnsafeRow</p>  <p><code>keyWithIndexRow</code> uses the &lt;&gt; to generate an <code>UnsafeRow</code> for the <code>key</code> and sets the <code>valueIndex</code> at the &lt;&gt; position. <p>NOTE: <code>keyWithIndexRow</code> is used when <code>KeyWithIndexToValueStore</code> is requested to &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt; and &lt;&gt;. <p>=== [[removeAllValues]] <code>removeAllValues</code> Method</p>","location":"streaming-join/KeyWithIndexToValueStore/#source-scala_4"},{"title":"[source, scala]","text":"<p>removeAllValues(   key: UnsafeRow,   numValues: Long): Unit</p>  <p><code>removeAllValues</code>...FIXME</p> <p>NOTE: <code>removeAllValues</code> does not seem to be used at all.</p> <p>=== [[iterator]] <code>iterator</code> Method</p>","location":"streaming-join/KeyWithIndexToValueStore/#source-scala_5"},{"title":"[source, scala]","text":"","location":"streaming-join/KeyWithIndexToValueStore/#source-scala_6"},{"title":"iterator: Iterator[KeyWithIndexAndValue]","text":"<p><code>iterator</code>...FIXME</p> <p>NOTE: <code>iterator</code> does not seem to be used at all.</p>","location":"streaming-join/KeyWithIndexToValueStore/#iterator-iteratorkeywithindexandvalue"},{"title":"Internal Properties","text":"<p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| indexOrdinalInKeyWithIndexRow a| [[indexOrdinalInKeyWithIndexRow]] Position of the index in the key row (which corresponds to the number of the key attributes)</p> <p>Used exclusively in the &lt;&gt; <p>| keyWithIndexExprs a| [[keyWithIndexExprs]] keyAttributes with <code>Literal(1L)</code> expression appended</p> <p>Used exclusively for the &lt;&gt; projection <p>| keyWithIndexRowGenerator a| [[keyWithIndexRowGenerator]] <code>UnsafeProjection</code> for the &lt;&gt; bound to the keyAttributes <p>Used exclusively in &lt;&gt; <p>|===</p>","location":"streaming-join/KeyWithIndexToValueStore/#internal-properties"},{"title":"OneSideHashJoiner","text":"<p><code>OneSideHashJoiner</code> manages join state of one side of a &lt;&gt; (using &lt;&gt;). <p><code>OneSideHashJoiner</code> is &lt;&gt; exclusively for &lt;&gt; physical operator (when requested to &lt;&gt;). <p>.OneSideHashJoiner and StreamingSymmetricHashJoinExec image::images/OneSideHashJoiner.png[align=\"center\"]</p> <p><code>StreamingSymmetricHashJoinExec</code> physical operator uses two <code>OneSideHashJoiners</code> per side of the stream-stream join (&lt;&gt; and &lt;&gt; sides). <p><code>OneSideHashJoiner</code> uses an &lt;&gt; to &lt;&gt;. <p>NOTE: <code>OneSideHashJoiner</code> is a Scala private internal class of &lt;&gt; and so has full access to <code>StreamingSymmetricHashJoinExec</code> properties.","location":"streaming-join/OneSideHashJoiner/"},{"title":"Creating OneSideHashJoiner Instance","text":"<p><code>OneSideHashJoiner</code> takes the following to be created:</p> <ul> <li>[[joinSide]] JoinSide</li> <li>[[inputAttributes]] Input attributes (<code>Seq[Attribute]</code>)</li> <li>[[joinKeys]] Join keys (<code>Seq[Expression]</code>)</li> <li>[[inputIter]] Input rows (<code>Iterator[InternalRow]</code>)</li> <li>[[preJoinFilterExpr]] Optional pre-join filter Catalyst expression</li> <li>[[postJoinFilter]] Post-join filter (<code>(InternalRow) =&gt; Boolean</code>)</li> <li>&lt;&gt;  <p><code>OneSideHashJoiner</code> initializes the &lt;&gt;. <p>=== [[joinStateManager]] SymmetricHashJoinStateManager -- <code>joinStateManager</code> Internal Property</p>","location":"streaming-join/OneSideHashJoiner/#creating-onesidehashjoiner-instance"},{"title":"[source, scala]","text":"","location":"streaming-join/OneSideHashJoiner/#source-scala"},{"title":"joinStateManager: SymmetricHashJoinStateManager","text":"<p><code>joinStateManager</code> is a SymmetricHashJoinStateManager that is created for a <code>OneSideHashJoiner</code> (with the &lt;&gt;, the &lt;&gt;, the &lt;&gt;, and the &lt;&gt; of the owning &lt;&gt;). <p><code>joinStateManager</code> is used when <code>OneSideHashJoiner</code> is requested for the following:</p> <ul> <li> <p>&lt;&gt;  <li> <p>&lt;&gt;  <li> <p>&lt;&gt;  <li> <p>&lt;&gt;   <p>=== [[updatedStateRowsCount]] Number of Updated State Rows -- <code>updatedStateRowsCount</code> Internal Counter</p> <p><code>updatedStateRowsCount</code> is the number the join keys and associated rows that were persisted as a join state, i.e. how many times &lt;&gt; requested the &lt;&gt; to append the join key and the input row (to a join state). <p><code>updatedStateRowsCount</code> is then used (via &lt;&gt; method) for the &lt;&gt; performance metric. <p><code>updatedStateRowsCount</code> is available via <code>numUpdatedStateRows</code> method.</p> <p>[[numUpdatedStateRows]] [source, scala]</p>","location":"streaming-join/OneSideHashJoiner/#joinstatemanager-symmetrichashjoinstatemanager"},{"title":"numUpdatedStateRows: Long","text":"<p>NOTE: <code>numUpdatedStateRows</code> is used exclusively when <code>StreamingSymmetricHashJoinExec</code> physical operator is requested to &lt;&gt; (and &lt;&gt;). <p>=== [[stateWatermarkPredicate]] Optional Join State Watermark Predicate -- <code>stateWatermarkPredicate</code> Internal Property</p>","location":"streaming-join/OneSideHashJoiner/#numupdatedstaterows-long"},{"title":"[source, scala]","text":"","location":"streaming-join/OneSideHashJoiner/#source-scala_1"},{"title":"stateWatermarkPredicate: Option[JoinStateWatermarkPredicate]","text":"<p>When &lt;&gt;, <code>OneSideHashJoiner</code> is given a &lt;&gt;. <p><code>stateWatermarkPredicate</code> is used for the &lt;&gt; (when a &lt;&gt;) and the &lt;&gt; (when a &lt;&gt;) that are both used when <code>OneSideHashJoiner</code> is requested to &lt;&gt;. <p>=== [[storeAndJoinWithOtherSide]] <code>storeAndJoinWithOtherSide</code> Method</p>","location":"streaming-join/OneSideHashJoiner/#statewatermarkpredicate-optionjoinstatewatermarkpredicate"},{"title":"[source, scala]","text":"<p>storeAndJoinWithOtherSide(   otherSideJoiner: OneSideHashJoiner)(   generateJoinedRow: (InternalRow, InternalRow) =&gt; JoinedRow): Iterator[InternalRow]</p>  <p><code>storeAndJoinWithOtherSide</code> tries to find the watermark attribute among the input attributes.</p> <p><code>storeAndJoinWithOtherSide</code> creates a watermark expression (for the watermark attribute and the current event-time watermark).</p> <p>[[storeAndJoinWithOtherSide-nonLateRows]] With the watermark attribute found, <code>storeAndJoinWithOtherSide</code> generates a new predicate for the watermark expression and the &lt;&gt; that is then used to filter out (exclude) late rows from the &lt;&gt;. Otherwise, the input rows are left unchanged (i.e. no rows are considered late and excluded). <p>[[storeAndJoinWithOtherSide-nonLateRows-flatMap]] For every &lt;&gt; (possibly &lt;&gt;), <code>storeAndJoinWithOtherSide</code> applies the &lt;&gt; predicate and branches off per result (&lt;&gt; or &lt;&gt;). <p>NOTE: <code>storeAndJoinWithOtherSide</code> is used when <code>StreamingSymmetricHashJoinExec</code> physical operator is requested to &lt;&gt;. <p>==== [[preJoinFilter-true]] <code>preJoinFilter</code> Predicate Positive (<code>true</code>)</p> <p>When the &lt;&gt; predicate succeeds on an input row, <code>storeAndJoinWithOtherSide</code> extracts the join key (using the &lt;&gt;) and requests the given <code>OneSideHashJoiner</code> (<code>otherSideJoiner</code>) for the &lt;&gt; that is in turn requested for the state values for the extracted join key. The values are then processed (mapped over) using the given <code>generateJoinedRow</code> function and then filtered by the &lt;&gt;. <p><code>storeAndJoinWithOtherSide</code> uses the &lt;&gt; (on the extracted join key) and the &lt;&gt; (on the current input row) to determine whether to request the &lt;&gt; to append the key and the input row (to a join state). If so, <code>storeAndJoinWithOtherSide</code> increments the &lt;&gt; counter. <p>==== [[preJoinFilter-false]] <code>preJoinFilter</code> Predicate Negative (<code>false</code>)</p> <p>When the &lt;&gt; predicate fails on an input row, <code>storeAndJoinWithOtherSide</code> creates a new <code>Iterator[InternalRow]</code> of joined rows per &lt;&gt; and &lt;&gt;: <ul> <li> <p>For LeftSide and <code>LeftOuter</code>, the join row is the current row with the values of the right side all <code>null</code> (<code>nullRight</code>)</p> </li> <li> <p>For RightSide and <code>RightOuter</code>, the join row is the current row with the values of the left side all <code>null</code> (<code>nullLeft</code>)</p> </li> <li> <p>For all other combinations, the iterator is simply empty (that will be removed from the output by the outer &lt;&gt;).   <p>=== [[removeOldState]] Removing Old State -- <code>removeOldState</code> Method</p>","location":"streaming-join/OneSideHashJoiner/#source-scala_2"},{"title":"[source, scala]","text":"","location":"streaming-join/OneSideHashJoiner/#source-scala_3"},{"title":"removeOldState(): Iterator[UnsafeRowPair]","text":"<p><code>removeOldState</code> branches off per the &lt;&gt;: <ul> <li> <p>For &lt;&gt;, <code>removeOldState</code> requests the &lt;&gt; to removeByKeyCondition (with the &lt;&gt;)  <li> <p>For &lt;&gt;, <code>removeOldState</code> requests the &lt;&gt; to removeByValueCondition (with the &lt;&gt;)  <li> <p>For any other predicates, <code>removeOldState</code> returns an empty iterator (no rows to process)</p> </li>  <p>NOTE: <code>removeOldState</code> is used exclusively when <code>StreamingSymmetricHashJoinExec</code> physical operator is requested to &lt;&gt;. <p>=== [[get]] Retrieving Value Rows For Key -- <code>get</code> Method</p>","location":"streaming-join/OneSideHashJoiner/#removeoldstate-iteratorunsaferowpair"},{"title":"[source, scala]","text":"","location":"streaming-join/OneSideHashJoiner/#source-scala_4"},{"title":"get(key: UnsafeRow): Iterator[UnsafeRow]","text":"<p><code>get</code> simply requests the &lt;&gt; to retrieve value rows for the key. <p>NOTE: <code>get</code> is used exclusively when <code>StreamingSymmetricHashJoinExec</code> physical operator is requested to &lt;&gt;. <p>=== [[commitStateAndGetMetrics]] Committing State (Changes) and Requesting Performance Metrics -- <code>commitStateAndGetMetrics</code> Method</p>","location":"streaming-join/OneSideHashJoiner/#getkey-unsaferow-iteratorunsaferow"},{"title":"[source, scala]","text":"","location":"streaming-join/OneSideHashJoiner/#source-scala_5"},{"title":"commitStateAndGetMetrics(): StateStoreMetrics","text":"<p><code>commitStateAndGetMetrics</code> simply requests the &lt;&gt; to commit followed by requesting for the performance metrics. <p><code>commitStateAndGetMetrics</code> is used when <code>StreamingSymmetricHashJoinExec</code> physical operator is requested to &lt;&gt;.","location":"streaming-join/OneSideHashJoiner/#commitstateandgetmetrics-statestoremetrics"},{"title":"Internal Properties","text":"<p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| keyGenerator a| [[keyGenerator]]</p>","location":"streaming-join/OneSideHashJoiner/#internal-properties"},{"title":"[source, scala]","text":"","location":"streaming-join/OneSideHashJoiner/#source-scala_6"},{"title":"keyGenerator: UnsafeProjection","text":"<p>Function to project (extract) join keys from an input row</p> <p>Used when...FIXME</p> <p>| preJoinFilter a| [[preJoinFilter]]</p>","location":"streaming-join/OneSideHashJoiner/#keygenerator-unsafeprojection"},{"title":"[source, scala]","text":"","location":"streaming-join/OneSideHashJoiner/#source-scala_7"},{"title":"preJoinFilter: InternalRow =&gt; Boolean","text":"<p>Used when...FIXME</p> <p>| stateKeyWatermarkPredicateFunc a| [[stateKeyWatermarkPredicateFunc]]</p>","location":"streaming-join/OneSideHashJoiner/#prejoinfilter-internalrow-boolean"},{"title":"[source, scala]","text":"","location":"streaming-join/OneSideHashJoiner/#source-scala_8"},{"title":"stateKeyWatermarkPredicateFunc: InternalRow =&gt; Boolean","text":"<p>Predicate for late rows based on the &lt;&gt; <p>Used for the following:</p> <ul> <li> <p>&lt;&gt; (and check out whether to append a row to the SymmetricHashJoinStateManager)  <li> <p>&lt;&gt;   <p>| stateValueWatermarkPredicateFunc a| [[stateValueWatermarkPredicateFunc]]</p>","location":"streaming-join/OneSideHashJoiner/#statekeywatermarkpredicatefunc-internalrow-boolean"},{"title":"[source, scala]","text":"","location":"streaming-join/OneSideHashJoiner/#source-scala_9"},{"title":"stateValueWatermarkPredicateFunc: InternalRow =&gt; Boolean","text":"<p>Predicate for late rows based on the &lt;&gt; <p>Used for the following:</p> <ul> <li> <p>&lt;&gt; (and check out whether to append a row to the &lt;&gt;)  <li> <p>&lt;&gt;   <p>|===</p>","location":"streaming-join/OneSideHashJoiner/#statevaluewatermarkpredicatefunc-internalrow-boolean"},{"title":"StateStoreAwareZipPartitionsHelper","text":"<p> <code>StateStoreAwareZipPartitionsHelper</code> is a Scala implicit class of a data RDD (of type <code>RDD[T]</code>) to create a StateStoreAwareZipPartitionsRDD for StreamingSymmetricHashJoinExec physical operator.  <p>Note</p> <p>Implicit Classes are a language feature in Scala for implicit conversions with extension methods for existing types.</p>","location":"streaming-join/StateStoreAwareZipPartitionsHelper/"},{"title":"Creating StateStoreAwareZipPartitionsRDD <pre><code>stateStoreAwareZipPartitions[U: ClassTag, V: ClassTag](\n  dataRDD2: RDD[U],\n  stateInfo: StatefulOperatorStateInfo,\n  storeNames: Seq[String],\n  storeCoordinator: StateStoreCoordinatorRef)(\n  f: (Iterator[T], Iterator[U]) =&gt; Iterator[V]): RDD[V]\n</code></pre> <p><code>stateStoreAwareZipPartitions</code> simply creates a new StateStoreAwareZipPartitionsRDD.</p> <p><code>stateStoreAwareZipPartitions</code> is used when StreamingSymmetricHashJoinExec physical operator is executed.</p>","text":"","location":"streaming-join/StateStoreAwareZipPartitionsHelper/#creating-statestoreawarezippartitionsrdd"},{"title":"StateStoreAwareZipPartitionsRDD","text":"<p><code>StateStoreAwareZipPartitionsRDD</code> is a <code>ZippedPartitionsRDD2</code> with the &lt;&gt; and &lt;&gt; parent RDDs. <p><code>StateStoreAwareZipPartitionsRDD</code> is &lt;&gt; when <code>StreamingSymmetricHashJoinExec</code> physical operator is requested to execute (and requests &lt;&gt; for one).","location":"streaming-join/StateStoreAwareZipPartitionsRDD/"},{"title":"Creating Instance","text":"<p><code>StateStoreAwareZipPartitionsRDD</code> takes the following to be created:</p> <ul> <li>[[sc]] <code>SparkContext</code></li> <li>[[f]] Function (<code>(Iterator[A], Iterator[B]) =&gt; Iterator[V]</code>, e.g. processPartitions)</li> <li>[[rdd1]] Left RDD - the RDD of the left side of a join (<code>RDD[A]</code>)</li> <li>[[rdd2]] Right RDD - the RDD of the right side of a join (<code>RDD[B]</code>)</li> <li>[[stateInfo]] StatefulOperatorStateInfo</li> <li>[[stateStoreNames]] Names of the state stores</li> <li>[[storeCoordinator]] StateStoreCoordinatorRef</li> </ul> <p>=== [[getPreferredLocations]] Placement Preferences of Partition (Preferred Locations) -- <code>getPreferredLocations</code> Method</p>","location":"streaming-join/StateStoreAwareZipPartitionsRDD/#creating-instance"},{"title":"[source, scala]","text":"","location":"streaming-join/StateStoreAwareZipPartitionsRDD/#source-scala"},{"title":"getPreferredLocations(partition: Partition): Seq[String]","text":"<p>NOTE: <code>getPreferredLocations</code> is a part of the RDD Contract to specify placement preferences (aka preferred task locations), i.e. where tasks should be executed to be as close to the data as possible.</p> <p><code>getPreferredLocations</code> simply requests the StateStoreCoordinatorRef for the location of every &lt;&gt; (with the &lt;&gt; and the partition ID) and returns unique executor IDs (so that processing a partition happens on the executor with the proper state store for the operator and the partition).","location":"streaming-join/StateStoreAwareZipPartitionsRDD/#getpreferredlocationspartition-partition-seqstring"},{"title":"StateStoreHandler","text":"<p><code>StateStoreHandler</code> is the internal &lt;&gt; of &lt;&gt; that manage a &lt;&gt; (i.e. &lt;&gt;, &lt;&gt; and &lt;&gt;). <p>[[stateStoreType]] <code>StateStoreHandler</code> takes a single <code>StateStoreType</code> to be created:</p> <ul> <li> <p>[[KeyToNumValuesType]] <code>KeyToNumValuesType</code> for &lt;&gt; (alias: <code>keyToNumValues</code>)  <li> <p>[[KeyWithIndexToValueType]] <code>KeyWithIndexToValueType</code> for &lt;&gt; (alias: <code>keyWithIndexToValue</code>)   <p>NOTE: <code>StateStoreHandler</code> is a Scala private abstract class and cannot be &lt;&gt; directly. It is created indirectly for the &lt;&gt;. <p>[[contract]] .StateStoreHandler Contract [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Method | Description</p> <p>| stateStore a| [[stateStore]]</p>","location":"streaming-join/StateStoreHandler/"},{"title":"[source, scala]","text":"","location":"streaming-join/StateStoreHandler/#source-scala"},{"title":"stateStore: StateStore","text":"<p>StateStore |===</p> <p>[[extensions]] .StateStoreHandlers [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | StateStoreHandler | Description</p> <p>| &lt;&gt; | [[KeyToNumValuesStore]] <code>StateStoreHandler</code> of &lt;&gt; <p>| &lt;&gt; | [[KeyWithIndexToValueStore]] <p>|===</p> <p>[[logging]] [TIP] ==== Enable <code>ALL</code> logging levels for <code>org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.StateStoreHandler</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.StateStoreHandler=ALL\n</code></pre>","location":"streaming-join/StateStoreHandler/#statestore-statestore"},{"title":"Refer to &lt;&gt;. <p>=== [[metrics]] Performance Metrics -- <code>metrics</code> Method</p>","text":"","location":"streaming-join/StateStoreHandler/#refer-to"},{"title":"[source, scala]","text":"","location":"streaming-join/StateStoreHandler/#source-scala_1"},{"title":"metrics: StateStoreMetrics","text":"<p><code>metrics</code> simply requests the StateStore for the StateStoreMetrics.</p> <p><code>metrics</code> is used when <code>SymmetricHashJoinStateManager</code> is requested for the metrics.</p> <p>=== [[commit]] Committing State (Changes to State Store) -- <code>commit</code> Method</p>","location":"streaming-join/StateStoreHandler/#metrics-statestoremetrics"},{"title":"[source, scala]","text":"","location":"streaming-join/StateStoreHandler/#source-scala_2"},{"title":"commit(): Unit","text":"<p><code>commit</code>...FIXME</p> <p>NOTE: <code>commit</code> is used when...FIXME</p> <p>=== [[abortIfNeeded]] <code>abortIfNeeded</code> Method</p>","location":"streaming-join/StateStoreHandler/#commit-unit"},{"title":"[source, scala]","text":"","location":"streaming-join/StateStoreHandler/#source-scala_3"},{"title":"abortIfNeeded(): Unit","text":"<p><code>abortIfNeeded</code>...FIXME</p> <p>NOTE: <code>abortIfNeeded</code> is used when...FIXME</p> <p>=== [[getStateStore]] Loading State Store (By Key and Value Schemas) -- <code>getStateStore</code> Method</p>","location":"streaming-join/StateStoreHandler/#abortifneeded-unit"},{"title":"[source, scala]","text":"<p>getStateStore(   keySchema: StructType,   valueSchema: StructType): StateStore</p>  <p><code>getStateStore</code> creates a new &lt;&gt; (for the StatefulOperatorStateInfo of the owning <code>SymmetricHashJoinStateManager</code>, the partition ID from the execution context, and the name of the state store for the JoinSide and &lt;&gt;). <p><code>getStateStore</code> uses the <code>StateStore</code> utility to look up a StateStore for the StateStoreProviderId.</p> <p>In the end, <code>getStateStore</code> prints out the following INFO message to the logs:</p> <pre><code>Loaded store [storeId]\n</code></pre> <p><code>getStateStore</code> is used when KeyToNumValuesStore and &lt;&gt; state store handlers are created (for SymmetricHashJoinStateManager). <p>=== [[StateStoreType]] <code>StateStoreType</code> Contract (Sealed Trait)</p> <p><code>StateStoreType</code> is required to create a &lt;&gt;. <p>[[StateStoreType-implementations]] .StateStoreTypes [cols=\"1m,1m,2\",options=\"header\",width=\"100%\"] |=== | StateStoreType | toString | Description</p> <p>| KeyToNumValuesType | keyToNumValues | [[KeyToNumValuesType]]</p> <p>| KeyWithIndexToValueType | keyWithIndexToValue | [[KeyWithIndexToValueType]] |===</p> <p>NOTE: <code>StateStoreType</code> is a Scala private sealed trait which means that all the &lt;&gt; are in the same compilation unit (a single file).","location":"streaming-join/StateStoreHandler/#source-scala_4"},{"title":"StreamingJoinHelper Utility","text":"<p><code>StreamingJoinHelper</code> is a Scala object with the following utility methods:</p> <ul> <li>&lt;&gt;  <p>[[logging]] [TIP] ==== Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.catalyst.analysis.StreamingJoinHelper</code> to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.catalyst.analysis.StreamingJoinHelper=ALL\n</code></pre>","location":"streaming-join/StreamingJoinHelper/"},{"title":"Refer to &lt;&gt;.","text":"","location":"streaming-join/StreamingJoinHelper/#refer-to"},{"title":"State Value Watermark <pre><code>getStateValueWatermark(\n  attributesToFindStateWatermarkFor: AttributeSet,\n  attributesWithEventWatermark: AttributeSet,\n  joinCondition: Option[Expression],\n  eventWatermark: Option[Long]): Option[Long]\n</code></pre> <p><code>getStateValueWatermark</code>...FIXME</p> <p><code>getStateValueWatermark</code> is used when:</p> <ul> <li> <p><code>UnsupportedOperationChecker</code> utility is used to checkForStreaming</p> </li> <li> <p><code>StreamingSymmetricHashJoinHelper</code> utility is used to create a JoinStateWatermarkPredicates</p> </li> </ul>","text":"","location":"streaming-join/StreamingJoinHelper/#state-value-watermark"},{"title":"StreamingSymmetricHashJoinHelper Utility","text":"<p><code>StreamingSymmetricHashJoinHelper</code> is a Scala object with the following utility methods:</p> <ul> <li>getStateWatermarkPredicates</li> </ul> <p>=== [[getStateWatermarkPredicates]] Creating JoinStateWatermarkPredicates -- <code>getStateWatermarkPredicates</code> Object Method</p>","location":"streaming-join/StreamingSymmetricHashJoinHelper/"},{"title":"[source, scala]","text":"<p>getStateWatermarkPredicates(   leftAttributes: Seq[Attribute],   rightAttributes: Seq[Attribute],   leftKeys: Seq[Expression],   rightKeys: Seq[Expression],   condition: Option[Expression],   eventTimeWatermark: Option[Long]): JoinStateWatermarkPredicates</p>  <p>[[getStateWatermarkPredicates-joinKeyOrdinalForWatermark]] <code>getStateWatermarkPredicates</code> tries to find the index of the watermark attribute among the left keys first, and if not found, the right keys.</p> <p><code>getStateWatermarkPredicates</code> &lt;&gt; for the left side of a join (for the given <code>leftAttributes</code>, the <code>leftKeys</code> and the <code>rightAttributes</code>). <p><code>getStateWatermarkPredicates</code> &lt;&gt; for the right side of a join (for the given <code>rightAttributes</code>, the <code>rightKeys</code> and the <code>leftAttributes</code>). <p>In the end, <code>getStateWatermarkPredicates</code> creates a JoinStateWatermarkPredicates with the left- and right-side state watermark predicates.</p> <p>NOTE: <code>getStateWatermarkPredicates</code> is used exclusively when <code>IncrementalExecution</code> is requested to apply the state preparation rule for batch-specific configuration (while optimizing query plans with StreamingSymmetricHashJoinExec physical operators).</p> <p>==== [[getOneSideStateWatermarkPredicate]] Join State Watermark Predicate (for One Side of Join) -- <code>getOneSideStateWatermarkPredicate</code> Internal Method</p>","location":"streaming-join/StreamingSymmetricHashJoinHelper/#source-scala"},{"title":"[source, scala]","text":"<p>getOneSideStateWatermarkPredicate(   oneSideInputAttributes: Seq[Attribute],   oneSideJoinKeys: Seq[Expression],   otherSideInputAttributes: Seq[Attribute]): Option[JoinStateWatermarkPredicate]</p>  <p><code>getOneSideStateWatermarkPredicate</code> finds what attributes were used to define the watermark attribute (the <code>oneSideInputAttributes</code> attributes, the &lt;&gt;) and creates a &lt;&gt; as follows: <ul> <li> <p>&lt;&gt; if the watermark was defined on a join key (with the watermark expression for the index of the join key expression)  <li> <p>&lt;&gt; if the watermark was defined among the <code>oneSideInputAttributes</code> (with the state value watermark based on the given <code>oneSideInputAttributes</code> and <code>otherSideInputAttributes</code>)   <p>NOTE: <code>getOneSideStateWatermarkPredicate</code> creates no &lt;&gt; (<code>None</code>) for no watermark found. <p>NOTE: <code>getStateWatermarkPredicates</code> is used exclusively to &lt;&gt;.","location":"streaming-join/StreamingSymmetricHashJoinHelper/#source-scala_1"},{"title":"SymmetricHashJoinStateManager","text":"<p><code>SymmetricHashJoinStateManager</code> is &lt;&gt; for the left and right OneSideHashJoiners of a StreamingSymmetricHashJoinExec physical operator (one for each side when <code>StreamingSymmetricHashJoinExec</code> is requested to process partitions of the left and right sides of a stream-stream join). <p></p> <p><code>SymmetricHashJoinStateManager</code> manages join state using the &lt;&gt; and the &lt;&gt; state store handlers (and simply acts like their facade).","location":"streaming-join/SymmetricHashJoinStateManager/"},{"title":"Creating Instance","text":"<p><code>SymmetricHashJoinStateManager</code> takes the following to be created:</p> <ul> <li>[[joinSide]] JoinSide</li> <li>[[inputValueAttributes]] Attributes of input values</li> <li>[[joinKeys]] Join keys (<code>Seq[Expression]</code>)</li> <li>[[stateInfo]] StatefulOperatorStateInfo</li> <li>[[storeConf]] StateStoreConf</li> <li>[[hadoopConf]] Hadoop Configuration</li> </ul> <p>=== [[keyToNumValues]][[keyWithIndexToValue]] KeyToNumValuesStore and KeyWithIndexToValueStore State Store Handlers -- <code>keyToNumValues</code> and <code>keyWithIndexToValue</code> Internal Properties</p> <p><code>SymmetricHashJoinStateManager</code> uses a &lt;&gt; (<code>keyToNumValues</code>) and a &lt;&gt; (<code>keyWithIndexToValue</code>) internally that are created immediately when <code>SymmetricHashJoinStateManager</code> is &lt;&gt; (for a OneSideHashJoiner). <p><code>keyToNumValues</code> and <code>keyWithIndexToValue</code> are used when <code>SymmetricHashJoinStateManager</code> is requested for the following:</p> <ul> <li> <p>&lt;&gt;  <li> <p>&lt;&gt;  <li> <p>&lt;&gt;  <li> <p>&lt;&gt;  <li> <p>&lt;&gt;  <li> <p>&lt;&gt;  <li> <p>&lt;&gt;   <p>=== [[joinSide-internals]] Join Side Marker -- <code>JoinSide</code> Internal Enum</p> <p><code>JoinSide</code> can be one of the two possible values:</p> <ul> <li> <p>[[LeftSide]][[left]] <code>LeftSide</code> (alias: <code>left</code>)</p> </li> <li> <p>[[RightSide]][[right]] <code>RightSide</code> (alias: <code>right</code>)</p> </li> </ul> <p>They are both used exclusively when <code>StreamingSymmetricHashJoinExec</code> binary physical operator is requested to &lt;&gt; (and &lt;&gt; with an OneSideHashJoiner). <p>=== [[metrics]] Performance Metrics -- <code>metrics</code> Method</p>","location":"streaming-join/SymmetricHashJoinStateManager/#creating-instance"},{"title":"[source, scala]","text":"","location":"streaming-join/SymmetricHashJoinStateManager/#source-scala"},{"title":"metrics: StateStoreMetrics","text":"<p><code>metrics</code> returns the combined StateStoreMetrics of the &lt;&gt; and the &lt;&gt; state store handlers. <p><code>metrics</code> is used when <code>OneSideHashJoiner</code> is requested to commitStateAndGetMetrics.</p> <p>=== [[removeByKeyCondition]] <code>removeByKeyCondition</code> Method</p>","location":"streaming-join/SymmetricHashJoinStateManager/#metrics-statestoremetrics"},{"title":"[source, scala]","text":"<p>removeByKeyCondition(   removalCondition: UnsafeRow =&gt; Boolean): Iterator[UnsafeRowPair]</p>  <p><code>removeByKeyCondition</code> creates an <code>Iterator</code> of <code>UnsafeRowPairs</code> that &lt;&gt; for which the given <code>removalCondition</code> predicate holds. <p>[[removeByKeyCondition-allKeyToNumValues]] <code>removeByKeyCondition</code> uses the &lt;&gt; for &lt;&gt;. <p><code>removeByKeyCondition</code> is used when <code>OneSideHashJoiner</code> is requested to remove an old state (for JoinStateKeyWatermarkPredicate).</p> <p>==== [[removeByKeyCondition-getNext]] <code>getNext</code> Internal Method (of <code>removeByKeyCondition</code> Method)</p>","location":"streaming-join/SymmetricHashJoinStateManager/#source-scala_1"},{"title":"[source, scala]","text":"","location":"streaming-join/SymmetricHashJoinStateManager/#source-scala_2"},{"title":"getNext(): UnsafeRowPair","text":"<p><code>getNext</code> goes over the keys and values in the &lt;&gt; sequence and &lt;&gt; (from the &lt;&gt;) and the &lt;&gt; (from the &lt;&gt;) for which the given <code>removalCondition</code> predicate holds. <p>=== [[removeByValueCondition]] <code>removeByValueCondition</code> Method</p>","location":"streaming-join/SymmetricHashJoinStateManager/#getnext-unsaferowpair"},{"title":"[source, scala]","text":"<p>removeByValueCondition(   removalCondition: UnsafeRow =&gt; Boolean): Iterator[UnsafeRowPair]</p>  <p><code>removeByValueCondition</code> creates an <code>Iterator</code> of <code>UnsafeRowPairs</code> that &lt;&gt; for which the given <code>removalCondition</code> predicate holds. <p><code>removeByValueCondition</code> is used when <code>OneSideHashJoiner</code> is requested to remove an old state (when JoinStateValueWatermarkPredicate is used).</p> <p>==== [[removeByValueCondition-getNext]] <code>getNext</code> Internal Method (of <code>removeByValueCondition</code> Method)</p>","location":"streaming-join/SymmetricHashJoinStateManager/#source-scala_3"},{"title":"[source, scala]","text":"","location":"streaming-join/SymmetricHashJoinStateManager/#source-scala_4"},{"title":"getNext(): UnsafeRowPair","text":"<p><code>getNext</code>...FIXME</p> <p>=== [[append]] Appending New Value Row to Key -- <code>append</code> Method</p>","location":"streaming-join/SymmetricHashJoinStateManager/#getnext-unsaferowpair_1"},{"title":"[source, scala]","text":"<p>append(   key: UnsafeRow,   value: UnsafeRow): Unit</p>  <p><code>append</code> requests the &lt;&gt; for the &lt;&gt;. <p>In the end, <code>append</code> requests the stores for the following:</p> <ul> <li> <p>&lt;&gt; to &lt;&gt;  <li> <p>&lt;&gt; to &lt;&gt;.   <p><code>append</code> is used when <code>OneSideHashJoiner</code> is requested to storeAndJoinWithOtherSide.</p> <p>=== [[get]] Retrieving Value Rows By Key -- <code>get</code> Method</p>","location":"streaming-join/SymmetricHashJoinStateManager/#source-scala_5"},{"title":"[source, scala]","text":"","location":"streaming-join/SymmetricHashJoinStateManager/#source-scala_6"},{"title":"get(key: UnsafeRow): Iterator[UnsafeRow]","text":"<p><code>get</code> requests the &lt;&gt; for the &lt;&gt;. <p>In the end, <code>get</code> requests the &lt;&gt; to &lt;&gt; and leaves value rows only. <p><code>get</code> is used when <code>OneSideHashJoiner</code> is requested to storeAndJoinWithOtherSide and retrieving value rows for a key.</p> <p>=== [[commit]] Committing State (Changes) -- <code>commit</code> Method</p>","location":"streaming-join/SymmetricHashJoinStateManager/#getkey-unsaferow-iteratorunsaferow"},{"title":"[source, scala]","text":"","location":"streaming-join/SymmetricHashJoinStateManager/#source-scala_7"},{"title":"commit(): Unit","text":"<p><code>commit</code> simply requests the &lt;&gt; and &lt;&gt; state store handlers to &lt;&gt;. <p><code>commit</code> is used when <code>OneSideHashJoiner</code> is requested to commit state changes and get performance metrics.</p> <p>=== [[abortIfNeeded]] Aborting State (Changes) -- <code>abortIfNeeded</code> Method</p>","location":"streaming-join/SymmetricHashJoinStateManager/#commit-unit"},{"title":"[source, scala]","text":"","location":"streaming-join/SymmetricHashJoinStateManager/#source-scala_8"},{"title":"abortIfNeeded(): Unit","text":"<p><code>abortIfNeeded</code>...FIXME</p> <p>NOTE: <code>abortIfNeeded</code> is used when...FIXME</p> <p>=== [[allStateStoreNames]] <code>allStateStoreNames</code> Object Method</p>","location":"streaming-join/SymmetricHashJoinStateManager/#abortifneeded-unit"},{"title":"[source, scala]","text":"","location":"streaming-join/SymmetricHashJoinStateManager/#source-scala_9"},{"title":"allStateStoreNames(joinSides: JoinSide*): Seq[String]","text":"<p><code>allStateStoreNames</code> simply returns the &lt;&gt; for all possible combinations of the given <code>JoinSides</code> and the two possible store types (e.g. &lt;&gt; and &lt;&gt;). <p>NOTE: <code>allStateStoreNames</code> is used exclusively when <code>StreamingSymmetricHashJoinExec</code> physical operator is requested to &lt;&gt; (as a <code>RDD[InternalRow]</code>). <p>=== [[getStateStoreName]] <code>getStateStoreName</code> Object Method</p>","location":"streaming-join/SymmetricHashJoinStateManager/#allstatestorenamesjoinsides-joinside-seqstring"},{"title":"[source, scala]","text":"<p>getStateStoreName(   joinSide: JoinSide,   storeType: StateStoreType): String</p>  <p><code>getStateStoreName</code> simply returns a string of the following format:</p> <pre><code>[joinSide]-[storeType]\n</code></pre>","location":"streaming-join/SymmetricHashJoinStateManager/#source-scala_10"},{"title":"[NOTE]","text":"<p><code>getStateStoreName</code> is used when:</p> <ul> <li><code>StateStoreHandler</code> is requested to &lt;&gt;","location":"streaming-join/SymmetricHashJoinStateManager/#note"},{"title":"* <code>SymmetricHashJoinStateManager</code> utility is requested for &lt;&gt; (for <code>StreamingSymmetricHashJoinExec</code> physical operator to &lt;&gt;) <p>=== [[updateNumValueForCurrentKey]] <code>updateNumValueForCurrentKey</code> Internal Method</p>","text":"","location":"streaming-join/SymmetricHashJoinStateManager/#symmetrichashjoinstatemanager-utility-is-requested-for-for-streamingsymmetrichashjoinexec-physical-operator-to"},{"title":"[source, scala]","text":"","location":"streaming-join/SymmetricHashJoinStateManager/#source-scala_11"},{"title":"updateNumValueForCurrentKey(): Unit","text":"<p><code>updateNumValueForCurrentKey</code>...FIXME</p> <p>NOTE: <code>updateNumValueForCurrentKey</code> is used exclusively when <code>SymmetricHashJoinStateManager</code> is requested to &lt;&gt;. <p>=== [[internal-properties]] Internal Properties</p> <p>[cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description</p> <p>| keyAttributes a| [[keyAttributes]] Key attributes, i.e. <code>AttributeReferences</code> of the &lt;&gt; <p>Used exclusively in <code>KeyWithIndexToValueStore</code> when requested for the &lt;&gt;, &lt;&gt;, &lt;&gt; and &lt;&gt; <p>| keySchema a| [[keySchema]] Key schema (<code>StructType</code>) based on the &lt;&gt; with the names in the format of field and their ordinals (index) <p>Used when:</p> <ul> <li> <p><code>SymmetricHashJoinStateManager</code> is requested for the &lt;&gt; (for &lt;&gt;)  <li> <p><code>KeyToNumValuesStore</code> is requested for the &lt;&gt;  <li> <p><code>KeyWithIndexToValueStore</code> is requested for the &lt;&gt; (for the internal &lt;&gt;)   <p>|===</p>","location":"streaming-join/SymmetricHashJoinStateManager/#updatenumvalueforcurrentkey-unit"},{"title":"Index","text":"<p>== Streaming Limit</p> <p>Streaming Limit is...FIXME</p>","location":"streaming-limit/"},{"title":"Streaming Watermark","text":"<p>Streaming Watermark of a stateful streaming query is how long to wait for late and possibly out-of-order events until a streaming state can be considered final and not to change.</p> <p>Streaming watermark is used to mark events (modeled as rows in a streaming query) that are older than the watermark threshold as \"too late\", and not \"interesting\" to update partial non-final streaming state (of an aggregation or a join).</p> <p>In Spark Structured Streaming, streaming watermark is defined using Dataset.withWatermark high-level operator.</p> <pre><code>withWatermark(\n  eventTime: String,\n  delayThreshold: String): Dataset[T]\n</code></pre> <p>In Dataset.withWatermark operator, <code>eventTime</code> is the name of the column to use to monitor event time whereas <code>delayThreshold</code> is the threshold.</p> <p>Watermark Threshold (Delay Threshold) says how late and possibly out-of-order events are still acceptable and contribute to the final result of a streaming state. Event-time watermark delay is used to calculate the difference between the event time of an event and the time in the past.</p> <p>Event-Time Watermark is then a time threshold (point in time) that is the minimum acceptable time of an event that is accepted in a streaming state.</p> <p>With streaming watermark, memory usage of a streaming state should be carefully monitored as late events can easily be dropped, and old state that are never going to be updated removed. This avoids unbounded streaming state that would inevitably use up all the available memory of long-running streaming queries and end up in out of memory errors.</p> <p>In Append output mode the current event-time streaming watermark is used for the following:</p> <ul> <li> <p>Output saved state rows that became expired (Expired events in the demo)</p> </li> <li> <p>Dropping late events, i.e. don't save them to a state store or include in aggregation (Late events in the demo)</p> </li> </ul> <p>Streaming watermark is required for a streaming aggregation in Append output mode.</p>","location":"streaming-watermark/"},{"title":"Streaming Aggregation","text":"<p>In streaming aggregation, a streaming watermark has to be defined on one or many grouping expressions of a streaming aggregation (directly or using window standard function).</p>  <p>Note</p> <p>Dataset.withWatermark operator has to be used before an aggregation operator (for the watermark to have an effect).</p>","location":"streaming-watermark/#streaming-aggregation"},{"title":"Streaming Join","text":"<p>In streaming join, a streaming watermark can be defined on join keys or any of the join sides.</p>","location":"streaming-watermark/#streaming-join"},{"title":"Demos","text":"<p>Use the following demos to learn more:</p> <ul> <li>Demo: Streaming Watermark with Aggregation in Append Output Mode</li> </ul>","location":"streaming-watermark/#demos"},{"title":"Internals","text":"<p>Under the covers, Dataset.withWatermark high-level operator creates a logical query plan with EventTimeWatermark logical operator.</p> <p><code>EventTimeWatermark</code> logical operator is planned to EventTimeWatermarkExec physical operator that extracts the event time values (from the rows processed) and adds them to an accumulator.</p> <p>Since the execution (data processing) happens on Spark executors, using the accumulator is the only Spark-approved way for communication between the tasks (on the executors) and the driver. Using accumulator updates the driver with the current event-time watermark.</p> <p>During the query planning phase (in MicroBatchExecution and ContinuousExecution) that also happens on the driver, <code>IncrementalExecution</code> is given the current OffsetSeqMetadata with the current event-time watermark.</p>","location":"streaming-watermark/#internals"},{"title":"Further Reading Or Watching","text":"<ul> <li>SPARK-18124 Observed delay based event time watermarks</li> </ul>","location":"streaming-watermark/#further-reading-or-watching"},{"title":"EventTimeStats","text":"<p><code>EventTimeStats</code> is used to help calculate event-time column statistics (statistics of the values of an event-time column):</p> <ul> <li> Maximum <li> Minimum <li> Average <li> Count  <p><code>EventTimeStats</code> is used by EventTimeStatsAccum accumulator.</p>","location":"streaming-watermark/EventTimeStats/"},{"title":"Zero Value <p><code>EventTimeStats</code> defines a special value zero with the following values:</p> <ul> <li><code>Long.MinValue</code> for the max</li> <li><code>Long.MaxValue</code> for the min</li> <li><code>0.0</code> for the avg</li> <li><code>0L</code> for the count</li> </ul>","text":"","location":"streaming-watermark/EventTimeStats/#zero-value"},{"title":"Adding Event-Time Value <pre><code>add(\n  eventTime: Long): Unit\n</code></pre> <p><code>add</code> updates the statistics given the <code>eventTime</code> value.</p>","text":"","location":"streaming-watermark/EventTimeStats/#adding-event-time-value"},{"title":"Merging EventTimeStats <pre><code>merge(\n  that: EventTimeStats): Unit\n</code></pre> <p><code>merge</code>...FIXME</p>","text":"","location":"streaming-watermark/EventTimeStats/#merging-eventtimestats"},{"title":"EventTimeStatsAccum Accumulator","text":"<p><code>EventTimeStatsAccum</code> is an <code>AccumulatorV2</code> (Spark Core) that accumulates <code>Long</code> values and produces an EventTimeStats.</p> <pre><code>AccumulatorV2[Long, EventTimeStats]\n</code></pre>","location":"streaming-watermark/EventTimeStatsAccum/"},{"title":"Creating Instance","text":"<p><code>EventTimeStatsAccum</code> takes the following to be created:</p> <ul> <li>EventTimeStats (default: EventTimeStats.zero)</li> </ul> <p><code>EventTimeStatsAccum</code> is created\u00a0when:</p> <ul> <li>EventTimeWatermarkExec unary physical operator is created (and initializes eventTimeStats)</li> </ul>  <p>EventTimeWatermarkExec Physical Operator</p> <p>When <code>EventTimeWatermarkExec</code> physical operator is requested to execute, every task simply adds the values of the event-time watermark column expression to the EventTimeStatsAccum accumulator.</p> <p>As per design of Spark accumulators in Apache Spark, accumulator updates are automatically sent out (propagated) from tasks to the driver every heartbeat and then they are accumulated together.</p>","location":"streaming-watermark/EventTimeStatsAccum/#creating-instance"},{"title":"EventTimeStats <p><code>EventTimeStatsAccum</code> is given an EventTimeStats when created.</p> <p>Every time <code>AccumulatorV2</code> methods are called, <code>EventTimeStatsAccum</code> simply delegates them to the <code>EventTimeStats</code> (that is responsible for event-time statistics, i.e. max, min, avg, count).</p>","text":"","location":"streaming-watermark/EventTimeStatsAccum/#eventtimestats"},{"title":"Adding Value <pre><code>add(\n  v: Long): Unit\n</code></pre> <p><code>add</code> is part of the <code>AccumulatorV2</code> (Spark Core) abstraction.</p> <p><code>add</code> simply requests the EventTimeStats to add the given <code>v</code> value.</p> <p><code>add</code> is used when EventTimeWatermarkExec physical operator is executed.</p>","text":"","location":"streaming-watermark/EventTimeStatsAccum/#adding-value"},{"title":"Structured Streaming Web UI","text":"<p>Spark Structured Streaming can be monitored using StreamingQueryTab that attaches the following two pages:</p> <ul> <li>StreamingQueryPage</li> <li>Statistics</li> </ul> <p>Streaming events are intercepted using StreamingQueryStatusListener (and persisted in the store). The store is used to create a StreamingQueryStatusStore for StreamingQueryTab (and the two attached pages).</p>","location":"webui/"},{"title":"StreamingQueryPage","text":"<p><code>StreamingQueryPage</code> is...FIXME</p>","location":"webui/StreamingQueryPage/"},{"title":"StreamingQueryStatisticsPage","text":"<p><code>StreamingQueryStatisticsPage</code> is a <code>WebUIPage</code> (Spark Core) with <code>statistics</code> URL prefix.</p> <p><code>StreamingQueryStatisticsPage</code> uses <code>id</code> request parameter as the <code>runId</code> of a streaming query to render a Streaming Query Statistics page.</p> <p><code>StreamingQueryStatisticsPage</code> renders the following main sections:</p> <ul> <li>Basic Info</li> <li>Statistics</li> </ul>","location":"webui/StreamingQueryStatisticsPage/"},{"title":"Aggregated Number Of Total State Rows","text":"<p></p>  <p>The number of total state rows (the sums of numRowsTotal metrics of stateOperators)</p> <p><code>StreamingQueryStatisticsPage</code> uses generateAggregatedStateOperators to generate a timeline and a histogram for <code>aggregated-num-total-state-rows-timeline</code> and <code>aggregated-num-total-state-rows-histogram</code> HTML <code>div</code>s, respectively.</p>  <p>Demo: Streaming Aggregation</p> <p>Use Demo: Streaming Aggregation to learn more.</p>","location":"webui/StreamingQueryStatisticsPage/#aggregated-number-of-total-state-rows"},{"title":"Global Watermark Gap","text":"<p></p>  <p>A time series of the gaps (differences) between the batch timestamp and the global watermark (in secs).</p> <p>Only displayed when there is a global watermark in a streaming query.</p>  <p>Global Watermark of Batch</p> <p>Global Watermark of a batch is the value of the <code>watermark</code> entry in the Event Time Statistics of a StreamingQueryProgress.</p> <p><code>watermark</code> entry will only be included in the Event Time Statistics for a streaming query with EventTimeWatermark logical operator.</p>  <p><code>StreamingQueryStatisticsPage</code> uses generateWatermark to generate a timeline and a histogram for <code>watermark-gap-timeline</code> and <code>watermark-gap-histogram</code> HTML <code>div</code>s, respectively.</p>","location":"webui/StreamingQueryStatisticsPage/#global-watermark-gap"},{"title":"Rendering Page <pre><code>render(\n  request: HttpServletRequest): Seq[Node]\n</code></pre> <p><code>render</code> is part of the <code>WebUIPage</code> (Spark Core) abstraction.</p>  <p><code>render</code> uses the <code>id</code> request parameter for the <code>runId</code> of the streaming query to render statistics of.</p> <p><code>render</code> requests the parent StreamingQueryTab for the StreamingQueryStatusStore for the allQueryUIData to find the data of the streaming query (by <code>runId</code>).</p> <p><code>render</code> generates a HTML page with the following sections:</p> <ul> <li>generateLoadResources</li> <li>generateBasicInfo</li> <li>generateStatTable</li> </ul>","text":"","location":"webui/StreamingQueryStatisticsPage/#rendering-page"},{"title":"generateStatTable <pre><code>generateStatTable(\n  query: StreamingQueryUIData): Seq[Node]\n</code></pre> <p><code>generateStatTable</code>...FIXME</p>","text":"","location":"webui/StreamingQueryStatisticsPage/#generatestattable"},{"title":"generateWatermark <pre><code>generateWatermark(\n  query: StreamingQueryUIData,\n  minBatchTime: Long,\n  maxBatchTime: Long,\n  jsCollector: JsCollector): Seq[Node]\n</code></pre> <p><code>generateWatermark</code> finds the global watermark (<code>watermark</code> entry in the eventTime) of the last StreamingQueryProgress of a streaming query.</p> <p>Unless found, <code>generateWatermark</code> returns nothing (an empty collection).</p> <p><code>generateWatermark</code> scans StreamingQueryProgresses for which the global watermark is greater than <code>0</code> and collects a time series (data points) with the following:</p> <ul> <li>Batch timestamp</li> <li>The gap (difference) between the batch timestamp and the global watermark (in secs)</li> </ul> <p><code>generateWatermark</code> creates a <code>GraphUIData</code> to generate a timeline and a histogram for <code>watermark-gap-timeline</code> and <code>watermark-gap-histogram</code> HTML <code>div</code>s, respectively.</p>","text":"","location":"webui/StreamingQueryStatisticsPage/#generatewatermark"},{"title":"generateAggregatedStateOperators <pre><code>generateAggregatedStateOperators(\n  query: StreamingQueryUIData,\n  minBatchTime: Long,\n  maxBatchTime: Long,\n  jsCollector: JsCollector): NodeBuffer\n</code></pre> <p><code>generateAggregatedStateOperators</code> takes stateOperators of the last StreamingQueryProgress of the given streaming query (as <code>StreamingQueryUIData</code>).</p> <p>Unless available, <code>generateAggregatedStateOperators</code> returns nothing (an empty collection).</p> <p><code>generateAggregatedStateOperators</code> generates data points for a timeline and a histogram for <code>aggregated-num-total-state-rows</code> as the sum of numRowsTotal metrics of stateOperators.</p> <p><code>generateAggregatedStateOperators</code>...FIXME</p>","text":"","location":"webui/StreamingQueryStatisticsPage/#generateaggregatedstateoperators"},{"title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.streaming.ui.StreamingQueryStatisticsPage</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.sql.streaming.ui.StreamingQueryStatisticsPage=ALL\n</code></pre> <p>Refer to Logging.</p>","text":"","location":"webui/StreamingQueryStatisticsPage/#logging"},{"title":"StreamingQueryStatusListener","text":"<p><code>StreamingQueryStatusListener</code> is a StreamingQueryListener that intercepts streaming events and writes them out to the ElementTrackingStore (for StreamingQueryTab).</p>","location":"webui/StreamingQueryStatusListener/"},{"title":"Creating Instance","text":"<p><code>StreamingQueryStatusListener</code> takes the following to be created:</p> <ul> <li> <code>SparkConf</code> (Spark Core) <li> <code>ElementTrackingStore</code> (Spark Core)  <p><code>StreamingQueryStatusListener</code> is created when:</p> <ul> <li><code>StreamingQueryHistoryServerPlugin</code> is requested to <code>createListeners</code></li> <li><code>SharedState</code> (Spark SQL) is created (with spark.sql.streaming.ui.enabled enabled)</li> </ul>","location":"webui/StreamingQueryStatusListener/#creating-instance"},{"title":"onQueryStarted <pre><code>onQueryStarted(\n  event: StreamingQueryListener.QueryStartedEvent): Unit\n</code></pre> <p><code>onQueryStarted</code> is part of the StreamingQueryListener abstraction.</p>  <p><code>onQueryStarted</code> writes out a new \"start\" <code>StreamingQueryData</code> to the ElementTrackingStore.</p>","text":"","location":"webui/StreamingQueryStatusListener/#onquerystarted"},{"title":"onQueryProgress <pre><code>onQueryProgress(\n  event: StreamingQueryListener.QueryProgressEvent): Unit\n</code></pre> <p><code>onQueryProgress</code> is part of the StreamingQueryListener abstraction.</p>  <p><code>onQueryProgress</code>...FIXME</p>","text":"","location":"webui/StreamingQueryStatusListener/#onqueryprogress"},{"title":"onQueryTerminated <pre><code>onQueryTerminated(\n  event: StreamingQueryListener.QueryTerminatedEvent): Unit\n</code></pre> <p><code>onQueryTerminated</code> is part of the StreamingQueryListener abstraction.</p>  <p><code>onQueryTerminated</code> finds the query summary (the <code>StreamingQueryData</code>) for the <code>runId</code> in the ElementTrackingStore.</p> <p><code>onQueryTerminated</code> writes out a new \"terminate\" <code>StreamingQueryData</code> to the ElementTrackingStore.</p> <p>In the end, <code>onQueryTerminated</code> removes the streaming query (by <code>runId</code>) from the queryToProgress registry.</p>","text":"","location":"webui/StreamingQueryStatusListener/#onqueryterminated"},{"title":"StreamingQueryStatusStore","text":"","location":"webui/StreamingQueryStatusStore/"},{"title":"Creating Instance","text":"<p><code>StreamingQueryStatusStore</code> takes the following to be created:</p> <ul> <li> <code>KVStore</code> (Spark Core)  <p><code>StreamingQueryStatusStore</code> is created when:</p> <ul> <li><code>StreamingQueryHistoryServerPlugin</code> is requested to <code>setupUI</code></li> <li><code>SharedState</code> (Spark SQL) is created (with spark.sql.streaming.ui.enabled enabled)</li> </ul>","location":"webui/StreamingQueryStatusStore/#creating-instance"},{"title":"allQueryUIData <pre><code>allQueryUIData: Seq[StreamingQueryUIData]\n</code></pre> <p><code>allQueryUIData</code> creates a view of <code>StreamingQueryData</code>s (in the KVStore) indexed by <code>startTimestamp</code> to makeUIData.</p>  <p><code>allQueryUIData</code> is used when:</p> <ul> <li><code>StreamingQueryHistoryServerPlugin</code> is requested to <code>setupUI</code></li> <li><code>StreamingQueryPage</code> is requested to generateStreamingQueryTable</li> <li><code>StreamingQueryStatisticsPage</code> is requested to render</li> </ul>","text":"","location":"webui/StreamingQueryStatusStore/#allqueryuidata"},{"title":"makeUIData <pre><code>makeUIData(\n  summary: StreamingQueryData): StreamingQueryUIData\n</code></pre> <p><code>makeUIData</code>...FIXME</p>","text":"","location":"webui/StreamingQueryStatusStore/#makeuidata"},{"title":"StreamingQueryTab","text":"<p><code>StreamingQueryTab</code> is a <code>SparkUITab</code> (Spark Core) with <code>StreamingQuery</code> URL prefix.</p> <p>When created, <code>StreamingQueryTab</code> attaches the following pages:</p> <ul> <li>StreamingQueryPage</li> <li>StreamingQueryStatisticsPage</li> </ul> <p><code>StreamingQueryTab</code> can be turned on/off using spark.sql.streaming.ui.enabled.</p>","location":"webui/StreamingQueryTab/"},{"title":"Creating Instance","text":"<p><code>StreamingQueryTab</code> takes the following to be created:</p> <ul> <li>StreamingQueryStatusStore</li> <li> <code>SparkUI</code> (Spark Core)  <p><code>StreamingQueryTab</code> is created when:</p> <ul> <li><code>StreamingQueryHistoryServerPlugin</code> is requested to <code>setupUI</code></li> <li><code>SharedState</code> (Spark SQL) is created (with spark.sql.streaming.ui.enabled enabled)</li> </ul>","location":"webui/StreamingQueryTab/#creating-instance"},{"title":"StreamingQueryStatusStore <p><code>StreamingQueryTab</code> is given a StreamingQueryStatusStore when created.</p> <p>The <code>StreamingQueryStatusStore</code> is used to fetch the streaming query data in the attached pages:</p> <ul> <li>StreamingQueryPage</li> <li>StreamingQueryStatisticsPage</li> </ul>","text":"","location":"webui/StreamingQueryTab/#streamingquerystatusstore"},{"title":"Tab Name <pre><code>name: String\n</code></pre> <p><code>name</code> is part of the <code>WebUITab</code> (Spark Core) abstraction.</p>  <p><code>name</code> is <code>Structured Streaming</code>.</p>","text":"","location":"webui/StreamingQueryTab/#tab-name"}]}