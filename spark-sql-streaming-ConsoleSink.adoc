== [[ConsoleSink]] ConsoleSink -- Showing DataFrames to Console

`ConsoleSink` is a link:spark-sql-streaming-Sink.adoc[streaming sink] that <<addBatch, shows the DataFrame (for a batch) to the console>>.

`ConsoleSink` is registered as *console* format (by link:spark-sql-streaming-ConsoleSinkProvider.adoc[ConsoleSinkProvider]).

[[options]]
.ConsoleSink's Options
[cols="1,1,2",options="header",width="100%"]
|===
| Name
| Default Value
| Description

| [[numRows]] `numRows`
| `20`
| Number of rows to display

| [[truncate]] `truncate`
| `true`
| Truncate the data to display to 20 characters
|===

[source, scala]
----
import org.apache.spark.sql.streaming.{OutputMode, Trigger}
val query = spark.
  readStream.
  format("rate").
  load.
  writeStream.
  format("console").  // <-- use ConsoleSink
  option("truncate", false).
  option("numRows", 10).
  trigger(Trigger.ProcessingTime("10 seconds")).
  queryName("rate-console").
  start

-------------------------------------------
Batch: 0
-------------------------------------------
+-----+
|value|
+-----+
|hello|
+-----+
----

=== [[addBatch]] Adding Batch (by Showing DataFrame to Console) -- `addBatch` Method

[source, scala]
----
addBatch(batchId: Long, data: DataFrame): Unit
----

NOTE: `addBatch` is a part of link:spark-sql-streaming-Sink.adoc#addBatch[Sink Contract].

Internally, `addBatch` records the input `batchId` in <<lastBatchId, lastBatchId>> internal property.

`addBatch` collects the input `data` `DataFrame` and creates a brand new DataFrame that it then shows (per <<numRowsToShow, numRowsToShow>> and <<isTruncated, isTruncated>> properties).

```
-------------------------------------------
Batch: [batchId]
-------------------------------------------
+---------+-----+
|timestamp|value|
+---------+-----+
+---------+-----+
```

NOTE: You may see `Rerun batch:` instead if the input `batchId` is below <<lastBatchId, lastBatchId>> (likely due to a batch failure).
